{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "# import data_helpers\n",
    "from CNN import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=128\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=1000\n",
      "NUM_FILTERS=128\n",
      "UNROLLED_LSTM=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 128, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 1000, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 494733\n",
      "Train/Dev split: 225000/25000\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "df = pd.read_csv('./short_clean_data/mapped_data_rand250k.csv')\n",
    "x_text = df.txt.values\n",
    "y = pd.get_dummies(df.lang).values\n",
    "del(df)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = 70 #max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:29 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:30 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:33 in <module>.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "Writing to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:41 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:42 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:45 in <module>.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:47 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:50 in <module>.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n",
      "WARNING:tensorflow:From <ipython-input-4-623ac9c9fd6d>:52 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "2017-01-10T20:21:06.984197: step 1, loss 9.74498, acc 0.0390625\n",
      "2017-01-10T20:21:09.007304: step 2, loss 8.45813, acc 0.0625\n",
      "2017-01-10T20:21:11.014751: step 3, loss 7.54944, acc 0.0546875\n",
      "2017-01-10T20:21:13.029489: step 4, loss 6.71324, acc 0.0390625\n",
      "2017-01-10T20:21:14.992414: step 5, loss 7.19498, acc 0.109375\n",
      "2017-01-10T20:21:16.996745: step 6, loss 7.36313, acc 0.078125\n",
      "2017-01-10T20:21:19.596136: step 7, loss 5.79526, acc 0.179688\n",
      "2017-01-10T20:21:21.869648: step 8, loss 6.94064, acc 0.101562\n",
      "2017-01-10T20:21:23.821501: step 9, loss 6.72755, acc 0.0390625\n",
      "2017-01-10T20:21:25.796215: step 10, loss 6.46225, acc 0.109375\n",
      "2017-01-10T20:21:27.759604: step 11, loss 6.54889, acc 0.101562\n",
      "2017-01-10T20:21:29.762674: step 12, loss 6.81375, acc 0.164062\n",
      "2017-01-10T20:21:31.693691: step 13, loss 6.77237, acc 0.125\n",
      "2017-01-10T20:21:33.689671: step 14, loss 5.79806, acc 0.132812\n",
      "2017-01-10T20:21:35.772573: step 15, loss 5.41944, acc 0.148438\n",
      "2017-01-10T20:21:37.738217: step 16, loss 6.21756, acc 0.164062\n",
      "2017-01-10T20:21:39.711222: step 17, loss 5.87592, acc 0.101562\n",
      "2017-01-10T20:21:41.715891: step 18, loss 4.8647, acc 0.179688\n",
      "2017-01-10T20:21:43.893656: step 19, loss 4.80154, acc 0.148438\n",
      "2017-01-10T20:21:45.832371: step 20, loss 4.75385, acc 0.226562\n",
      "2017-01-10T20:21:47.746601: step 21, loss 4.98154, acc 0.210938\n",
      "2017-01-10T20:21:49.770608: step 22, loss 5.69382, acc 0.140625\n",
      "2017-01-10T20:21:51.784746: step 23, loss 4.91073, acc 0.15625\n",
      "2017-01-10T20:21:53.741850: step 24, loss 5.30449, acc 0.195312\n",
      "2017-01-10T20:21:55.673173: step 25, loss 5.36725, acc 0.148438\n",
      "2017-01-10T20:21:57.628491: step 26, loss 4.84997, acc 0.21875\n",
      "2017-01-10T20:21:59.594939: step 27, loss 5.42424, acc 0.171875\n",
      "2017-01-10T20:22:01.617598: step 28, loss 5.17548, acc 0.226562\n",
      "2017-01-10T20:22:03.622710: step 29, loss 4.611, acc 0.234375\n",
      "2017-01-10T20:22:05.609208: step 30, loss 4.97206, acc 0.148438\n",
      "2017-01-10T20:22:07.528099: step 31, loss 5.77418, acc 0.125\n",
      "2017-01-10T20:22:09.639385: step 32, loss 4.89442, acc 0.15625\n",
      "2017-01-10T20:22:11.755690: step 33, loss 4.41087, acc 0.242188\n",
      "2017-01-10T20:22:13.900608: step 34, loss 4.92001, acc 0.203125\n",
      "2017-01-10T20:22:16.051452: step 35, loss 4.49646, acc 0.179688\n",
      "2017-01-10T20:22:18.173883: step 36, loss 5.15079, acc 0.195312\n",
      "2017-01-10T20:22:20.259210: step 37, loss 5.3603, acc 0.164062\n",
      "2017-01-10T20:22:22.259085: step 38, loss 4.24304, acc 0.265625\n",
      "2017-01-10T20:22:24.220428: step 39, loss 4.43863, acc 0.242188\n",
      "2017-01-10T20:22:26.200904: step 40, loss 3.94734, acc 0.257812\n",
      "2017-01-10T20:22:28.158256: step 41, loss 4.61206, acc 0.234375\n",
      "2017-01-10T20:22:30.126633: step 42, loss 4.45285, acc 0.203125\n",
      "2017-01-10T20:22:32.106625: step 43, loss 4.4458, acc 0.195312\n",
      "2017-01-10T20:22:34.090077: step 44, loss 4.31205, acc 0.273438\n",
      "2017-01-10T20:22:36.062717: step 45, loss 3.72493, acc 0.28125\n",
      "2017-01-10T20:22:38.054764: step 46, loss 3.75788, acc 0.304688\n",
      "2017-01-10T20:22:40.039822: step 47, loss 4.07595, acc 0.28125\n",
      "2017-01-10T20:22:42.073850: step 48, loss 4.16549, acc 0.234375\n",
      "2017-01-10T20:22:44.061993: step 49, loss 3.973, acc 0.304688\n",
      "2017-01-10T20:22:46.033348: step 50, loss 4.00751, acc 0.234375\n",
      "2017-01-10T20:22:48.228869: step 51, loss 3.58994, acc 0.320312\n",
      "2017-01-10T20:22:50.241100: step 52, loss 4.11454, acc 0.25\n",
      "2017-01-10T20:22:52.312521: step 53, loss 3.52845, acc 0.3125\n",
      "2017-01-10T20:22:54.242835: step 54, loss 3.56132, acc 0.34375\n",
      "2017-01-10T20:22:56.225338: step 55, loss 3.75003, acc 0.296875\n",
      "2017-01-10T20:22:58.131983: step 56, loss 3.18308, acc 0.40625\n",
      "2017-01-10T20:23:00.060111: step 57, loss 3.99893, acc 0.226562\n",
      "2017-01-10T20:23:02.059372: step 58, loss 3.56075, acc 0.289062\n",
      "2017-01-10T20:23:04.030322: step 59, loss 3.56525, acc 0.328125\n",
      "2017-01-10T20:23:05.987280: step 60, loss 3.46951, acc 0.320312\n",
      "2017-01-10T20:23:07.960091: step 61, loss 2.4681, acc 0.453125\n",
      "2017-01-10T20:23:09.875058: step 62, loss 3.50822, acc 0.304688\n",
      "2017-01-10T20:23:11.827106: step 63, loss 3.17024, acc 0.320312\n",
      "2017-01-10T20:23:13.794224: step 64, loss 3.67654, acc 0.289062\n",
      "2017-01-10T20:23:15.743125: step 65, loss 3.17066, acc 0.34375\n",
      "2017-01-10T20:23:17.749564: step 66, loss 3.2432, acc 0.3125\n",
      "2017-01-10T20:23:20.388163: step 67, loss 3.7535, acc 0.304688\n",
      "2017-01-10T20:23:22.466712: step 68, loss 3.55259, acc 0.335938\n",
      "2017-01-10T20:23:24.469208: step 69, loss 3.12299, acc 0.390625\n",
      "2017-01-10T20:23:26.446891: step 70, loss 3.05169, acc 0.375\n",
      "2017-01-10T20:23:28.440004: step 71, loss 2.73153, acc 0.414062\n",
      "2017-01-10T20:23:30.456929: step 72, loss 3.16135, acc 0.367188\n",
      "2017-01-10T20:23:32.436893: step 73, loss 2.60715, acc 0.46875\n",
      "2017-01-10T20:23:34.404157: step 74, loss 2.87827, acc 0.414062\n",
      "2017-01-10T20:23:37.345125: step 75, loss 2.9681, acc 0.429688\n",
      "2017-01-10T20:23:39.345666: step 76, loss 2.94717, acc 0.414062\n",
      "2017-01-10T20:23:41.333373: step 77, loss 3.06192, acc 0.398438\n",
      "2017-01-10T20:23:43.327196: step 78, loss 2.94569, acc 0.359375\n",
      "2017-01-10T20:23:45.335046: step 79, loss 3.28605, acc 0.34375\n",
      "2017-01-10T20:23:47.274874: step 80, loss 2.43149, acc 0.507812\n",
      "2017-01-10T20:23:49.261438: step 81, loss 2.99446, acc 0.359375\n",
      "2017-01-10T20:23:51.259325: step 82, loss 2.51776, acc 0.476562\n",
      "2017-01-10T20:23:53.321376: step 83, loss 2.34293, acc 0.460938\n",
      "2017-01-10T20:23:55.305765: step 84, loss 2.67695, acc 0.429688\n",
      "2017-01-10T20:23:57.266936: step 85, loss 2.52423, acc 0.453125\n",
      "2017-01-10T20:23:59.289600: step 86, loss 2.7127, acc 0.484375\n",
      "2017-01-10T20:24:01.269379: step 87, loss 2.70502, acc 0.4375\n",
      "2017-01-10T20:24:03.209270: step 88, loss 2.41695, acc 0.390625\n",
      "2017-01-10T20:24:05.185849: step 89, loss 2.44706, acc 0.492188\n",
      "2017-01-10T20:24:07.183781: step 90, loss 2.70601, acc 0.40625\n",
      "2017-01-10T20:24:09.181551: step 91, loss 2.96541, acc 0.484375\n",
      "2017-01-10T20:24:11.162194: step 92, loss 2.67317, acc 0.476562\n",
      "2017-01-10T20:24:13.144378: step 93, loss 2.10476, acc 0.523438\n",
      "2017-01-10T20:24:15.158042: step 94, loss 2.65643, acc 0.53125\n",
      "2017-01-10T20:24:17.157432: step 95, loss 2.55424, acc 0.445312\n",
      "2017-01-10T20:24:19.101375: step 96, loss 2.28961, acc 0.5\n",
      "2017-01-10T20:24:21.133024: step 97, loss 2.11737, acc 0.515625\n",
      "2017-01-10T20:24:23.258967: step 98, loss 1.84303, acc 0.585938\n",
      "2017-01-10T20:24:25.264024: step 99, loss 2.56724, acc 0.390625\n",
      "2017-01-10T20:24:27.259240: step 100, loss 2.45943, acc 0.492188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:25:00.934562: step 100, loss 0.859257, acc 0.77672\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-100\n",
      "\n",
      "2017-01-10T20:25:05.826738: step 101, loss 2.62629, acc 0.507812\n",
      "2017-01-10T20:25:07.811403: step 102, loss 2.18058, acc 0.539062\n",
      "2017-01-10T20:25:09.792341: step 103, loss 2.12105, acc 0.523438\n",
      "2017-01-10T20:25:11.786871: step 104, loss 2.49283, acc 0.460938\n",
      "2017-01-10T20:25:13.765661: step 105, loss 2.28952, acc 0.546875\n",
      "2017-01-10T20:25:15.777073: step 106, loss 2.64257, acc 0.484375\n",
      "2017-01-10T20:25:17.719355: step 107, loss 1.82494, acc 0.59375\n",
      "2017-01-10T20:25:19.779601: step 108, loss 2.13132, acc 0.515625\n",
      "2017-01-10T20:25:21.862812: step 109, loss 2.35834, acc 0.453125\n",
      "2017-01-10T20:25:23.854653: step 110, loss 2.15837, acc 0.523438\n",
      "2017-01-10T20:25:25.811421: step 111, loss 1.90692, acc 0.539062\n",
      "2017-01-10T20:25:27.807111: step 112, loss 1.70435, acc 0.570312\n",
      "2017-01-10T20:25:29.860115: step 113, loss 2.28393, acc 0.5\n",
      "2017-01-10T20:25:32.023293: step 114, loss 1.97193, acc 0.578125\n",
      "2017-01-10T20:25:33.985277: step 115, loss 1.80119, acc 0.625\n",
      "2017-01-10T20:25:35.988721: step 116, loss 2.18025, acc 0.523438\n",
      "2017-01-10T20:25:37.955738: step 117, loss 1.86046, acc 0.554688\n",
      "2017-01-10T20:25:39.922310: step 118, loss 2.39086, acc 0.453125\n",
      "2017-01-10T20:25:41.921166: step 119, loss 1.97233, acc 0.5625\n",
      "2017-01-10T20:25:43.902142: step 120, loss 1.99708, acc 0.578125\n",
      "2017-01-10T20:25:46.331979: step 121, loss 1.74315, acc 0.625\n",
      "2017-01-10T20:25:48.412238: step 122, loss 1.91711, acc 0.609375\n",
      "2017-01-10T20:25:50.446136: step 123, loss 1.87749, acc 0.5625\n",
      "2017-01-10T20:25:52.460955: step 124, loss 1.58547, acc 0.632812\n",
      "2017-01-10T20:25:54.450710: step 125, loss 1.39164, acc 0.617188\n",
      "2017-01-10T20:25:56.453425: step 126, loss 1.7231, acc 0.546875\n",
      "2017-01-10T20:25:58.439123: step 127, loss 1.93145, acc 0.554688\n",
      "2017-01-10T20:26:00.427114: step 128, loss 1.78278, acc 0.617188\n",
      "2017-01-10T20:26:03.969632: step 129, loss 2.21273, acc 0.523438\n",
      "2017-01-10T20:26:06.609808: step 130, loss 2.24556, acc 0.546875\n",
      "2017-01-10T20:26:08.593139: step 131, loss 2.02323, acc 0.554688\n",
      "2017-01-10T20:26:10.584474: step 132, loss 1.72389, acc 0.609375\n",
      "2017-01-10T20:26:12.584967: step 133, loss 1.99134, acc 0.5625\n",
      "2017-01-10T20:26:14.555607: step 134, loss 1.82927, acc 0.5625\n",
      "2017-01-10T20:26:16.566550: step 135, loss 1.42731, acc 0.617188\n",
      "2017-01-10T20:26:18.521297: step 136, loss 0.980927, acc 0.71875\n",
      "2017-01-10T20:26:20.545924: step 137, loss 2.06551, acc 0.585938\n",
      "2017-01-10T20:26:22.495643: step 138, loss 1.76552, acc 0.601562\n",
      "2017-01-10T20:26:24.465063: step 139, loss 1.67635, acc 0.570312\n",
      "2017-01-10T20:26:26.407927: step 140, loss 1.78399, acc 0.585938\n",
      "2017-01-10T20:26:28.349430: step 141, loss 1.57896, acc 0.632812\n",
      "2017-01-10T20:26:30.330510: step 142, loss 1.83381, acc 0.5625\n",
      "2017-01-10T20:26:32.292582: step 143, loss 1.70145, acc 0.625\n",
      "2017-01-10T20:26:34.293161: step 144, loss 2.10475, acc 0.59375\n",
      "2017-01-10T20:26:36.461428: step 145, loss 1.88601, acc 0.609375\n",
      "2017-01-10T20:26:38.449563: step 146, loss 1.67843, acc 0.625\n",
      "2017-01-10T20:26:40.420703: step 147, loss 1.9995, acc 0.546875\n",
      "2017-01-10T20:26:42.437655: step 148, loss 1.75016, acc 0.640625\n",
      "2017-01-10T20:26:44.448272: step 149, loss 1.40219, acc 0.671875\n",
      "2017-01-10T20:26:46.411884: step 150, loss 1.9261, acc 0.546875\n",
      "2017-01-10T20:26:48.372557: step 151, loss 1.7267, acc 0.640625\n",
      "2017-01-10T20:26:50.375033: step 152, loss 1.70562, acc 0.640625\n",
      "2017-01-10T20:26:52.640398: step 153, loss 1.46385, acc 0.679688\n",
      "2017-01-10T20:26:54.627935: step 154, loss 1.67142, acc 0.625\n",
      "2017-01-10T20:26:56.612863: step 155, loss 1.89138, acc 0.570312\n",
      "2017-01-10T20:26:58.553858: step 156, loss 1.41417, acc 0.695312\n",
      "2017-01-10T20:27:00.501378: step 157, loss 1.25294, acc 0.679688\n",
      "2017-01-10T20:27:02.498812: step 158, loss 1.15324, acc 0.6875\n",
      "2017-01-10T20:27:04.534415: step 159, loss 1.27265, acc 0.6875\n",
      "2017-01-10T20:27:06.511134: step 160, loss 1.90128, acc 0.632812\n",
      "2017-01-10T20:27:09.621781: step 161, loss 1.80293, acc 0.578125\n",
      "2017-01-10T20:27:11.554114: step 162, loss 1.66035, acc 0.609375\n",
      "2017-01-10T20:27:13.558492: step 163, loss 1.87596, acc 0.617188\n",
      "2017-01-10T20:27:15.542476: step 164, loss 1.83253, acc 0.59375\n",
      "2017-01-10T20:27:17.526438: step 165, loss 1.36286, acc 0.671875\n",
      "2017-01-10T20:27:19.574905: step 166, loss 1.51106, acc 0.710938\n",
      "2017-01-10T20:27:21.629998: step 167, loss 1.19151, acc 0.710938\n",
      "2017-01-10T20:27:23.597278: step 168, loss 1.70122, acc 0.609375\n",
      "2017-01-10T20:27:25.579879: step 169, loss 1.32722, acc 0.648438\n",
      "2017-01-10T20:27:27.600178: step 170, loss 1.34167, acc 0.664062\n",
      "2017-01-10T20:27:29.585301: step 171, loss 1.41587, acc 0.679688\n",
      "2017-01-10T20:27:31.523174: step 172, loss 1.46369, acc 0.679688\n",
      "2017-01-10T20:27:33.525440: step 173, loss 1.57772, acc 0.617188\n",
      "2017-01-10T20:27:35.536839: step 174, loss 1.49562, acc 0.671875\n",
      "2017-01-10T20:27:37.542078: step 175, loss 1.10779, acc 0.703125\n",
      "2017-01-10T20:27:39.525419: step 176, loss 1.65093, acc 0.65625\n",
      "2017-01-10T20:27:41.584497: step 177, loss 1.1387, acc 0.703125\n",
      "2017-01-10T20:27:43.575785: step 178, loss 1.0084, acc 0.8125\n",
      "2017-01-10T20:27:45.530114: step 179, loss 1.395, acc 0.664062\n",
      "2017-01-10T20:27:47.542781: step 180, loss 1.39951, acc 0.671875\n",
      "2017-01-10T20:27:49.579454: step 181, loss 1.30066, acc 0.6875\n",
      "2017-01-10T20:27:51.620528: step 182, loss 1.50119, acc 0.648438\n",
      "2017-01-10T20:27:53.687412: step 183, loss 1.31172, acc 0.710938\n",
      "2017-01-10T20:27:55.677217: step 184, loss 1.29779, acc 0.695312\n",
      "2017-01-10T20:27:57.653917: step 185, loss 1.04979, acc 0.734375\n",
      "2017-01-10T20:27:59.622554: step 186, loss 1.60208, acc 0.671875\n",
      "2017-01-10T20:28:01.632384: step 187, loss 1.53372, acc 0.617188\n",
      "2017-01-10T20:28:03.644019: step 188, loss 1.54871, acc 0.65625\n",
      "2017-01-10T20:28:05.680766: step 189, loss 1.52842, acc 0.648438\n",
      "2017-01-10T20:28:07.663186: step 190, loss 1.5802, acc 0.601562\n",
      "2017-01-10T20:28:09.636294: step 191, loss 1.16961, acc 0.742188\n",
      "2017-01-10T20:28:11.659066: step 192, loss 1.19938, acc 0.742188\n",
      "2017-01-10T20:28:13.686471: step 193, loss 1.27836, acc 0.695312\n",
      "2017-01-10T20:28:15.690899: step 194, loss 1.37057, acc 0.695312\n",
      "2017-01-10T20:28:17.664698: step 195, loss 1.06254, acc 0.75\n",
      "2017-01-10T20:28:19.712138: step 196, loss 0.912937, acc 0.796875\n",
      "2017-01-10T20:28:21.790461: step 197, loss 1.31897, acc 0.632812\n",
      "2017-01-10T20:28:23.732678: step 198, loss 1.19811, acc 0.671875\n",
      "2017-01-10T20:28:25.718288: step 199, loss 1.29876, acc 0.671875\n",
      "2017-01-10T20:28:27.715977: step 200, loss 1.06547, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:28:59.119247: step 200, loss 0.430159, acc 0.87396\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-200\n",
      "\n",
      "2017-01-10T20:29:03.693681: step 201, loss 1.59729, acc 0.609375\n",
      "2017-01-10T20:29:05.662924: step 202, loss 0.988445, acc 0.6875\n",
      "2017-01-10T20:29:07.646333: step 203, loss 1.37733, acc 0.664062\n",
      "2017-01-10T20:29:09.621337: step 204, loss 1.48805, acc 0.671875\n",
      "2017-01-10T20:29:11.603579: step 205, loss 1.27749, acc 0.6875\n",
      "2017-01-10T20:29:13.584173: step 206, loss 1.40741, acc 0.695312\n",
      "2017-01-10T20:29:15.583822: step 207, loss 1.28609, acc 0.703125\n",
      "2017-01-10T20:29:17.581027: step 208, loss 1.31635, acc 0.71875\n",
      "2017-01-10T20:29:19.582621: step 209, loss 0.989087, acc 0.789062\n",
      "2017-01-10T20:29:21.778388: step 210, loss 1.0504, acc 0.742188\n",
      "2017-01-10T20:29:23.748256: step 211, loss 1.37802, acc 0.679688\n",
      "2017-01-10T20:29:25.760482: step 212, loss 0.948295, acc 0.6875\n",
      "2017-01-10T20:29:27.753258: step 213, loss 1.0102, acc 0.734375\n",
      "2017-01-10T20:29:29.736905: step 214, loss 1.04032, acc 0.75\n",
      "2017-01-10T20:29:31.748663: step 215, loss 1.03858, acc 0.75\n",
      "2017-01-10T20:29:33.736432: step 216, loss 1.08478, acc 0.726562\n",
      "2017-01-10T20:29:35.703806: step 217, loss 1.34247, acc 0.703125\n",
      "2017-01-10T20:29:37.683218: step 218, loss 1.07862, acc 0.710938\n",
      "2017-01-10T20:29:39.646779: step 219, loss 1.13105, acc 0.710938\n",
      "2017-01-10T20:29:41.617195: step 220, loss 1.01014, acc 0.78125\n",
      "2017-01-10T20:29:43.595626: step 221, loss 1.01959, acc 0.726562\n",
      "2017-01-10T20:29:45.553752: step 222, loss 1.00615, acc 0.75\n",
      "2017-01-10T20:29:47.536267: step 223, loss 1.03802, acc 0.710938\n",
      "2017-01-10T20:29:49.523029: step 224, loss 1.18846, acc 0.71875\n",
      "2017-01-10T20:29:51.587011: step 225, loss 1.21755, acc 0.65625\n",
      "2017-01-10T20:29:53.685052: step 226, loss 0.79333, acc 0.789062\n",
      "2017-01-10T20:29:55.674716: step 227, loss 1.04945, acc 0.734375\n",
      "2017-01-10T20:29:57.653439: step 228, loss 1.00369, acc 0.765625\n",
      "2017-01-10T20:29:59.679320: step 229, loss 1.19137, acc 0.75\n",
      "2017-01-10T20:30:01.670801: step 230, loss 0.920832, acc 0.78125\n",
      "2017-01-10T20:30:03.684001: step 231, loss 0.951583, acc 0.765625\n",
      "2017-01-10T20:30:05.681746: step 232, loss 1.04609, acc 0.757812\n",
      "2017-01-10T20:30:07.672627: step 233, loss 1.20794, acc 0.734375\n",
      "2017-01-10T20:30:09.641190: step 234, loss 0.997316, acc 0.757812\n",
      "2017-01-10T20:30:11.639138: step 235, loss 1.16464, acc 0.710938\n",
      "2017-01-10T20:30:13.617678: step 236, loss 0.900493, acc 0.734375\n",
      "2017-01-10T20:30:15.577580: step 237, loss 1.14238, acc 0.710938\n",
      "2017-01-10T20:30:17.572737: step 238, loss 0.981018, acc 0.773438\n",
      "2017-01-10T20:30:19.612803: step 239, loss 0.748677, acc 0.804688\n",
      "2017-01-10T20:30:21.690558: step 240, loss 1.05356, acc 0.734375\n",
      "2017-01-10T20:30:23.663768: step 241, loss 1.04145, acc 0.726562\n",
      "2017-01-10T20:30:25.817111: step 242, loss 1.05874, acc 0.757812\n",
      "2017-01-10T20:30:27.835821: step 243, loss 0.803808, acc 0.726562\n",
      "2017-01-10T20:30:29.812073: step 244, loss 1.01939, acc 0.734375\n",
      "2017-01-10T20:30:31.786810: step 245, loss 0.706787, acc 0.78125\n",
      "2017-01-10T20:30:33.789204: step 246, loss 0.803117, acc 0.789062\n",
      "2017-01-10T20:30:35.777707: step 247, loss 0.735858, acc 0.8125\n",
      "2017-01-10T20:30:37.749842: step 248, loss 0.969382, acc 0.71875\n",
      "2017-01-10T20:30:39.748955: step 249, loss 0.950335, acc 0.757812\n",
      "2017-01-10T20:30:41.707293: step 250, loss 0.968243, acc 0.734375\n",
      "2017-01-10T20:30:43.698736: step 251, loss 1.03878, acc 0.71875\n",
      "2017-01-10T20:30:45.658376: step 252, loss 0.772297, acc 0.804688\n",
      "2017-01-10T20:30:47.658169: step 253, loss 0.709235, acc 0.765625\n",
      "2017-01-10T20:30:49.640822: step 254, loss 1.10005, acc 0.695312\n",
      "2017-01-10T20:30:51.712687: step 255, loss 0.859786, acc 0.765625\n",
      "2017-01-10T20:30:53.703252: step 256, loss 0.976161, acc 0.78125\n",
      "2017-01-10T20:30:55.688274: step 257, loss 0.99544, acc 0.726562\n",
      "2017-01-10T20:30:57.824329: step 258, loss 0.762038, acc 0.765625\n",
      "2017-01-10T20:30:59.806932: step 259, loss 1.11762, acc 0.695312\n",
      "2017-01-10T20:31:01.823282: step 260, loss 1.29569, acc 0.679688\n",
      "2017-01-10T20:31:03.821255: step 261, loss 0.980322, acc 0.71875\n",
      "2017-01-10T20:31:05.796543: step 262, loss 0.865511, acc 0.789062\n",
      "2017-01-10T20:31:07.818168: step 263, loss 0.94037, acc 0.75\n",
      "2017-01-10T20:31:09.785756: step 264, loss 1.13121, acc 0.734375\n",
      "2017-01-10T20:31:11.784926: step 265, loss 1.20194, acc 0.734375\n",
      "2017-01-10T20:31:13.785218: step 266, loss 0.862852, acc 0.773438\n",
      "2017-01-10T20:31:15.769800: step 267, loss 0.930283, acc 0.734375\n",
      "2017-01-10T20:31:17.738358: step 268, loss 0.713098, acc 0.78125\n",
      "2017-01-10T20:31:19.744834: step 269, loss 0.897027, acc 0.765625\n",
      "2017-01-10T20:31:21.797496: step 270, loss 1.01787, acc 0.75\n",
      "2017-01-10T20:31:23.812896: step 271, loss 0.852313, acc 0.75\n",
      "2017-01-10T20:31:25.742186: step 272, loss 0.881227, acc 0.804688\n",
      "2017-01-10T20:31:27.735686: step 273, loss 0.963111, acc 0.726562\n",
      "2017-01-10T20:31:30.426701: step 274, loss 0.973455, acc 0.726562\n",
      "2017-01-10T20:31:32.429773: step 275, loss 0.68311, acc 0.820312\n",
      "2017-01-10T20:31:34.437394: step 276, loss 0.833945, acc 0.773438\n",
      "2017-01-10T20:31:36.435037: step 277, loss 0.807329, acc 0.773438\n",
      "2017-01-10T20:31:38.449908: step 278, loss 0.99836, acc 0.71875\n",
      "2017-01-10T20:31:40.395301: step 279, loss 0.841521, acc 0.765625\n",
      "2017-01-10T20:31:42.364531: step 280, loss 1.16306, acc 0.734375\n",
      "2017-01-10T20:31:44.341863: step 281, loss 0.576402, acc 0.835938\n",
      "2017-01-10T20:31:47.692438: step 282, loss 0.786317, acc 0.796875\n",
      "2017-01-10T20:31:50.290535: step 283, loss 0.734903, acc 0.789062\n",
      "2017-01-10T20:31:52.337688: step 284, loss 1.06332, acc 0.71875\n",
      "2017-01-10T20:31:54.285374: step 285, loss 0.600069, acc 0.820312\n",
      "2017-01-10T20:31:56.307836: step 286, loss 0.643157, acc 0.828125\n",
      "2017-01-10T20:31:58.461691: step 287, loss 0.912743, acc 0.773438\n",
      "2017-01-10T20:32:00.496564: step 288, loss 0.935257, acc 0.773438\n",
      "2017-01-10T20:32:02.611980: step 289, loss 0.744749, acc 0.789062\n",
      "2017-01-10T20:32:04.644810: step 290, loss 0.910048, acc 0.71875\n",
      "2017-01-10T20:32:06.583608: step 291, loss 0.859333, acc 0.796875\n",
      "2017-01-10T20:32:08.610395: step 292, loss 0.929095, acc 0.742188\n",
      "2017-01-10T20:32:10.584295: step 293, loss 0.74664, acc 0.789062\n",
      "2017-01-10T20:32:12.635773: step 294, loss 0.960871, acc 0.75\n",
      "2017-01-10T20:32:15.587640: step 295, loss 0.791262, acc 0.804688\n",
      "2017-01-10T20:32:17.536805: step 296, loss 0.891969, acc 0.773438\n",
      "2017-01-10T20:32:19.540661: step 297, loss 1.2017, acc 0.679688\n",
      "2017-01-10T20:32:21.585036: step 298, loss 0.880583, acc 0.757812\n",
      "2017-01-10T20:32:23.558738: step 299, loss 0.5449, acc 0.835938\n",
      "2017-01-10T20:32:25.578102: step 300, loss 0.772444, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:32:52.358196: step 300, loss 0.275974, acc 0.92376\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-300\n",
      "\n",
      "2017-01-10T20:32:56.737772: step 301, loss 0.53228, acc 0.835938\n",
      "2017-01-10T20:32:58.729943: step 302, loss 0.562048, acc 0.796875\n",
      "2017-01-10T20:33:00.755689: step 303, loss 0.870724, acc 0.78125\n",
      "2017-01-10T20:33:02.784993: step 304, loss 0.893763, acc 0.757812\n",
      "2017-01-10T20:33:04.840163: step 305, loss 0.994077, acc 0.75\n",
      "2017-01-10T20:33:07.091369: step 306, loss 0.685223, acc 0.8125\n",
      "2017-01-10T20:33:09.119358: step 307, loss 1.06916, acc 0.742188\n",
      "2017-01-10T20:33:11.101658: step 308, loss 0.974483, acc 0.75\n",
      "2017-01-10T20:33:13.089961: step 309, loss 0.693663, acc 0.828125\n",
      "2017-01-10T20:33:15.102227: step 310, loss 0.688063, acc 0.835938\n",
      "2017-01-10T20:33:17.107317: step 311, loss 0.991772, acc 0.742188\n",
      "2017-01-10T20:33:19.097646: step 312, loss 0.755871, acc 0.796875\n",
      "2017-01-10T20:33:21.081274: step 313, loss 0.798838, acc 0.796875\n",
      "2017-01-10T20:33:23.076959: step 314, loss 0.802196, acc 0.757812\n",
      "2017-01-10T20:33:25.074575: step 315, loss 0.829752, acc 0.75\n",
      "2017-01-10T20:33:27.088692: step 316, loss 0.632107, acc 0.828125\n",
      "2017-01-10T20:33:29.100484: step 317, loss 0.648218, acc 0.804688\n",
      "2017-01-10T20:33:31.091741: step 318, loss 0.65162, acc 0.84375\n",
      "2017-01-10T20:33:33.112553: step 319, loss 0.619141, acc 0.828125\n",
      "2017-01-10T20:33:35.126030: step 320, loss 0.587658, acc 0.851562\n",
      "2017-01-10T20:33:37.097464: step 321, loss 0.63824, acc 0.828125\n",
      "2017-01-10T20:33:39.243648: step 322, loss 0.716813, acc 0.804688\n",
      "2017-01-10T20:33:41.199295: step 323, loss 0.700119, acc 0.8125\n",
      "2017-01-10T20:33:43.221201: step 324, loss 0.800125, acc 0.789062\n",
      "2017-01-10T20:33:45.242880: step 325, loss 1.02956, acc 0.734375\n",
      "2017-01-10T20:33:47.218055: step 326, loss 0.716702, acc 0.78125\n",
      "2017-01-10T20:33:49.259407: step 327, loss 0.562734, acc 0.828125\n",
      "2017-01-10T20:33:51.305646: step 328, loss 0.829263, acc 0.789062\n",
      "2017-01-10T20:33:53.319098: step 329, loss 0.764392, acc 0.78125\n",
      "2017-01-10T20:33:55.308182: step 330, loss 0.641846, acc 0.835938\n",
      "2017-01-10T20:33:57.306244: step 331, loss 0.686352, acc 0.828125\n",
      "2017-01-10T20:33:59.321471: step 332, loss 1.00997, acc 0.75\n",
      "2017-01-10T20:34:01.346354: step 333, loss 0.77454, acc 0.804688\n",
      "2017-01-10T20:34:03.357016: step 334, loss 0.473155, acc 0.828125\n",
      "2017-01-10T20:34:05.368925: step 335, loss 0.808741, acc 0.78125\n",
      "2017-01-10T20:34:07.328362: step 336, loss 1.10295, acc 0.757812\n",
      "2017-01-10T20:34:09.336559: step 337, loss 0.604483, acc 0.828125\n",
      "2017-01-10T20:34:11.500988: step 338, loss 0.503997, acc 0.859375\n",
      "2017-01-10T20:34:13.492214: step 339, loss 0.593771, acc 0.820312\n",
      "2017-01-10T20:34:15.490087: step 340, loss 0.51044, acc 0.828125\n",
      "2017-01-10T20:34:17.502206: step 341, loss 0.743707, acc 0.828125\n",
      "2017-01-10T20:34:19.506014: step 342, loss 0.908386, acc 0.75\n",
      "2017-01-10T20:34:21.578437: step 343, loss 0.588322, acc 0.851562\n",
      "2017-01-10T20:34:23.581938: step 344, loss 0.805928, acc 0.820312\n",
      "2017-01-10T20:34:25.566542: step 345, loss 0.594018, acc 0.8125\n",
      "2017-01-10T20:34:27.518585: step 346, loss 0.859657, acc 0.773438\n",
      "2017-01-10T20:34:29.484570: step 347, loss 0.677821, acc 0.804688\n",
      "2017-01-10T20:34:31.482698: step 348, loss 0.623341, acc 0.804688\n",
      "2017-01-10T20:34:33.484033: step 349, loss 0.769184, acc 0.789062\n",
      "2017-01-10T20:34:35.514074: step 350, loss 0.618004, acc 0.796875\n",
      "2017-01-10T20:34:37.514406: step 351, loss 0.565213, acc 0.835938\n",
      "2017-01-10T20:34:39.513118: step 352, loss 0.461003, acc 0.84375\n",
      "2017-01-10T20:34:41.513863: step 353, loss 0.936111, acc 0.765625\n",
      "2017-01-10T20:34:43.703323: step 354, loss 0.742342, acc 0.757812\n",
      "2017-01-10T20:34:45.714621: step 355, loss 0.872355, acc 0.789062\n",
      "2017-01-10T20:34:47.742559: step 356, loss 0.594046, acc 0.820312\n",
      "2017-01-10T20:34:49.769671: step 357, loss 0.769769, acc 0.789062\n",
      "2017-01-10T20:34:51.820100: step 358, loss 0.592857, acc 0.804688\n",
      "2017-01-10T20:34:53.822228: step 359, loss 0.989632, acc 0.71875\n",
      "2017-01-10T20:34:55.819141: step 360, loss 0.601537, acc 0.804688\n",
      "2017-01-10T20:34:57.794504: step 361, loss 0.701674, acc 0.835938\n",
      "2017-01-10T20:34:59.750925: step 362, loss 0.628294, acc 0.8125\n",
      "2017-01-10T20:35:01.762172: step 363, loss 0.37784, acc 0.90625\n",
      "2017-01-10T20:35:03.735980: step 364, loss 0.552866, acc 0.851562\n",
      "2017-01-10T20:35:05.719126: step 365, loss 0.478174, acc 0.8125\n",
      "2017-01-10T20:35:07.730307: step 366, loss 0.784768, acc 0.828125\n",
      "2017-01-10T20:35:09.742987: step 367, loss 0.746447, acc 0.765625\n",
      "2017-01-10T20:35:11.706732: step 368, loss 0.793264, acc 0.8125\n",
      "2017-01-10T20:35:13.725247: step 369, loss 0.822645, acc 0.773438\n",
      "2017-01-10T20:35:15.869188: step 370, loss 0.579912, acc 0.859375\n",
      "2017-01-10T20:35:17.836378: step 371, loss 0.466965, acc 0.875\n",
      "2017-01-10T20:35:19.861793: step 372, loss 0.87688, acc 0.804688\n",
      "2017-01-10T20:35:21.905981: step 373, loss 0.722422, acc 0.828125\n",
      "2017-01-10T20:35:23.867658: step 374, loss 0.654192, acc 0.820312\n",
      "2017-01-10T20:35:25.832578: step 375, loss 0.341957, acc 0.914062\n",
      "2017-01-10T20:35:27.861304: step 376, loss 0.638336, acc 0.828125\n",
      "2017-01-10T20:35:29.859623: step 377, loss 0.685593, acc 0.804688\n",
      "2017-01-10T20:35:31.847250: step 378, loss 0.737148, acc 0.835938\n",
      "2017-01-10T20:35:33.790183: step 379, loss 0.702737, acc 0.804688\n",
      "2017-01-10T20:35:35.726122: step 380, loss 0.513398, acc 0.867188\n",
      "2017-01-10T20:35:37.682413: step 381, loss 0.897856, acc 0.765625\n",
      "2017-01-10T20:35:39.614469: step 382, loss 0.585845, acc 0.828125\n",
      "2017-01-10T20:35:41.629870: step 383, loss 0.689452, acc 0.804688\n",
      "2017-01-10T20:35:43.581303: step 384, loss 0.51793, acc 0.867188\n",
      "2017-01-10T20:35:45.565075: step 385, loss 0.536548, acc 0.84375\n",
      "2017-01-10T20:35:47.726200: step 386, loss 0.430587, acc 0.851562\n",
      "2017-01-10T20:35:49.728950: step 387, loss 0.498828, acc 0.859375\n",
      "2017-01-10T20:35:51.779766: step 388, loss 0.64648, acc 0.84375\n",
      "2017-01-10T20:35:53.804371: step 389, loss 0.716009, acc 0.804688\n",
      "2017-01-10T20:35:55.826950: step 390, loss 0.772299, acc 0.789062\n",
      "2017-01-10T20:35:57.849036: step 391, loss 0.755843, acc 0.773438\n",
      "2017-01-10T20:35:59.780115: step 392, loss 0.581341, acc 0.820312\n",
      "2017-01-10T20:36:01.756209: step 393, loss 0.723651, acc 0.828125\n",
      "2017-01-10T20:36:03.765294: step 394, loss 0.566791, acc 0.859375\n",
      "2017-01-10T20:36:05.783403: step 395, loss 0.797905, acc 0.820312\n",
      "2017-01-10T20:36:07.814927: step 396, loss 0.87912, acc 0.773438\n",
      "2017-01-10T20:36:09.782757: step 397, loss 0.443508, acc 0.867188\n",
      "2017-01-10T20:36:11.724205: step 398, loss 0.722328, acc 0.828125\n",
      "2017-01-10T20:36:13.732683: step 399, loss 0.609464, acc 0.8125\n",
      "2017-01-10T20:36:15.746991: step 400, loss 0.446223, acc 0.882812\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:36:43.390184: step 400, loss 0.215465, acc 0.94236\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-400\n",
      "\n",
      "2017-01-10T20:36:48.961890: step 401, loss 0.62149, acc 0.8125\n",
      "2017-01-10T20:36:51.120757: step 402, loss 0.527096, acc 0.835938\n",
      "2017-01-10T20:36:53.278373: step 403, loss 0.53063, acc 0.84375\n",
      "2017-01-10T20:36:55.252611: step 404, loss 0.569487, acc 0.84375\n",
      "2017-01-10T20:36:57.248524: step 405, loss 0.529786, acc 0.84375\n",
      "2017-01-10T20:36:59.294718: step 406, loss 0.418731, acc 0.875\n",
      "2017-01-10T20:37:01.303376: step 407, loss 0.922287, acc 0.757812\n",
      "2017-01-10T20:37:03.318221: step 408, loss 0.585466, acc 0.851562\n",
      "2017-01-10T20:37:05.292210: step 409, loss 0.597462, acc 0.859375\n",
      "2017-01-10T20:37:07.270595: step 410, loss 0.497587, acc 0.835938\n",
      "2017-01-10T20:37:09.278491: step 411, loss 0.542462, acc 0.8125\n",
      "2017-01-10T20:37:11.297599: step 412, loss 0.561026, acc 0.851562\n",
      "2017-01-10T20:37:13.323871: step 413, loss 0.558447, acc 0.851562\n",
      "2017-01-10T20:37:15.335015: step 414, loss 0.532292, acc 0.84375\n",
      "2017-01-10T20:37:17.335326: step 415, loss 0.44542, acc 0.859375\n",
      "2017-01-10T20:37:19.362720: step 416, loss 0.650152, acc 0.804688\n",
      "2017-01-10T20:37:21.402533: step 417, loss 0.543937, acc 0.828125\n",
      "2017-01-10T20:37:23.426578: step 418, loss 0.436732, acc 0.867188\n",
      "2017-01-10T20:37:25.568773: step 419, loss 0.558059, acc 0.851562\n",
      "2017-01-10T20:37:27.546402: step 420, loss 0.649524, acc 0.828125\n",
      "2017-01-10T20:37:29.685211: step 421, loss 0.408769, acc 0.835938\n",
      "2017-01-10T20:37:31.696112: step 422, loss 0.421598, acc 0.859375\n",
      "2017-01-10T20:37:33.698611: step 423, loss 0.650234, acc 0.804688\n",
      "2017-01-10T20:37:35.693541: step 424, loss 0.636333, acc 0.828125\n",
      "2017-01-10T20:37:37.708738: step 425, loss 0.707209, acc 0.820312\n",
      "2017-01-10T20:37:39.710263: step 426, loss 0.649398, acc 0.796875\n",
      "2017-01-10T20:37:41.691738: step 427, loss 0.582817, acc 0.8125\n",
      "2017-01-10T20:37:43.675019: step 428, loss 0.613094, acc 0.828125\n",
      "2017-01-10T20:37:46.530235: step 429, loss 0.620452, acc 0.84375\n",
      "2017-01-10T20:37:48.618760: step 430, loss 0.521317, acc 0.867188\n",
      "2017-01-10T20:37:50.692056: step 431, loss 0.421405, acc 0.859375\n",
      "2017-01-10T20:37:52.676362: step 432, loss 0.564553, acc 0.8125\n",
      "2017-01-10T20:37:54.732001: step 433, loss 0.4753, acc 0.835938\n",
      "2017-01-10T20:37:57.164908: step 434, loss 0.363924, acc 0.882812\n",
      "2017-01-10T20:37:59.401571: step 435, loss 0.575699, acc 0.835938\n",
      "2017-01-10T20:38:01.500643: step 436, loss 0.3287, acc 0.898438\n",
      "2017-01-10T20:38:03.587541: step 437, loss 0.426488, acc 0.84375\n",
      "2017-01-10T20:38:05.650648: step 438, loss 0.41666, acc 0.84375\n",
      "2017-01-10T20:38:07.653591: step 439, loss 0.684867, acc 0.820312\n",
      "2017-01-10T20:38:09.620323: step 440, loss 0.635449, acc 0.796875\n",
      "2017-01-10T20:38:11.609898: step 441, loss 0.349681, acc 0.90625\n",
      "2017-01-10T20:38:13.589243: step 442, loss 0.524538, acc 0.828125\n",
      "2017-01-10T20:38:15.560795: step 443, loss 0.455742, acc 0.851562\n",
      "2017-01-10T20:38:17.539602: step 444, loss 0.334846, acc 0.898438\n",
      "2017-01-10T20:38:19.531042: step 445, loss 0.380439, acc 0.890625\n",
      "2017-01-10T20:38:21.578653: step 446, loss 0.424927, acc 0.851562\n",
      "2017-01-10T20:38:23.564356: step 447, loss 0.36848, acc 0.875\n",
      "2017-01-10T20:38:25.562683: step 448, loss 0.464675, acc 0.875\n",
      "2017-01-10T20:38:27.551653: step 449, loss 0.688652, acc 0.804688\n",
      "2017-01-10T20:38:29.661784: step 450, loss 0.37165, acc 0.867188\n",
      "2017-01-10T20:38:31.668612: step 451, loss 0.767885, acc 0.804688\n",
      "2017-01-10T20:38:33.643242: step 452, loss 0.560859, acc 0.835938\n",
      "2017-01-10T20:38:35.860589: step 453, loss 0.327195, acc 0.875\n",
      "2017-01-10T20:38:37.863732: step 454, loss 0.63207, acc 0.8125\n",
      "2017-01-10T20:38:39.870301: step 455, loss 0.424921, acc 0.84375\n",
      "2017-01-10T20:38:41.853319: step 456, loss 0.39971, acc 0.867188\n",
      "2017-01-10T20:38:44.249168: step 457, loss 0.560213, acc 0.84375\n",
      "2017-01-10T20:38:46.888684: step 458, loss 0.542639, acc 0.835938\n",
      "2017-01-10T20:38:48.884909: step 459, loss 0.37437, acc 0.882812\n",
      "2017-01-10T20:38:51.584260: step 460, loss 0.499758, acc 0.84375\n",
      "2017-01-10T20:38:53.700510: step 461, loss 0.413338, acc 0.867188\n",
      "2017-01-10T20:38:55.696382: step 462, loss 0.460215, acc 0.875\n",
      "2017-01-10T20:38:57.676348: step 463, loss 0.412779, acc 0.875\n",
      "2017-01-10T20:38:59.710873: step 464, loss 0.414244, acc 0.890625\n",
      "2017-01-10T20:39:01.887846: step 465, loss 0.453497, acc 0.867188\n",
      "2017-01-10T20:39:03.925305: step 466, loss 0.668438, acc 0.820312\n",
      "2017-01-10T20:39:05.891503: step 467, loss 0.484209, acc 0.851562\n",
      "2017-01-10T20:39:07.912961: step 468, loss 0.756652, acc 0.78125\n",
      "2017-01-10T20:39:09.926841: step 469, loss 0.679574, acc 0.851562\n",
      "2017-01-10T20:39:11.925585: step 470, loss 0.453175, acc 0.867188\n",
      "2017-01-10T20:39:13.943852: step 471, loss 0.474644, acc 0.851562\n",
      "2017-01-10T20:39:15.893687: step 472, loss 0.558753, acc 0.851562\n",
      "2017-01-10T20:39:17.911449: step 473, loss 0.6011, acc 0.8125\n",
      "2017-01-10T20:39:19.946608: step 474, loss 0.607323, acc 0.804688\n",
      "2017-01-10T20:39:21.990243: step 475, loss 0.423943, acc 0.875\n",
      "2017-01-10T20:39:23.990299: step 476, loss 0.484807, acc 0.84375\n",
      "2017-01-10T20:39:25.979677: step 477, loss 0.429878, acc 0.890625\n",
      "2017-01-10T20:39:27.974796: step 478, loss 0.445726, acc 0.851562\n",
      "2017-01-10T20:39:29.968341: step 479, loss 0.717795, acc 0.820312\n",
      "2017-01-10T20:39:31.966242: step 480, loss 0.458713, acc 0.867188\n",
      "2017-01-10T20:39:34.134508: step 481, loss 0.56076, acc 0.828125\n",
      "2017-01-10T20:39:36.155687: step 482, loss 0.430972, acc 0.882812\n",
      "2017-01-10T20:39:38.157042: step 483, loss 0.454023, acc 0.820312\n",
      "2017-01-10T20:39:40.173824: step 484, loss 0.562388, acc 0.84375\n",
      "2017-01-10T20:39:42.173755: step 485, loss 0.45878, acc 0.859375\n",
      "2017-01-10T20:39:44.193715: step 486, loss 0.410585, acc 0.84375\n",
      "2017-01-10T20:39:46.185990: step 487, loss 0.433071, acc 0.859375\n",
      "2017-01-10T20:39:48.145651: step 488, loss 0.253452, acc 0.929688\n",
      "2017-01-10T20:39:50.174602: step 489, loss 0.558948, acc 0.828125\n",
      "2017-01-10T20:39:52.243085: step 490, loss 0.479984, acc 0.875\n",
      "2017-01-10T20:39:54.228777: step 491, loss 0.39557, acc 0.882812\n",
      "2017-01-10T20:39:56.227970: step 492, loss 0.459725, acc 0.882812\n",
      "2017-01-10T20:39:58.277713: step 493, loss 0.484484, acc 0.867188\n",
      "2017-01-10T20:40:00.262605: step 494, loss 0.563711, acc 0.835938\n",
      "2017-01-10T20:40:02.281097: step 495, loss 0.43793, acc 0.859375\n",
      "2017-01-10T20:40:04.357810: step 496, loss 0.460705, acc 0.84375\n",
      "2017-01-10T20:40:06.469842: step 497, loss 0.388103, acc 0.875\n",
      "2017-01-10T20:40:08.472815: step 498, loss 0.533454, acc 0.835938\n",
      "2017-01-10T20:40:10.443995: step 499, loss 0.4435, acc 0.859375\n",
      "2017-01-10T20:40:12.419430: step 500, loss 0.310894, acc 0.914062\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:40:44.512642: step 500, loss 0.189876, acc 0.9462\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-500\n",
      "\n",
      "2017-01-10T20:40:49.588094: step 501, loss 0.345126, acc 0.898438\n",
      "2017-01-10T20:40:51.630790: step 502, loss 0.394875, acc 0.890625\n",
      "2017-01-10T20:40:53.640773: step 503, loss 0.422831, acc 0.867188\n",
      "2017-01-10T20:40:55.649161: step 504, loss 0.297546, acc 0.898438\n",
      "2017-01-10T20:40:57.657636: step 505, loss 0.580665, acc 0.84375\n",
      "2017-01-10T20:40:59.631291: step 506, loss 0.387776, acc 0.890625\n",
      "2017-01-10T20:41:01.647526: step 507, loss 0.617405, acc 0.828125\n",
      "2017-01-10T20:41:03.645963: step 508, loss 0.59572, acc 0.828125\n",
      "2017-01-10T20:41:05.659092: step 509, loss 0.685998, acc 0.828125\n",
      "2017-01-10T20:41:07.639302: step 510, loss 0.407934, acc 0.875\n",
      "2017-01-10T20:41:09.636822: step 511, loss 0.573001, acc 0.8125\n",
      "2017-01-10T20:41:11.789540: step 512, loss 0.631353, acc 0.828125\n",
      "2017-01-10T20:41:13.958707: step 513, loss 0.340941, acc 0.867188\n",
      "2017-01-10T20:41:15.958549: step 514, loss 0.330347, acc 0.90625\n",
      "2017-01-10T20:41:17.941352: step 515, loss 0.524104, acc 0.867188\n",
      "2017-01-10T20:41:19.900646: step 516, loss 0.474854, acc 0.875\n",
      "2017-01-10T20:41:21.971617: step 517, loss 0.523059, acc 0.867188\n",
      "2017-01-10T20:41:23.975654: step 518, loss 0.327446, acc 0.898438\n",
      "2017-01-10T20:41:25.955291: step 519, loss 0.580381, acc 0.8125\n",
      "2017-01-10T20:41:27.983290: step 520, loss 0.475441, acc 0.8125\n",
      "2017-01-10T20:41:30.257960: step 521, loss 0.43087, acc 0.859375\n",
      "2017-01-10T20:41:32.317718: step 522, loss 0.295755, acc 0.90625\n",
      "2017-01-10T20:41:34.311318: step 523, loss 0.406468, acc 0.882812\n",
      "2017-01-10T20:41:36.290764: step 524, loss 0.388683, acc 0.898438\n",
      "2017-01-10T20:41:38.304019: step 525, loss 0.391564, acc 0.867188\n",
      "2017-01-10T20:41:40.291748: step 526, loss 0.493484, acc 0.867188\n",
      "2017-01-10T20:41:42.291662: step 527, loss 0.40892, acc 0.882812\n",
      "2017-01-10T20:41:44.348323: step 528, loss 0.328855, acc 0.890625\n",
      "2017-01-10T20:41:47.590765: step 529, loss 0.286307, acc 0.882812\n",
      "2017-01-10T20:41:49.603394: step 530, loss 0.31928, acc 0.890625\n",
      "2017-01-10T20:41:51.673315: step 531, loss 0.402593, acc 0.898438\n",
      "2017-01-10T20:41:53.665444: step 532, loss 0.46757, acc 0.875\n",
      "2017-01-10T20:41:55.689536: step 533, loss 0.502501, acc 0.835938\n",
      "2017-01-10T20:41:57.630480: step 534, loss 0.41813, acc 0.835938\n",
      "2017-01-10T20:41:59.601496: step 535, loss 0.438413, acc 0.921875\n",
      "2017-01-10T20:42:01.605181: step 536, loss 0.397941, acc 0.875\n",
      "2017-01-10T20:42:03.609257: step 537, loss 0.380368, acc 0.875\n",
      "2017-01-10T20:42:05.618227: step 538, loss 0.352042, acc 0.882812\n",
      "2017-01-10T20:42:07.636637: step 539, loss 0.418584, acc 0.859375\n",
      "2017-01-10T20:42:09.677549: step 540, loss 0.519473, acc 0.859375\n",
      "2017-01-10T20:42:11.654370: step 541, loss 0.410941, acc 0.867188\n",
      "2017-01-10T20:42:13.585967: step 542, loss 0.31949, acc 0.90625\n",
      "2017-01-10T20:42:15.554237: step 543, loss 0.502282, acc 0.851562\n",
      "2017-01-10T20:42:17.562002: step 544, loss 0.549674, acc 0.835938\n",
      "2017-01-10T20:42:19.791906: step 545, loss 0.406201, acc 0.867188\n",
      "2017-01-10T20:42:21.826064: step 546, loss 0.444485, acc 0.867188\n",
      "2017-01-10T20:42:23.790146: step 547, loss 0.735717, acc 0.78125\n",
      "2017-01-10T20:42:25.779802: step 548, loss 0.474782, acc 0.875\n",
      "2017-01-10T20:42:27.747171: step 549, loss 0.315678, acc 0.882812\n",
      "2017-01-10T20:42:29.759793: step 550, loss 0.430625, acc 0.859375\n",
      "2017-01-10T20:42:31.737758: step 551, loss 0.443816, acc 0.835938\n",
      "2017-01-10T20:42:33.696402: step 552, loss 0.341738, acc 0.890625\n",
      "2017-01-10T20:42:35.670094: step 553, loss 0.454036, acc 0.890625\n",
      "2017-01-10T20:42:37.674910: step 554, loss 0.348328, acc 0.875\n",
      "2017-01-10T20:42:39.670281: step 555, loss 0.264623, acc 0.929688\n",
      "2017-01-10T20:42:41.671452: step 556, loss 0.473839, acc 0.859375\n",
      "2017-01-10T20:42:43.647231: step 557, loss 0.562351, acc 0.851562\n",
      "2017-01-10T20:42:45.642086: step 558, loss 0.289503, acc 0.90625\n",
      "2017-01-10T20:42:47.627910: step 559, loss 0.435057, acc 0.882812\n",
      "2017-01-10T20:42:49.670901: step 560, loss 0.339414, acc 0.898438\n",
      "2017-01-10T20:42:51.892337: step 561, loss 0.343832, acc 0.875\n",
      "2017-01-10T20:42:53.907269: step 562, loss 0.388784, acc 0.890625\n",
      "2017-01-10T20:42:55.931364: step 563, loss 0.410819, acc 0.890625\n",
      "2017-01-10T20:42:57.905839: step 564, loss 0.471235, acc 0.867188\n",
      "2017-01-10T20:42:59.908608: step 565, loss 0.278714, acc 0.914062\n",
      "2017-01-10T20:43:01.929679: step 566, loss 0.360807, acc 0.898438\n",
      "2017-01-10T20:43:03.950097: step 567, loss 0.43246, acc 0.875\n",
      "2017-01-10T20:43:05.938196: step 568, loss 0.424829, acc 0.882812\n",
      "2017-01-10T20:43:07.941607: step 569, loss 0.483353, acc 0.828125\n",
      "2017-01-10T20:43:09.901420: step 570, loss 0.339095, acc 0.882812\n",
      "2017-01-10T20:43:11.919197: step 571, loss 0.23236, acc 0.914062\n",
      "2017-01-10T20:43:13.918076: step 572, loss 0.316347, acc 0.914062\n",
      "2017-01-10T20:43:15.947895: step 573, loss 0.653628, acc 0.828125\n",
      "2017-01-10T20:43:17.993375: step 574, loss 0.389998, acc 0.875\n",
      "2017-01-10T20:43:19.980942: step 575, loss 0.376093, acc 0.882812\n",
      "2017-01-10T20:43:22.037119: step 576, loss 0.513108, acc 0.84375\n",
      "2017-01-10T20:43:24.212864: step 577, loss 0.313227, acc 0.890625\n",
      "2017-01-10T20:43:26.236459: step 578, loss 0.322089, acc 0.898438\n",
      "2017-01-10T20:43:28.238108: step 579, loss 0.343018, acc 0.898438\n",
      "2017-01-10T20:43:30.219518: step 580, loss 0.504958, acc 0.859375\n",
      "2017-01-10T20:43:32.233009: step 581, loss 0.418785, acc 0.882812\n",
      "2017-01-10T20:43:34.301527: step 582, loss 0.296554, acc 0.882812\n",
      "2017-01-10T20:43:36.380279: step 583, loss 0.361157, acc 0.882812\n",
      "2017-01-10T20:43:38.359955: step 584, loss 0.264969, acc 0.90625\n",
      "2017-01-10T20:43:40.361556: step 585, loss 0.388913, acc 0.875\n",
      "2017-01-10T20:43:42.332376: step 586, loss 0.358354, acc 0.890625\n",
      "2017-01-10T20:43:44.344972: step 587, loss 0.179138, acc 0.9375\n",
      "2017-01-10T20:43:46.360611: step 588, loss 0.407296, acc 0.882812\n",
      "2017-01-10T20:43:48.377139: step 589, loss 0.496596, acc 0.875\n",
      "2017-01-10T20:43:50.933346: step 590, loss 0.430753, acc 0.90625\n",
      "2017-01-10T20:43:53.285610: step 591, loss 0.327832, acc 0.898438\n",
      "2017-01-10T20:43:55.426982: step 592, loss 0.41435, acc 0.882812\n",
      "2017-01-10T20:43:57.434744: step 593, loss 0.588688, acc 0.804688\n",
      "2017-01-10T20:43:59.394374: step 594, loss 0.459218, acc 0.898438\n",
      "2017-01-10T20:44:01.414253: step 595, loss 0.371615, acc 0.90625\n",
      "2017-01-10T20:44:03.489089: step 596, loss 0.403589, acc 0.882812\n",
      "2017-01-10T20:44:05.502184: step 597, loss 0.445769, acc 0.875\n",
      "2017-01-10T20:44:07.531165: step 598, loss 0.435222, acc 0.875\n",
      "2017-01-10T20:44:09.532992: step 599, loss 0.453559, acc 0.851562\n",
      "2017-01-10T20:44:11.529908: step 600, loss 0.47796, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:44:33.341261: step 600, loss 0.175312, acc 0.95116\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-600\n",
      "\n",
      "2017-01-10T20:44:38.044722: step 601, loss 0.435436, acc 0.84375\n",
      "2017-01-10T20:44:40.045459: step 602, loss 0.468552, acc 0.851562\n",
      "2017-01-10T20:44:42.116640: step 603, loss 0.434354, acc 0.921875\n",
      "2017-01-10T20:44:44.146287: step 604, loss 0.733354, acc 0.8125\n",
      "2017-01-10T20:44:46.172638: step 605, loss 0.611565, acc 0.828125\n",
      "2017-01-10T20:44:48.170864: step 606, loss 0.399092, acc 0.859375\n",
      "2017-01-10T20:44:50.228033: step 607, loss 0.339288, acc 0.890625\n",
      "2017-01-10T20:44:52.306817: step 608, loss 0.266819, acc 0.898438\n",
      "2017-01-10T20:44:54.314101: step 609, loss 0.434073, acc 0.882812\n",
      "2017-01-10T20:44:57.026252: step 610, loss 0.331266, acc 0.898438\n",
      "2017-01-10T20:44:59.235072: step 611, loss 0.554161, acc 0.828125\n",
      "2017-01-10T20:45:01.256671: step 612, loss 0.329594, acc 0.890625\n",
      "2017-01-10T20:45:03.273482: step 613, loss 0.428651, acc 0.851562\n",
      "2017-01-10T20:45:05.332847: step 614, loss 0.322043, acc 0.898438\n",
      "2017-01-10T20:45:07.323465: step 615, loss 0.380179, acc 0.890625\n",
      "2017-01-10T20:45:09.360285: step 616, loss 0.298157, acc 0.90625\n",
      "2017-01-10T20:45:11.362323: step 617, loss 0.35168, acc 0.867188\n",
      "2017-01-10T20:45:13.364735: step 618, loss 0.604323, acc 0.84375\n",
      "2017-01-10T20:45:15.398487: step 619, loss 0.446882, acc 0.867188\n",
      "2017-01-10T20:45:17.405156: step 620, loss 0.377257, acc 0.859375\n",
      "2017-01-10T20:45:19.426979: step 621, loss 0.554808, acc 0.820312\n",
      "2017-01-10T20:45:21.466470: step 622, loss 0.447384, acc 0.859375\n",
      "2017-01-10T20:45:23.515885: step 623, loss 0.45511, acc 0.859375\n",
      "2017-01-10T20:45:25.522707: step 624, loss 0.359992, acc 0.859375\n",
      "2017-01-10T20:45:27.547441: step 625, loss 0.502381, acc 0.867188\n",
      "2017-01-10T20:45:29.536386: step 626, loss 0.413786, acc 0.882812\n",
      "2017-01-10T20:45:31.692457: step 627, loss 0.464246, acc 0.875\n",
      "2017-01-10T20:45:33.893332: step 628, loss 0.340451, acc 0.882812\n",
      "2017-01-10T20:45:35.917682: step 629, loss 0.3605, acc 0.890625\n",
      "2017-01-10T20:45:37.913350: step 630, loss 0.367445, acc 0.890625\n",
      "2017-01-10T20:45:39.923985: step 631, loss 0.403359, acc 0.882812\n",
      "2017-01-10T20:45:42.074008: step 632, loss 0.29383, acc 0.90625\n",
      "2017-01-10T20:45:44.095312: step 633, loss 0.349528, acc 0.90625\n",
      "2017-01-10T20:45:46.131953: step 634, loss 0.370257, acc 0.890625\n",
      "2017-01-10T20:45:48.108055: step 635, loss 0.265759, acc 0.945312\n",
      "2017-01-10T20:45:50.141705: step 636, loss 0.267514, acc 0.914062\n",
      "2017-01-10T20:45:52.170569: step 637, loss 0.211327, acc 0.921875\n",
      "2017-01-10T20:45:54.152595: step 638, loss 0.410032, acc 0.875\n",
      "2017-01-10T20:45:56.640517: step 639, loss 0.340428, acc 0.90625\n",
      "2017-01-10T20:45:59.054375: step 640, loss 0.588754, acc 0.8125\n",
      "2017-01-10T20:46:01.032074: step 641, loss 0.325414, acc 0.914062\n",
      "2017-01-10T20:46:03.130231: step 642, loss 0.253224, acc 0.90625\n",
      "2017-01-10T20:46:05.154588: step 643, loss 0.291222, acc 0.898438\n",
      "2017-01-10T20:46:07.131841: step 644, loss 0.262081, acc 0.914062\n",
      "2017-01-10T20:46:09.119888: step 645, loss 0.457591, acc 0.867188\n",
      "2017-01-10T20:46:11.120511: step 646, loss 0.376822, acc 0.914062\n",
      "2017-01-10T20:46:13.075260: step 647, loss 0.296584, acc 0.90625\n",
      "2017-01-10T20:46:15.092927: step 648, loss 0.286318, acc 0.90625\n",
      "2017-01-10T20:46:17.084272: step 649, loss 0.410384, acc 0.875\n",
      "2017-01-10T20:46:19.099322: step 650, loss 0.278732, acc 0.921875\n",
      "2017-01-10T20:46:21.105667: step 651, loss 0.482756, acc 0.851562\n",
      "2017-01-10T20:46:23.083254: step 652, loss 0.30658, acc 0.921875\n",
      "2017-01-10T20:46:25.083865: step 653, loss 0.47708, acc 0.882812\n",
      "2017-01-10T20:46:27.168076: step 654, loss 0.326651, acc 0.898438\n",
      "2017-01-10T20:46:29.193516: step 655, loss 0.62157, acc 0.84375\n",
      "2017-01-10T20:46:31.233492: step 656, loss 0.412189, acc 0.90625\n",
      "2017-01-10T20:46:33.260722: step 657, loss 0.226019, acc 0.9375\n",
      "2017-01-10T20:46:35.469764: step 658, loss 0.389511, acc 0.890625\n",
      "2017-01-10T20:46:37.439069: step 659, loss 0.297383, acc 0.914062\n",
      "2017-01-10T20:46:39.457328: step 660, loss 0.300357, acc 0.882812\n",
      "2017-01-10T20:46:41.433136: step 661, loss 0.476474, acc 0.859375\n",
      "2017-01-10T20:46:43.437387: step 662, loss 0.299409, acc 0.914062\n",
      "2017-01-10T20:46:45.434749: step 663, loss 0.490447, acc 0.859375\n",
      "2017-01-10T20:46:47.427615: step 664, loss 0.266739, acc 0.914062\n",
      "2017-01-10T20:46:49.368889: step 665, loss 0.28048, acc 0.898438\n",
      "2017-01-10T20:46:51.348000: step 666, loss 0.277467, acc 0.921875\n",
      "2017-01-10T20:46:53.325893: step 667, loss 0.302265, acc 0.875\n",
      "2017-01-10T20:46:55.333202: step 668, loss 0.325402, acc 0.914062\n",
      "2017-01-10T20:46:57.318771: step 669, loss 0.534965, acc 0.84375\n",
      "2017-01-10T20:46:59.280894: step 670, loss 0.294561, acc 0.921875\n",
      "2017-01-10T20:47:01.282301: step 671, loss 0.626896, acc 0.875\n",
      "2017-01-10T20:47:03.351431: step 672, loss 0.45294, acc 0.867188\n",
      "2017-01-10T20:47:05.368456: step 673, loss 0.432619, acc 0.851562\n",
      "2017-01-10T20:47:07.528091: step 674, loss 0.217461, acc 0.945312\n",
      "2017-01-10T20:47:09.554298: step 675, loss 0.490098, acc 0.835938\n",
      "2017-01-10T20:47:11.552586: step 676, loss 0.241994, acc 0.914062\n",
      "2017-01-10T20:47:13.588920: step 677, loss 0.22651, acc 0.945312\n",
      "2017-01-10T20:47:15.582141: step 678, loss 0.230721, acc 0.9375\n",
      "2017-01-10T20:47:17.581453: step 679, loss 0.39914, acc 0.890625\n",
      "2017-01-10T20:47:19.593336: step 680, loss 0.331907, acc 0.875\n",
      "2017-01-10T20:47:21.661118: step 681, loss 0.444307, acc 0.859375\n",
      "2017-01-10T20:47:23.680731: step 682, loss 0.452297, acc 0.882812\n",
      "2017-01-10T20:47:25.695414: step 683, loss 0.329369, acc 0.890625\n",
      "2017-01-10T20:47:27.659910: step 684, loss 0.348845, acc 0.875\n",
      "2017-01-10T20:47:29.628407: step 685, loss 0.285003, acc 0.890625\n",
      "2017-01-10T20:47:31.594489: step 686, loss 0.512576, acc 0.828125\n",
      "2017-01-10T20:47:33.579446: step 687, loss 0.462475, acc 0.851562\n",
      "2017-01-10T20:47:35.614544: step 688, loss 0.211067, acc 0.921875\n",
      "2017-01-10T20:47:37.730353: step 689, loss 0.33577, acc 0.890625\n",
      "2017-01-10T20:47:39.880856: step 690, loss 0.340951, acc 0.867188\n",
      "2017-01-10T20:47:41.858164: step 691, loss 0.296064, acc 0.882812\n",
      "2017-01-10T20:47:43.858644: step 692, loss 0.153187, acc 0.953125\n",
      "2017-01-10T20:47:45.847355: step 693, loss 0.464696, acc 0.84375\n",
      "2017-01-10T20:47:47.857498: step 694, loss 0.335003, acc 0.898438\n",
      "2017-01-10T20:47:49.901421: step 695, loss 0.190939, acc 0.929688\n",
      "2017-01-10T20:47:51.963487: step 696, loss 0.273093, acc 0.898438\n",
      "2017-01-10T20:47:53.917019: step 697, loss 0.301802, acc 0.90625\n",
      "2017-01-10T20:47:55.929282: step 698, loss 0.403744, acc 0.875\n",
      "2017-01-10T20:47:57.973677: step 699, loss 0.53892, acc 0.835938\n",
      "2017-01-10T20:47:59.960084: step 700, loss 0.196994, acc 0.929688\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:48:27.107284: step 700, loss 0.163509, acc 0.95292\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-700\n",
      "\n",
      "2017-01-10T20:48:31.757260: step 701, loss 0.418047, acc 0.90625\n",
      "2017-01-10T20:48:33.748392: step 702, loss 0.216864, acc 0.945312\n",
      "2017-01-10T20:48:35.752715: step 703, loss 0.248384, acc 0.898438\n",
      "2017-01-10T20:48:37.764806: step 704, loss 0.476812, acc 0.867188\n",
      "2017-01-10T20:48:39.767687: step 705, loss 0.217183, acc 0.945312\n",
      "2017-01-10T20:48:41.776659: step 706, loss 0.249365, acc 0.929688\n",
      "2017-01-10T20:48:43.888578: step 707, loss 0.330448, acc 0.890625\n",
      "2017-01-10T20:48:45.868433: step 708, loss 0.509008, acc 0.84375\n",
      "2017-01-10T20:48:47.853640: step 709, loss 0.204299, acc 0.929688\n",
      "2017-01-10T20:48:50.003407: step 710, loss 0.24826, acc 0.929688\n",
      "2017-01-10T20:48:52.045129: step 711, loss 0.284222, acc 0.921875\n",
      "2017-01-10T20:48:54.016873: step 712, loss 0.335267, acc 0.882812\n",
      "2017-01-10T20:48:56.080023: step 713, loss 0.301107, acc 0.890625\n",
      "2017-01-10T20:48:58.089912: step 714, loss 0.265153, acc 0.90625\n",
      "2017-01-10T20:49:00.119861: step 715, loss 0.413121, acc 0.859375\n",
      "2017-01-10T20:49:02.169303: step 716, loss 0.218331, acc 0.929688\n",
      "2017-01-10T20:49:04.159889: step 717, loss 0.487943, acc 0.890625\n",
      "2017-01-10T20:49:06.164295: step 718, loss 0.460227, acc 0.867188\n",
      "2017-01-10T20:49:08.187760: step 719, loss 0.433578, acc 0.851562\n",
      "2017-01-10T20:49:10.161192: step 720, loss 0.336212, acc 0.914062\n",
      "2017-01-10T20:49:12.175597: step 721, loss 0.232166, acc 0.90625\n",
      "2017-01-10T20:49:14.209116: step 722, loss 0.32034, acc 0.882812\n",
      "2017-01-10T20:49:16.331607: step 723, loss 0.272804, acc 0.914062\n",
      "2017-01-10T20:49:18.346505: step 724, loss 0.375851, acc 0.898438\n",
      "2017-01-10T20:49:20.403407: step 725, loss 0.182281, acc 0.921875\n",
      "2017-01-10T20:49:22.505825: step 726, loss 0.357082, acc 0.898438\n",
      "2017-01-10T20:49:24.497911: step 727, loss 0.427832, acc 0.882812\n",
      "2017-01-10T20:49:26.542956: step 728, loss 0.421237, acc 0.882812\n",
      "2017-01-10T20:49:28.608425: step 729, loss 0.163502, acc 0.945312\n",
      "2017-01-10T20:49:30.571410: step 730, loss 0.320871, acc 0.898438\n",
      "2017-01-10T20:49:32.584542: step 731, loss 0.281516, acc 0.953125\n",
      "2017-01-10T20:49:34.601441: step 732, loss 0.248304, acc 0.921875\n",
      "2017-01-10T20:49:36.630964: step 733, loss 0.269069, acc 0.898438\n",
      "2017-01-10T20:49:38.658292: step 734, loss 0.29657, acc 0.9375\n",
      "2017-01-10T20:49:40.768133: step 735, loss 0.42133, acc 0.890625\n",
      "2017-01-10T20:49:42.845396: step 736, loss 0.342564, acc 0.867188\n",
      "2017-01-10T20:49:44.839756: step 737, loss 0.382289, acc 0.867188\n",
      "2017-01-10T20:49:46.856523: step 738, loss 0.319146, acc 0.90625\n",
      "2017-01-10T20:49:49.058606: step 739, loss 0.520966, acc 0.835938\n",
      "2017-01-10T20:49:51.130862: step 740, loss 0.217725, acc 0.929688\n",
      "2017-01-10T20:49:53.189425: step 741, loss 0.371773, acc 0.90625\n",
      "2017-01-10T20:49:55.208816: step 742, loss 0.514997, acc 0.84375\n",
      "2017-01-10T20:49:58.215352: step 743, loss 0.369154, acc 0.90625\n",
      "2017-01-10T20:50:00.261266: step 744, loss 0.357651, acc 0.90625\n",
      "2017-01-10T20:50:02.251438: step 745, loss 0.257316, acc 0.898438\n",
      "2017-01-10T20:50:04.300673: step 746, loss 0.42475, acc 0.882812\n",
      "2017-01-10T20:50:06.313025: step 747, loss 0.287827, acc 0.898438\n",
      "2017-01-10T20:50:08.356565: step 748, loss 0.239596, acc 0.90625\n",
      "2017-01-10T20:50:10.352526: step 749, loss 0.313729, acc 0.914062\n",
      "2017-01-10T20:50:12.424191: step 750, loss 0.323067, acc 0.882812\n",
      "2017-01-10T20:50:14.445569: step 751, loss 0.183937, acc 0.914062\n",
      "2017-01-10T20:50:16.439945: step 752, loss 0.383755, acc 0.898438\n",
      "2017-01-10T20:50:18.427450: step 753, loss 0.310757, acc 0.90625\n",
      "2017-01-10T20:50:20.636872: step 754, loss 0.128706, acc 0.945312\n",
      "2017-01-10T20:50:22.586415: step 755, loss 0.293335, acc 0.921875\n",
      "2017-01-10T20:50:24.601925: step 756, loss 0.259677, acc 0.898438\n",
      "2017-01-10T20:50:26.615264: step 757, loss 0.424406, acc 0.851562\n",
      "2017-01-10T20:50:28.627019: step 758, loss 0.273635, acc 0.929688\n",
      "2017-01-10T20:50:30.685734: step 759, loss 0.207986, acc 0.921875\n",
      "2017-01-10T20:50:32.684599: step 760, loss 0.339449, acc 0.882812\n",
      "2017-01-10T20:50:34.731540: step 761, loss 0.375912, acc 0.890625\n",
      "2017-01-10T20:50:36.756124: step 762, loss 0.217553, acc 0.9375\n",
      "2017-01-10T20:50:38.757760: step 763, loss 0.360481, acc 0.90625\n",
      "2017-01-10T20:50:40.689514: step 764, loss 0.293893, acc 0.890625\n",
      "2017-01-10T20:50:42.680105: step 765, loss 0.369414, acc 0.882812\n",
      "2017-01-10T20:50:44.663331: step 766, loss 0.191661, acc 0.929688\n",
      "2017-01-10T20:50:47.062823: step 767, loss 0.418592, acc 0.851562\n",
      "2017-01-10T20:50:49.067860: step 768, loss 0.366839, acc 0.898438\n",
      "2017-01-10T20:50:51.103522: step 769, loss 0.211703, acc 0.9375\n",
      "2017-01-10T20:50:53.281826: step 770, loss 0.379229, acc 0.890625\n",
      "2017-01-10T20:50:55.269229: step 771, loss 0.366057, acc 0.90625\n",
      "2017-01-10T20:50:57.310359: step 772, loss 0.318777, acc 0.898438\n",
      "2017-01-10T20:50:59.340693: step 773, loss 0.309305, acc 0.90625\n",
      "2017-01-10T20:51:01.355516: step 774, loss 0.246156, acc 0.929688\n",
      "2017-01-10T20:51:04.513290: step 775, loss 0.401356, acc 0.875\n",
      "2017-01-10T20:51:07.472478: step 776, loss 0.226175, acc 0.9375\n",
      "2017-01-10T20:51:09.478393: step 777, loss 0.333858, acc 0.921875\n",
      "2017-01-10T20:51:11.477983: step 778, loss 0.300971, acc 0.882812\n",
      "2017-01-10T20:51:13.473601: step 779, loss 0.311258, acc 0.914062\n",
      "2017-01-10T20:51:15.481134: step 780, loss 0.30056, acc 0.890625\n",
      "2017-01-10T20:51:17.477879: step 781, loss 0.249604, acc 0.929688\n",
      "2017-01-10T20:51:19.506215: step 782, loss 0.3687, acc 0.882812\n",
      "2017-01-10T20:51:21.554339: step 783, loss 0.150141, acc 0.945312\n",
      "2017-01-10T20:51:23.581325: step 784, loss 0.275983, acc 0.921875\n",
      "2017-01-10T20:51:25.704227: step 785, loss 0.37464, acc 0.867188\n",
      "2017-01-10T20:51:27.734596: step 786, loss 0.266102, acc 0.90625\n",
      "2017-01-10T20:51:29.693093: step 787, loss 0.311433, acc 0.898438\n",
      "2017-01-10T20:51:31.715656: step 788, loss 0.252733, acc 0.898438\n",
      "2017-01-10T20:51:33.686227: step 789, loss 0.462622, acc 0.859375\n",
      "2017-01-10T20:51:35.713327: step 790, loss 0.208725, acc 0.929688\n",
      "2017-01-10T20:51:37.709667: step 791, loss 0.35558, acc 0.90625\n",
      "2017-01-10T20:51:39.707331: step 792, loss 0.260011, acc 0.921875\n",
      "2017-01-10T20:51:41.710503: step 793, loss 0.298279, acc 0.890625\n",
      "2017-01-10T20:51:43.701464: step 794, loss 0.211649, acc 0.929688\n",
      "2017-01-10T20:51:45.687618: step 795, loss 0.394912, acc 0.890625\n",
      "2017-01-10T20:51:47.695552: step 796, loss 0.268943, acc 0.90625\n",
      "2017-01-10T20:51:49.758375: step 797, loss 0.316457, acc 0.914062\n",
      "2017-01-10T20:51:51.845144: step 798, loss 0.216449, acc 0.914062\n",
      "2017-01-10T20:51:53.848670: step 799, loss 0.334548, acc 0.882812\n",
      "2017-01-10T20:51:55.944653: step 800, loss 0.258926, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:52:30.600690: step 800, loss 0.151786, acc 0.9568\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-800\n",
      "\n",
      "2017-01-10T20:52:35.601398: step 801, loss 0.24927, acc 0.921875\n",
      "2017-01-10T20:52:37.675015: step 802, loss 0.249588, acc 0.929688\n",
      "2017-01-10T20:52:39.685947: step 803, loss 0.0884715, acc 0.960938\n",
      "2017-01-10T20:52:41.695649: step 804, loss 0.342852, acc 0.90625\n",
      "2017-01-10T20:52:43.704636: step 805, loss 0.372651, acc 0.898438\n",
      "2017-01-10T20:52:45.698491: step 806, loss 0.387631, acc 0.890625\n",
      "2017-01-10T20:52:47.711547: step 807, loss 0.357644, acc 0.890625\n",
      "2017-01-10T20:52:49.786477: step 808, loss 0.226521, acc 0.945312\n",
      "2017-01-10T20:52:51.805971: step 809, loss 0.251495, acc 0.90625\n",
      "2017-01-10T20:52:53.790966: step 810, loss 0.338181, acc 0.898438\n",
      "2017-01-10T20:52:55.792321: step 811, loss 0.329187, acc 0.890625\n",
      "2017-01-10T20:52:57.809922: step 812, loss 0.473571, acc 0.859375\n",
      "2017-01-10T20:52:59.808836: step 813, loss 0.246492, acc 0.921875\n",
      "2017-01-10T20:53:01.879023: step 814, loss 0.416242, acc 0.882812\n",
      "2017-01-10T20:53:04.121060: step 815, loss 0.358165, acc 0.867188\n",
      "2017-01-10T20:53:06.140660: step 816, loss 0.207684, acc 0.945312\n",
      "2017-01-10T20:53:08.131531: step 817, loss 0.204196, acc 0.9375\n",
      "2017-01-10T20:53:10.132164: step 818, loss 0.301937, acc 0.875\n",
      "2017-01-10T20:53:12.135525: step 819, loss 0.418709, acc 0.828125\n",
      "2017-01-10T20:53:14.128775: step 820, loss 0.365959, acc 0.914062\n",
      "2017-01-10T20:53:16.158642: step 821, loss 0.249295, acc 0.921875\n",
      "2017-01-10T20:53:18.166075: step 822, loss 0.349224, acc 0.90625\n",
      "2017-01-10T20:53:20.216799: step 823, loss 0.268501, acc 0.898438\n",
      "2017-01-10T20:53:22.267061: step 824, loss 0.255149, acc 0.921875\n",
      "2017-01-10T20:53:24.290154: step 825, loss 0.218193, acc 0.929688\n",
      "2017-01-10T20:53:26.294822: step 826, loss 0.298946, acc 0.90625\n",
      "2017-01-10T20:53:28.321297: step 827, loss 0.338703, acc 0.929688\n",
      "2017-01-10T20:53:30.301850: step 828, loss 0.244196, acc 0.914062\n",
      "2017-01-10T20:53:32.324386: step 829, loss 0.236582, acc 0.945312\n",
      "2017-01-10T20:53:34.320023: step 830, loss 0.303999, acc 0.914062\n",
      "2017-01-10T20:53:36.512112: step 831, loss 0.443821, acc 0.890625\n",
      "2017-01-10T20:53:38.499076: step 832, loss 0.3431, acc 0.851562\n",
      "2017-01-10T20:53:40.502030: step 833, loss 0.239872, acc 0.921875\n",
      "2017-01-10T20:53:42.515062: step 834, loss 0.337345, acc 0.882812\n",
      "2017-01-10T20:53:44.500249: step 835, loss 0.428964, acc 0.851562\n",
      "2017-01-10T20:53:46.546275: step 836, loss 0.32296, acc 0.90625\n",
      "2017-01-10T20:53:48.571934: step 837, loss 0.38909, acc 0.875\n",
      "2017-01-10T20:53:50.624233: step 838, loss 0.294354, acc 0.914062\n",
      "2017-01-10T20:53:52.659471: step 839, loss 0.419973, acc 0.875\n",
      "2017-01-10T20:53:54.665238: step 840, loss 0.477455, acc 0.859375\n",
      "2017-01-10T20:53:56.690728: step 841, loss 0.222697, acc 0.9375\n",
      "2017-01-10T20:53:58.729315: step 842, loss 0.342331, acc 0.898438\n",
      "2017-01-10T20:54:00.738266: step 843, loss 0.187322, acc 0.945312\n",
      "2017-01-10T20:54:02.745412: step 844, loss 0.123769, acc 0.960938\n",
      "2017-01-10T20:54:04.744152: step 845, loss 0.242472, acc 0.921875\n",
      "2017-01-10T20:54:06.772632: step 846, loss 0.151825, acc 0.945312\n",
      "2017-01-10T20:54:08.940083: step 847, loss 0.18752, acc 0.9375\n",
      "2017-01-10T20:54:10.973059: step 848, loss 0.317494, acc 0.914062\n",
      "2017-01-10T20:54:12.973621: step 849, loss 0.227501, acc 0.914062\n",
      "2017-01-10T20:54:14.943729: step 850, loss 0.347667, acc 0.921875\n",
      "2017-01-10T20:54:16.964023: step 851, loss 0.325413, acc 0.90625\n",
      "2017-01-10T20:54:18.984501: step 852, loss 0.198832, acc 0.953125\n",
      "2017-01-10T20:54:21.049416: step 853, loss 0.247288, acc 0.929688\n",
      "2017-01-10T20:54:23.061070: step 854, loss 0.197084, acc 0.9375\n",
      "2017-01-10T20:54:25.058258: step 855, loss 0.365949, acc 0.898438\n",
      "2017-01-10T20:54:27.088107: step 856, loss 0.408608, acc 0.875\n",
      "2017-01-10T20:54:29.041093: step 857, loss 0.230299, acc 0.929688\n",
      "2017-01-10T20:54:31.048442: step 858, loss 0.507145, acc 0.90625\n",
      "2017-01-10T20:54:33.061741: step 859, loss 0.154993, acc 0.945312\n",
      "2017-01-10T20:54:35.091137: step 860, loss 0.308574, acc 0.90625\n",
      "2017-01-10T20:54:37.130795: step 861, loss 0.32654, acc 0.890625\n",
      "2017-01-10T20:54:39.107002: step 862, loss 0.247317, acc 0.9375\n",
      "2017-01-10T20:54:41.275429: step 863, loss 0.257514, acc 0.929688\n",
      "2017-01-10T20:54:43.274567: step 864, loss 0.353946, acc 0.875\n",
      "2017-01-10T20:54:45.492606: step 865, loss 0.267908, acc 0.898438\n",
      "2017-01-10T20:54:47.694682: step 866, loss 0.383381, acc 0.890625\n",
      "2017-01-10T20:54:49.952366: step 867, loss 0.349004, acc 0.875\n",
      "2017-01-10T20:54:51.983451: step 868, loss 0.403149, acc 0.898438\n",
      "2017-01-10T20:54:54.245379: step 869, loss 0.330759, acc 0.890625\n",
      "2017-01-10T20:54:56.241641: step 870, loss 0.406462, acc 0.882812\n",
      "2017-01-10T20:54:58.272670: step 871, loss 0.176175, acc 0.9375\n",
      "2017-01-10T20:55:00.267360: step 872, loss 0.34067, acc 0.859375\n",
      "2017-01-10T20:55:02.271558: step 873, loss 0.267559, acc 0.921875\n",
      "2017-01-10T20:55:04.278678: step 874, loss 0.143197, acc 0.960938\n",
      "2017-01-10T20:55:06.324255: step 875, loss 0.292146, acc 0.90625\n",
      "2017-01-10T20:55:08.344728: step 876, loss 0.274459, acc 0.921875\n",
      "2017-01-10T20:55:10.354714: step 877, loss 0.545483, acc 0.851562\n",
      "2017-01-10T20:55:12.464591: step 878, loss 0.221399, acc 0.945312\n",
      "2017-01-10T20:55:14.425447: step 879, loss 0.277761, acc 0.914062\n",
      "2017-01-10T20:55:16.473353: step 880, loss 0.378297, acc 0.898438\n",
      "2017-01-10T20:55:18.461576: step 881, loss 0.254445, acc 0.914062\n",
      "2017-01-10T20:55:20.538151: step 882, loss 0.312335, acc 0.90625\n",
      "2017-01-10T20:55:22.583582: step 883, loss 0.216174, acc 0.921875\n",
      "2017-01-10T20:55:24.615074: step 884, loss 0.212495, acc 0.945312\n",
      "2017-01-10T20:55:26.620371: step 885, loss 0.286527, acc 0.890625\n",
      "2017-01-10T20:55:28.643502: step 886, loss 0.28728, acc 0.898438\n",
      "2017-01-10T20:55:30.672398: step 887, loss 0.30826, acc 0.898438\n",
      "2017-01-10T20:55:32.679612: step 888, loss 0.28106, acc 0.921875\n",
      "2017-01-10T20:55:34.726248: step 889, loss 0.354504, acc 0.90625\n",
      "2017-01-10T20:55:36.741248: step 890, loss 0.27538, acc 0.929688\n",
      "2017-01-10T20:55:38.761621: step 891, loss 0.133604, acc 0.976562\n",
      "2017-01-10T20:55:40.778347: step 892, loss 0.299722, acc 0.890625\n",
      "2017-01-10T20:55:42.806311: step 893, loss 0.275045, acc 0.921875\n",
      "2017-01-10T20:55:45.012217: step 894, loss 0.255865, acc 0.9375\n",
      "2017-01-10T20:55:47.488202: step 895, loss 0.301776, acc 0.914062\n",
      "2017-01-10T20:55:49.563074: step 896, loss 0.307547, acc 0.921875\n",
      "2017-01-10T20:55:51.659280: step 897, loss 0.466798, acc 0.851562\n",
      "2017-01-10T20:55:53.639979: step 898, loss 0.35309, acc 0.875\n",
      "2017-01-10T20:55:55.628442: step 899, loss 0.284356, acc 0.90625\n",
      "2017-01-10T20:55:57.674961: step 900, loss 0.262726, acc 0.914062\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T20:56:30.945244: step 900, loss 0.145673, acc 0.95812\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-900\n",
      "\n",
      "2017-01-10T20:56:35.585407: step 901, loss 0.302234, acc 0.914062\n",
      "2017-01-10T20:56:37.635552: step 902, loss 0.404677, acc 0.867188\n",
      "2017-01-10T20:56:39.693116: step 903, loss 0.197758, acc 0.914062\n",
      "2017-01-10T20:56:41.715993: step 904, loss 0.171928, acc 0.9375\n",
      "2017-01-10T20:56:43.747094: step 905, loss 0.245296, acc 0.914062\n",
      "2017-01-10T20:56:45.705467: step 906, loss 0.22562, acc 0.90625\n",
      "2017-01-10T20:56:47.705127: step 907, loss 0.348814, acc 0.882812\n",
      "2017-01-10T20:56:49.714470: step 908, loss 0.131982, acc 0.953125\n",
      "2017-01-10T20:56:51.821578: step 909, loss 0.322518, acc 0.90625\n",
      "2017-01-10T20:56:54.240074: step 910, loss 0.160188, acc 0.929688\n",
      "2017-01-10T20:56:56.247729: step 911, loss 0.403429, acc 0.890625\n",
      "2017-01-10T20:56:58.261827: step 912, loss 0.30884, acc 0.914062\n",
      "2017-01-10T20:57:00.278845: step 913, loss 0.262361, acc 0.914062\n",
      "2017-01-10T20:57:02.291078: step 914, loss 0.161433, acc 0.921875\n",
      "2017-01-10T20:57:04.319704: step 915, loss 0.325926, acc 0.90625\n",
      "2017-01-10T20:57:06.336614: step 916, loss 0.197312, acc 0.953125\n",
      "2017-01-10T20:57:08.365090: step 917, loss 0.246462, acc 0.929688\n",
      "2017-01-10T20:57:11.379543: step 918, loss 0.159392, acc 0.953125\n",
      "2017-01-10T20:57:13.394456: step 919, loss 0.384217, acc 0.898438\n",
      "2017-01-10T20:57:15.387760: step 920, loss 0.162302, acc 0.953125\n",
      "2017-01-10T20:57:17.426102: step 921, loss 0.382601, acc 0.867188\n",
      "2017-01-10T20:57:19.497840: step 922, loss 0.569355, acc 0.898438\n",
      "2017-01-10T20:57:21.551681: step 923, loss 0.313777, acc 0.914062\n",
      "2017-01-10T20:57:23.555698: step 924, loss 0.32865, acc 0.90625\n",
      "2017-01-10T20:57:25.672968: step 925, loss 0.210699, acc 0.929688\n",
      "2017-01-10T20:57:27.718649: step 926, loss 0.251902, acc 0.929688\n",
      "2017-01-10T20:57:29.714952: step 927, loss 0.183604, acc 0.929688\n",
      "2017-01-10T20:57:31.682768: step 928, loss 0.29553, acc 0.890625\n",
      "2017-01-10T20:57:33.690760: step 929, loss 0.254297, acc 0.929688\n",
      "2017-01-10T20:57:35.752574: step 930, loss 0.28324, acc 0.882812\n",
      "2017-01-10T20:57:37.716631: step 931, loss 0.204447, acc 0.960938\n",
      "2017-01-10T20:57:39.730185: step 932, loss 0.223662, acc 0.9375\n",
      "2017-01-10T20:57:41.727767: step 933, loss 0.289465, acc 0.890625\n",
      "2017-01-10T20:57:43.715285: step 934, loss 0.343612, acc 0.882812\n",
      "2017-01-10T20:57:45.747554: step 935, loss 0.287737, acc 0.914062\n",
      "2017-01-10T20:57:47.746590: step 936, loss 0.469278, acc 0.84375\n",
      "2017-01-10T20:57:49.786044: step 937, loss 0.217444, acc 0.914062\n",
      "2017-01-10T20:57:51.829948: step 938, loss 0.285979, acc 0.882812\n",
      "2017-01-10T20:57:53.798138: step 939, loss 0.229324, acc 0.921875\n",
      "2017-01-10T20:57:55.777941: step 940, loss 0.36307, acc 0.898438\n",
      "2017-01-10T20:57:57.969270: step 941, loss 0.216789, acc 0.914062\n",
      "2017-01-10T20:57:59.952494: step 942, loss 0.214152, acc 0.929688\n",
      "2017-01-10T20:58:01.968009: step 943, loss 0.203387, acc 0.921875\n",
      "2017-01-10T20:58:04.011768: step 944, loss 0.361641, acc 0.875\n",
      "2017-01-10T20:58:06.008949: step 945, loss 0.327647, acc 0.914062\n",
      "2017-01-10T20:58:08.004233: step 946, loss 0.356877, acc 0.898438\n",
      "2017-01-10T20:58:10.036047: step 947, loss 0.263954, acc 0.898438\n",
      "2017-01-10T20:58:12.037524: step 948, loss 0.214868, acc 0.921875\n",
      "2017-01-10T20:58:14.025124: step 949, loss 0.458572, acc 0.898438\n",
      "2017-01-10T20:58:16.008255: step 950, loss 0.241823, acc 0.9375\n",
      "2017-01-10T20:58:18.040320: step 951, loss 0.276985, acc 0.90625\n",
      "2017-01-10T20:58:20.100082: step 952, loss 0.272179, acc 0.929688\n",
      "2017-01-10T20:58:22.128884: step 953, loss 0.26547, acc 0.914062\n",
      "2017-01-10T20:58:24.147257: step 954, loss 0.23879, acc 0.929688\n",
      "2017-01-10T20:58:26.196575: step 955, loss 0.234016, acc 0.929688\n",
      "2017-01-10T20:58:28.225708: step 956, loss 0.312382, acc 0.898438\n",
      "2017-01-10T20:58:30.405006: step 957, loss 0.330737, acc 0.921875\n",
      "2017-01-10T20:58:32.464599: step 958, loss 0.356659, acc 0.90625\n",
      "2017-01-10T20:58:34.502372: step 959, loss 0.305706, acc 0.914062\n",
      "2017-01-10T20:58:36.448207: step 960, loss 0.334723, acc 0.90625\n",
      "2017-01-10T20:58:38.408119: step 961, loss 0.3687, acc 0.929688\n",
      "2017-01-10T20:58:40.473687: step 962, loss 0.26399, acc 0.921875\n",
      "2017-01-10T20:58:42.512176: step 963, loss 0.33003, acc 0.890625\n",
      "2017-01-10T20:58:44.552064: step 964, loss 0.35057, acc 0.882812\n",
      "2017-01-10T20:58:46.572925: step 965, loss 0.156537, acc 0.9375\n",
      "2017-01-10T20:58:48.577362: step 966, loss 0.197811, acc 0.921875\n",
      "2017-01-10T20:58:50.680509: step 967, loss 0.191795, acc 0.945312\n",
      "2017-01-10T20:58:52.686199: step 968, loss 0.214815, acc 0.921875\n",
      "2017-01-10T20:58:54.710270: step 969, loss 0.243878, acc 0.929688\n",
      "2017-01-10T20:58:56.732603: step 970, loss 0.104456, acc 0.960938\n",
      "2017-01-10T20:58:58.757776: step 971, loss 0.347816, acc 0.875\n",
      "2017-01-10T20:59:00.821538: step 972, loss 0.105923, acc 0.945312\n",
      "2017-01-10T20:59:03.015268: step 973, loss 0.120017, acc 0.960938\n",
      "2017-01-10T20:59:05.023552: step 974, loss 0.211305, acc 0.929688\n",
      "2017-01-10T20:59:07.068243: step 975, loss 0.261009, acc 0.929688\n",
      "2017-01-10T20:59:09.094248: step 976, loss 0.207862, acc 0.914062\n",
      "2017-01-10T20:59:11.097804: step 977, loss 0.103115, acc 0.984375\n",
      "2017-01-10T20:59:13.088780: step 978, loss 0.265269, acc 0.90625\n",
      "2017-01-10T20:59:15.072592: step 979, loss 0.218462, acc 0.921875\n",
      "2017-01-10T20:59:17.091786: step 980, loss 0.231837, acc 0.9375\n",
      "2017-01-10T20:59:19.115957: step 981, loss 0.241113, acc 0.9375\n",
      "2017-01-10T20:59:21.116706: step 982, loss 0.281378, acc 0.90625\n",
      "2017-01-10T20:59:23.146969: step 983, loss 0.110318, acc 0.960938\n",
      "2017-01-10T20:59:25.167563: step 984, loss 0.267935, acc 0.9375\n",
      "2017-01-10T20:59:27.194971: step 985, loss 0.317874, acc 0.882812\n",
      "2017-01-10T20:59:29.226485: step 986, loss 0.241241, acc 0.921875\n",
      "2017-01-10T20:59:31.256282: step 987, loss 0.202408, acc 0.921875\n",
      "2017-01-10T20:59:33.251896: step 988, loss 0.214921, acc 0.921875\n",
      "2017-01-10T20:59:35.446024: step 989, loss 0.349258, acc 0.882812\n",
      "2017-01-10T20:59:37.465710: step 990, loss 0.181644, acc 0.9375\n",
      "2017-01-10T20:59:39.490074: step 991, loss 0.268865, acc 0.921875\n",
      "2017-01-10T20:59:41.520286: step 992, loss 0.26077, acc 0.9375\n",
      "2017-01-10T20:59:43.544576: step 993, loss 0.177937, acc 0.96875\n",
      "2017-01-10T20:59:45.583623: step 994, loss 0.258103, acc 0.929688\n",
      "2017-01-10T20:59:47.584168: step 995, loss 0.179158, acc 0.9375\n",
      "2017-01-10T20:59:49.654202: step 996, loss 0.251964, acc 0.953125\n",
      "2017-01-10T20:59:51.720196: step 997, loss 0.218259, acc 0.9375\n",
      "2017-01-10T20:59:53.726833: step 998, loss 0.245345, acc 0.914062\n",
      "2017-01-10T20:59:55.728392: step 999, loss 0.150789, acc 0.945312\n",
      "2017-01-10T20:59:57.763753: step 1000, loss 0.28446, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:00:24.040773: step 1000, loss 0.138412, acc 0.96\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1000\n",
      "\n",
      "2017-01-10T21:00:28.719252: step 1001, loss 0.313433, acc 0.90625\n",
      "2017-01-10T21:00:30.758902: step 1002, loss 0.260808, acc 0.90625\n",
      "2017-01-10T21:00:32.744385: step 1003, loss 0.316323, acc 0.898438\n",
      "2017-01-10T21:00:34.814147: step 1004, loss 0.165349, acc 0.953125\n",
      "2017-01-10T21:00:36.871098: step 1005, loss 0.114251, acc 0.953125\n",
      "2017-01-10T21:00:39.097702: step 1006, loss 0.244505, acc 0.90625\n",
      "2017-01-10T21:00:41.137445: step 1007, loss 0.218558, acc 0.929688\n",
      "2017-01-10T21:00:43.165085: step 1008, loss 0.41018, acc 0.875\n",
      "2017-01-10T21:00:45.167588: step 1009, loss 0.173469, acc 0.960938\n",
      "2017-01-10T21:00:47.197314: step 1010, loss 0.156327, acc 0.960938\n",
      "2017-01-10T21:00:49.219375: step 1011, loss 0.230088, acc 0.929688\n",
      "2017-01-10T21:00:51.227120: step 1012, loss 0.300892, acc 0.898438\n",
      "2017-01-10T21:00:53.232539: step 1013, loss 0.158742, acc 0.929688\n",
      "2017-01-10T21:00:55.192141: step 1014, loss 0.317172, acc 0.90625\n",
      "2017-01-10T21:00:57.234421: step 1015, loss 0.263721, acc 0.898438\n",
      "2017-01-10T21:00:59.248637: step 1016, loss 0.183855, acc 0.921875\n",
      "2017-01-10T21:01:01.232448: step 1017, loss 0.195048, acc 0.9375\n",
      "2017-01-10T21:01:03.247345: step 1018, loss 0.222329, acc 0.9375\n",
      "2017-01-10T21:01:05.276913: step 1019, loss 0.15444, acc 0.960938\n",
      "2017-01-10T21:01:07.309167: step 1020, loss 0.151959, acc 0.953125\n",
      "2017-01-10T21:01:09.334682: step 1021, loss 0.0891732, acc 0.96875\n",
      "2017-01-10T21:01:11.451112: step 1022, loss 0.308325, acc 0.898438\n",
      "2017-01-10T21:01:13.469980: step 1023, loss 0.0809536, acc 0.984375\n",
      "2017-01-10T21:01:15.471960: step 1024, loss 0.339055, acc 0.875\n",
      "2017-01-10T21:01:17.512075: step 1025, loss 0.351968, acc 0.890625\n",
      "2017-01-10T21:01:19.552001: step 1026, loss 0.235385, acc 0.9375\n",
      "2017-01-10T21:01:21.629284: step 1027, loss 0.223589, acc 0.9375\n",
      "2017-01-10T21:01:23.666269: step 1028, loss 0.211766, acc 0.9375\n",
      "2017-01-10T21:01:25.690117: step 1029, loss 0.257987, acc 0.929688\n",
      "2017-01-10T21:01:27.733718: step 1030, loss 0.283524, acc 0.898438\n",
      "2017-01-10T21:01:30.340795: step 1031, loss 0.324739, acc 0.898438\n",
      "2017-01-10T21:01:32.374576: step 1032, loss 0.291007, acc 0.929688\n",
      "2017-01-10T21:01:34.382400: step 1033, loss 0.250655, acc 0.914062\n",
      "2017-01-10T21:01:36.370235: step 1034, loss 0.357591, acc 0.945312\n",
      "2017-01-10T21:01:38.405649: step 1035, loss 0.332026, acc 0.882812\n",
      "2017-01-10T21:01:40.435464: step 1036, loss 0.16274, acc 0.945312\n",
      "2017-01-10T21:01:42.468083: step 1037, loss 0.215177, acc 0.914062\n",
      "2017-01-10T21:01:44.798878: step 1038, loss 0.2704, acc 0.945312\n",
      "2017-01-10T21:01:48.179081: step 1039, loss 0.131562, acc 0.953125\n",
      "2017-01-10T21:01:50.744571: step 1040, loss 0.258459, acc 0.914062\n",
      "2017-01-10T21:01:52.753381: step 1041, loss 0.251682, acc 0.898438\n",
      "2017-01-10T21:01:54.769269: step 1042, loss 0.27425, acc 0.945312\n",
      "2017-01-10T21:01:56.778126: step 1043, loss 0.30233, acc 0.898438\n",
      "2017-01-10T21:01:58.807124: step 1044, loss 0.046837, acc 0.992188\n",
      "2017-01-10T21:02:00.837405: step 1045, loss 0.320965, acc 0.90625\n",
      "2017-01-10T21:02:02.791309: step 1046, loss 0.223973, acc 0.9375\n",
      "2017-01-10T21:02:04.837779: step 1047, loss 0.200083, acc 0.929688\n",
      "2017-01-10T21:02:06.849091: step 1048, loss 0.165742, acc 0.945312\n",
      "2017-01-10T21:02:08.898497: step 1049, loss 0.2748, acc 0.914062\n",
      "2017-01-10T21:02:10.907732: step 1050, loss 0.165074, acc 0.945312\n",
      "2017-01-10T21:02:12.914635: step 1051, loss 0.168717, acc 0.953125\n",
      "2017-01-10T21:02:15.063799: step 1052, loss 0.201529, acc 0.914062\n",
      "2017-01-10T21:02:17.126293: step 1053, loss 0.226936, acc 0.945312\n",
      "2017-01-10T21:02:19.111998: step 1054, loss 0.150142, acc 0.960938\n",
      "2017-01-10T21:02:21.155291: step 1055, loss 0.196886, acc 0.929688\n",
      "2017-01-10T21:02:23.207285: step 1056, loss 0.186978, acc 0.945312\n",
      "2017-01-10T21:02:25.232137: step 1057, loss 0.166025, acc 0.945312\n",
      "2017-01-10T21:02:27.253451: step 1058, loss 0.285487, acc 0.90625\n",
      "2017-01-10T21:02:29.278885: step 1059, loss 0.317325, acc 0.921875\n",
      "2017-01-10T21:02:31.333204: step 1060, loss 0.290802, acc 0.9375\n",
      "2017-01-10T21:02:33.381590: step 1061, loss 0.238303, acc 0.929688\n",
      "2017-01-10T21:02:35.408305: step 1062, loss 0.184766, acc 0.9375\n",
      "2017-01-10T21:02:37.436550: step 1063, loss 0.223574, acc 0.929688\n",
      "2017-01-10T21:02:39.442029: step 1064, loss 0.251258, acc 0.929688\n",
      "2017-01-10T21:02:41.424183: step 1065, loss 0.320527, acc 0.914062\n",
      "2017-01-10T21:02:43.442238: step 1066, loss 0.242515, acc 0.914062\n",
      "2017-01-10T21:02:45.454806: step 1067, loss 0.242987, acc 0.929688\n",
      "2017-01-10T21:02:47.604853: step 1068, loss 0.230839, acc 0.921875\n",
      "2017-01-10T21:02:49.636700: step 1069, loss 0.165683, acc 0.945312\n",
      "2017-01-10T21:02:51.726549: step 1070, loss 0.22588, acc 0.921875\n",
      "2017-01-10T21:02:53.757435: step 1071, loss 0.15911, acc 0.945312\n",
      "2017-01-10T21:02:55.781839: step 1072, loss 0.194166, acc 0.929688\n",
      "2017-01-10T21:02:57.786711: step 1073, loss 0.183101, acc 0.929688\n",
      "2017-01-10T21:02:59.958884: step 1074, loss 0.408039, acc 0.898438\n",
      "2017-01-10T21:03:02.003037: step 1075, loss 0.237808, acc 0.945312\n",
      "2017-01-10T21:03:04.049919: step 1076, loss 0.318412, acc 0.929688\n",
      "2017-01-10T21:03:06.086945: step 1077, loss 0.192526, acc 0.9375\n",
      "2017-01-10T21:03:08.122734: step 1078, loss 0.394418, acc 0.867188\n",
      "2017-01-10T21:03:10.178613: step 1079, loss 0.236484, acc 0.929688\n",
      "2017-01-10T21:03:12.228793: step 1080, loss 0.180855, acc 0.945312\n",
      "2017-01-10T21:03:14.415629: step 1081, loss 0.144991, acc 0.945312\n",
      "2017-01-10T21:03:17.282583: step 1082, loss 0.247414, acc 0.921875\n",
      "2017-01-10T21:03:19.441494: step 1083, loss 0.217065, acc 0.90625\n",
      "2017-01-10T21:03:21.519528: step 1084, loss 0.218797, acc 0.9375\n",
      "2017-01-10T21:03:23.509874: step 1085, loss 0.262243, acc 0.921875\n",
      "2017-01-10T21:03:25.573871: step 1086, loss 0.227231, acc 0.9375\n",
      "2017-01-10T21:03:27.569501: step 1087, loss 0.238459, acc 0.921875\n",
      "2017-01-10T21:03:29.568242: step 1088, loss 0.162341, acc 0.945312\n",
      "2017-01-10T21:03:31.544322: step 1089, loss 0.28532, acc 0.90625\n",
      "2017-01-10T21:03:33.537064: step 1090, loss 0.163531, acc 0.960938\n",
      "2017-01-10T21:03:35.585702: step 1091, loss 0.189773, acc 0.945312\n",
      "2017-01-10T21:03:37.610717: step 1092, loss 0.386362, acc 0.90625\n",
      "2017-01-10T21:03:39.626358: step 1093, loss 0.185032, acc 0.9375\n",
      "2017-01-10T21:03:41.681570: step 1094, loss 0.356272, acc 0.9375\n",
      "2017-01-10T21:03:43.672045: step 1095, loss 0.208606, acc 0.9375\n",
      "2017-01-10T21:03:45.693368: step 1096, loss 0.255946, acc 0.921875\n",
      "2017-01-10T21:03:47.704505: step 1097, loss 0.201675, acc 0.9375\n",
      "2017-01-10T21:03:49.770398: step 1098, loss 0.183506, acc 0.953125\n",
      "2017-01-10T21:03:52.026909: step 1099, loss 0.123282, acc 0.953125\n",
      "2017-01-10T21:03:54.056569: step 1100, loss 0.141644, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:04:23.276006: step 1100, loss 0.133069, acc 0.96156\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1100\n",
      "\n",
      "2017-01-10T21:04:28.083994: step 1101, loss 0.304894, acc 0.921875\n",
      "2017-01-10T21:04:30.134129: step 1102, loss 0.373034, acc 0.914062\n",
      "2017-01-10T21:04:32.195564: step 1103, loss 0.171377, acc 0.945312\n",
      "2017-01-10T21:04:34.173406: step 1104, loss 0.293886, acc 0.914062\n",
      "2017-01-10T21:04:36.225524: step 1105, loss 0.224518, acc 0.945312\n",
      "2017-01-10T21:04:38.272410: step 1106, loss 0.28365, acc 0.921875\n",
      "2017-01-10T21:04:40.234407: step 1107, loss 0.273214, acc 0.914062\n",
      "2017-01-10T21:04:42.240384: step 1108, loss 0.186599, acc 0.921875\n",
      "2017-01-10T21:04:44.270578: step 1109, loss 0.249433, acc 0.921875\n",
      "2017-01-10T21:04:46.324255: step 1110, loss 0.220708, acc 0.9375\n",
      "2017-01-10T21:04:48.349093: step 1111, loss 0.344393, acc 0.90625\n",
      "2017-01-10T21:04:50.343918: step 1112, loss 0.242442, acc 0.929688\n",
      "2017-01-10T21:04:52.311420: step 1113, loss 0.260255, acc 0.929688\n",
      "2017-01-10T21:04:54.335787: step 1114, loss 0.23222, acc 0.929688\n",
      "2017-01-10T21:04:56.340358: step 1115, loss 0.289602, acc 0.90625\n",
      "2017-01-10T21:04:58.436728: step 1116, loss 0.231843, acc 0.914062\n",
      "2017-01-10T21:05:00.505536: step 1117, loss 0.325621, acc 0.882812\n",
      "2017-01-10T21:05:02.569237: step 1118, loss 0.240982, acc 0.9375\n",
      "2017-01-10T21:05:04.629029: step 1119, loss 0.235624, acc 0.929688\n",
      "2017-01-10T21:05:06.672539: step 1120, loss 0.102948, acc 0.976562\n",
      "2017-01-10T21:05:08.719331: step 1121, loss 0.172086, acc 0.945312\n",
      "2017-01-10T21:05:10.706464: step 1122, loss 0.286429, acc 0.898438\n",
      "2017-01-10T21:05:12.757122: step 1123, loss 0.321872, acc 0.921875\n",
      "2017-01-10T21:05:14.793269: step 1124, loss 0.107822, acc 0.96875\n",
      "2017-01-10T21:05:16.792637: step 1125, loss 0.20368, acc 0.929688\n",
      "2017-01-10T21:05:18.754673: step 1126, loss 0.158436, acc 0.953125\n",
      "2017-01-10T21:05:20.777280: step 1127, loss 0.335824, acc 0.90625\n",
      "2017-01-10T21:05:22.766582: step 1128, loss 0.219348, acc 0.929688\n",
      "2017-01-10T21:05:24.780023: step 1129, loss 0.279247, acc 0.90625\n",
      "2017-01-10T21:05:26.824644: step 1130, loss 0.239503, acc 0.929688\n",
      "2017-01-10T21:05:28.858563: step 1131, loss 0.177908, acc 0.945312\n",
      "2017-01-10T21:05:31.070780: step 1132, loss 0.238082, acc 0.9375\n",
      "2017-01-10T21:05:33.067283: step 1133, loss 0.343837, acc 0.898438\n",
      "2017-01-10T21:05:35.114182: step 1134, loss 0.286193, acc 0.929688\n",
      "2017-01-10T21:05:37.125638: step 1135, loss 0.300664, acc 0.90625\n",
      "2017-01-10T21:05:39.192586: step 1136, loss 0.177661, acc 0.960938\n",
      "2017-01-10T21:05:41.216256: step 1137, loss 0.243681, acc 0.9375\n",
      "2017-01-10T21:05:43.247636: step 1138, loss 0.314747, acc 0.929688\n",
      "2017-01-10T21:05:45.265421: step 1139, loss 0.231852, acc 0.929688\n",
      "2017-01-10T21:05:47.274895: step 1140, loss 0.241927, acc 0.929688\n",
      "2017-01-10T21:05:49.280136: step 1141, loss 0.295379, acc 0.90625\n",
      "2017-01-10T21:05:51.389619: step 1142, loss 0.333912, acc 0.914062\n",
      "2017-01-10T21:05:53.428038: step 1143, loss 0.357643, acc 0.882812\n",
      "2017-01-10T21:05:55.461792: step 1144, loss 0.172273, acc 0.953125\n",
      "2017-01-10T21:05:57.485208: step 1145, loss 0.431546, acc 0.882812\n",
      "2017-01-10T21:05:59.531850: step 1146, loss 0.231863, acc 0.945312\n",
      "2017-01-10T21:06:01.584650: step 1147, loss 0.192328, acc 0.945312\n",
      "2017-01-10T21:06:03.770272: step 1148, loss 0.208918, acc 0.914062\n",
      "2017-01-10T21:06:05.794613: step 1149, loss 0.235872, acc 0.929688\n",
      "2017-01-10T21:06:07.809286: step 1150, loss 0.232591, acc 0.929688\n",
      "2017-01-10T21:06:09.825599: step 1151, loss 0.0989618, acc 0.96875\n",
      "2017-01-10T21:06:11.808836: step 1152, loss 0.125574, acc 0.976562\n",
      "2017-01-10T21:06:13.851978: step 1153, loss 0.275812, acc 0.890625\n",
      "2017-01-10T21:06:15.838142: step 1154, loss 0.192682, acc 0.921875\n",
      "2017-01-10T21:06:17.792349: step 1155, loss 0.148829, acc 0.953125\n",
      "2017-01-10T21:06:19.892467: step 1156, loss 0.279806, acc 0.90625\n",
      "2017-01-10T21:06:21.916050: step 1157, loss 0.269372, acc 0.9375\n",
      "2017-01-10T21:06:23.909010: step 1158, loss 0.225502, acc 0.929688\n",
      "2017-01-10T21:06:25.907663: step 1159, loss 0.174949, acc 0.945312\n",
      "2017-01-10T21:06:27.939790: step 1160, loss 0.131157, acc 0.96875\n",
      "2017-01-10T21:06:30.147884: step 1161, loss 0.201301, acc 0.953125\n",
      "2017-01-10T21:06:32.182069: step 1162, loss 0.216122, acc 0.9375\n",
      "2017-01-10T21:06:34.259089: step 1163, loss 0.27243, acc 0.914062\n",
      "2017-01-10T21:06:36.288576: step 1164, loss 0.188854, acc 0.929688\n",
      "2017-01-10T21:06:38.287631: step 1165, loss 0.216523, acc 0.929688\n",
      "2017-01-10T21:06:40.241189: step 1166, loss 0.111805, acc 0.960938\n",
      "2017-01-10T21:06:42.279601: step 1167, loss 0.152922, acc 0.960938\n",
      "2017-01-10T21:06:44.657953: step 1168, loss 0.245143, acc 0.921875\n",
      "2017-01-10T21:06:47.286707: step 1169, loss 0.115988, acc 0.96875\n",
      "2017-01-10T21:06:49.293390: step 1170, loss 0.218715, acc 0.929688\n",
      "2017-01-10T21:06:51.355616: step 1171, loss 0.173346, acc 0.960938\n",
      "2017-01-10T21:06:53.368008: step 1172, loss 0.204824, acc 0.921875\n",
      "2017-01-10T21:06:55.400267: step 1173, loss 0.127574, acc 0.960938\n",
      "2017-01-10T21:06:57.425544: step 1174, loss 0.213815, acc 0.921875\n",
      "2017-01-10T21:06:59.415710: step 1175, loss 0.159694, acc 0.929688\n",
      "2017-01-10T21:07:01.458146: step 1176, loss 0.10935, acc 0.96875\n",
      "2017-01-10T21:07:03.528541: step 1177, loss 0.202487, acc 0.929688\n",
      "2017-01-10T21:07:05.563265: step 1178, loss 0.152708, acc 0.960938\n",
      "2017-01-10T21:07:07.747734: step 1179, loss 0.111327, acc 0.960938\n",
      "2017-01-10T21:07:09.772674: step 1180, loss 0.203754, acc 0.921875\n",
      "2017-01-10T21:07:11.738109: step 1181, loss 0.0946583, acc 0.960938\n",
      "2017-01-10T21:07:13.773483: step 1182, loss 0.13983, acc 0.960938\n",
      "2017-01-10T21:07:15.791822: step 1183, loss 0.115432, acc 0.976562\n",
      "2017-01-10T21:07:17.785580: step 1184, loss 0.256801, acc 0.921875\n",
      "2017-01-10T21:07:19.873374: step 1185, loss 0.202867, acc 0.945312\n",
      "2017-01-10T21:07:21.968764: step 1186, loss 0.174907, acc 0.914062\n",
      "2017-01-10T21:07:24.007273: step 1187, loss 0.124287, acc 0.96875\n",
      "2017-01-10T21:07:26.012302: step 1188, loss 0.221206, acc 0.929688\n",
      "2017-01-10T21:07:28.080963: step 1189, loss 0.214435, acc 0.945312\n",
      "2017-01-10T21:07:30.263134: step 1190, loss 0.379617, acc 0.921875\n",
      "2017-01-10T21:07:32.288605: step 1191, loss 0.169659, acc 0.953125\n",
      "2017-01-10T21:07:34.285331: step 1192, loss 0.135228, acc 0.953125\n",
      "2017-01-10T21:07:36.319088: step 1193, loss 0.226978, acc 0.921875\n",
      "2017-01-10T21:07:38.418661: step 1194, loss 0.222583, acc 0.9375\n",
      "2017-01-10T21:07:40.489358: step 1195, loss 0.0553439, acc 0.984375\n",
      "2017-01-10T21:07:42.527072: step 1196, loss 0.216697, acc 0.921875\n",
      "2017-01-10T21:07:45.111728: step 1197, loss 0.213281, acc 0.929688\n",
      "2017-01-10T21:07:47.525985: step 1198, loss 0.183672, acc 0.953125\n",
      "2017-01-10T21:07:49.567304: step 1199, loss 0.138085, acc 0.96875\n",
      "2017-01-10T21:07:51.657009: step 1200, loss 0.240119, acc 0.921875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:08:24.279913: step 1200, loss 0.127913, acc 0.96328\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1200\n",
      "\n",
      "2017-01-10T21:08:28.988347: step 1201, loss 0.242324, acc 0.9375\n",
      "2017-01-10T21:08:30.986268: step 1202, loss 0.211398, acc 0.9375\n",
      "2017-01-10T21:08:33.008796: step 1203, loss 0.221381, acc 0.914062\n",
      "2017-01-10T21:08:35.060747: step 1204, loss 0.270581, acc 0.90625\n",
      "2017-01-10T21:08:37.090068: step 1205, loss 0.29626, acc 0.921875\n",
      "2017-01-10T21:08:39.096454: step 1206, loss 0.114828, acc 0.960938\n",
      "2017-01-10T21:08:41.123689: step 1207, loss 0.106723, acc 0.96875\n",
      "2017-01-10T21:08:43.154713: step 1208, loss 0.090634, acc 0.976562\n",
      "2017-01-10T21:08:45.193724: step 1209, loss 0.118662, acc 0.953125\n",
      "2017-01-10T21:08:47.441567: step 1210, loss 0.203913, acc 0.9375\n",
      "2017-01-10T21:08:49.549732: step 1211, loss 0.16559, acc 0.945312\n",
      "2017-01-10T21:08:51.647841: step 1212, loss 0.158448, acc 0.945312\n",
      "2017-01-10T21:08:53.652105: step 1213, loss 0.198748, acc 0.929688\n",
      "2017-01-10T21:08:55.650764: step 1214, loss 0.356811, acc 0.898438\n",
      "2017-01-10T21:08:57.649271: step 1215, loss 0.414573, acc 0.9375\n",
      "2017-01-10T21:08:59.648997: step 1216, loss 0.123501, acc 0.953125\n",
      "2017-01-10T21:09:01.647148: step 1217, loss 0.155068, acc 0.945312\n",
      "2017-01-10T21:09:03.630826: step 1218, loss 0.291174, acc 0.921875\n",
      "2017-01-10T21:09:05.841219: step 1219, loss 0.244427, acc 0.929688\n",
      "2017-01-10T21:09:07.849841: step 1220, loss 0.183891, acc 0.9375\n",
      "2017-01-10T21:09:09.871523: step 1221, loss 0.309595, acc 0.9375\n",
      "2017-01-10T21:09:11.883381: step 1222, loss 0.199792, acc 0.9375\n",
      "2017-01-10T21:09:13.899073: step 1223, loss 0.224555, acc 0.9375\n",
      "2017-01-10T21:09:15.912880: step 1224, loss 0.242721, acc 0.929688\n",
      "2017-01-10T21:09:17.942551: step 1225, loss 0.209821, acc 0.914062\n",
      "2017-01-10T21:09:20.303267: step 1226, loss 0.164868, acc 0.953125\n",
      "2017-01-10T21:09:23.251619: step 1227, loss 0.0958609, acc 0.960938\n",
      "2017-01-10T21:09:25.267696: step 1228, loss 0.185102, acc 0.945312\n",
      "2017-01-10T21:09:27.307082: step 1229, loss 0.145366, acc 0.960938\n",
      "2017-01-10T21:09:29.311943: step 1230, loss 0.326577, acc 0.890625\n",
      "2017-01-10T21:09:31.312605: step 1231, loss 0.165594, acc 0.953125\n",
      "2017-01-10T21:09:33.322070: step 1232, loss 0.181104, acc 0.9375\n",
      "2017-01-10T21:09:35.390278: step 1233, loss 0.302428, acc 0.921875\n",
      "2017-01-10T21:09:37.424240: step 1234, loss 0.226765, acc 0.9375\n",
      "2017-01-10T21:09:39.432902: step 1235, loss 0.108354, acc 0.976562\n",
      "2017-01-10T21:09:41.455668: step 1236, loss 0.231466, acc 0.90625\n",
      "2017-01-10T21:09:43.466977: step 1237, loss 0.0994484, acc 0.96875\n",
      "2017-01-10T21:09:45.464617: step 1238, loss 0.194828, acc 0.945312\n",
      "2017-01-10T21:09:47.475337: step 1239, loss 0.178583, acc 0.9375\n",
      "2017-01-10T21:09:49.531636: step 1240, loss 0.27642, acc 0.914062\n",
      "2017-01-10T21:09:51.805650: step 1241, loss 0.322946, acc 0.90625\n",
      "2017-01-10T21:09:53.823288: step 1242, loss 0.141349, acc 0.960938\n",
      "2017-01-10T21:09:55.789981: step 1243, loss 0.149445, acc 0.960938\n",
      "2017-01-10T21:09:57.827184: step 1244, loss 0.187605, acc 0.96875\n",
      "2017-01-10T21:09:59.849151: step 1245, loss 0.0928897, acc 0.976562\n",
      "2017-01-10T21:10:01.852603: step 1246, loss 0.196754, acc 0.9375\n",
      "2017-01-10T21:10:03.877920: step 1247, loss 0.177511, acc 0.9375\n",
      "2017-01-10T21:10:05.891123: step 1248, loss 0.210565, acc 0.921875\n",
      "2017-01-10T21:10:07.915759: step 1249, loss 0.190772, acc 0.9375\n",
      "2017-01-10T21:10:09.960326: step 1250, loss 0.179853, acc 0.9375\n",
      "2017-01-10T21:10:11.997530: step 1251, loss 0.150095, acc 0.9375\n",
      "2017-01-10T21:10:13.985835: step 1252, loss 0.145598, acc 0.945312\n",
      "2017-01-10T21:10:16.017393: step 1253, loss 0.198241, acc 0.921875\n",
      "2017-01-10T21:10:18.033206: step 1254, loss 0.310067, acc 0.90625\n",
      "2017-01-10T21:10:20.071027: step 1255, loss 0.321641, acc 0.921875\n",
      "2017-01-10T21:10:22.155284: step 1256, loss 0.254955, acc 0.914062\n",
      "2017-01-10T21:10:24.337451: step 1257, loss 0.248327, acc 0.929688\n",
      "2017-01-10T21:10:26.377645: step 1258, loss 0.274316, acc 0.929688\n",
      "2017-01-10T21:10:28.386293: step 1259, loss 0.290255, acc 0.898438\n",
      "2017-01-10T21:10:30.438502: step 1260, loss 0.129942, acc 0.96875\n",
      "2017-01-10T21:10:32.458078: step 1261, loss 0.199531, acc 0.921875\n",
      "2017-01-10T21:10:34.541118: step 1262, loss 0.229938, acc 0.929688\n",
      "2017-01-10T21:10:36.627660: step 1263, loss 0.229911, acc 0.90625\n",
      "2017-01-10T21:10:38.684933: step 1264, loss 0.238876, acc 0.929688\n",
      "2017-01-10T21:10:40.713140: step 1265, loss 0.179402, acc 0.945312\n",
      "2017-01-10T21:10:42.678710: step 1266, loss 0.270196, acc 0.9375\n",
      "2017-01-10T21:10:44.686346: step 1267, loss 0.237685, acc 0.929688\n",
      "2017-01-10T21:10:46.703704: step 1268, loss 0.208401, acc 0.9375\n",
      "2017-01-10T21:10:48.734075: step 1269, loss 0.256238, acc 0.921875\n",
      "2017-01-10T21:10:50.769150: step 1270, loss 0.224918, acc 0.921875\n",
      "2017-01-10T21:10:52.787709: step 1271, loss 0.146415, acc 0.953125\n",
      "2017-01-10T21:10:54.860144: step 1272, loss 0.20017, acc 0.929688\n",
      "2017-01-10T21:10:56.933873: step 1273, loss 0.194427, acc 0.953125\n",
      "2017-01-10T21:10:58.977295: step 1274, loss 0.195531, acc 0.929688\n",
      "2017-01-10T21:11:01.003689: step 1275, loss 0.134991, acc 0.96875\n",
      "2017-01-10T21:11:03.004100: step 1276, loss 0.153168, acc 0.960938\n",
      "2017-01-10T21:11:05.036617: step 1277, loss 0.128421, acc 0.96875\n",
      "2017-01-10T21:11:07.065903: step 1278, loss 0.18665, acc 0.921875\n",
      "2017-01-10T21:11:09.092043: step 1279, loss 0.227083, acc 0.921875\n",
      "2017-01-10T21:11:11.124911: step 1280, loss 0.263123, acc 0.921875\n",
      "2017-01-10T21:11:13.148668: step 1281, loss 0.20528, acc 0.929688\n",
      "2017-01-10T21:11:15.155124: step 1282, loss 0.191007, acc 0.953125\n",
      "2017-01-10T21:11:17.204263: step 1283, loss 0.165514, acc 0.945312\n",
      "2017-01-10T21:11:19.202684: step 1284, loss 0.0586833, acc 0.976562\n",
      "2017-01-10T21:11:21.259666: step 1285, loss 0.327264, acc 0.882812\n",
      "2017-01-10T21:11:23.323103: step 1286, loss 0.224154, acc 0.953125\n",
      "2017-01-10T21:11:25.317179: step 1287, loss 0.209494, acc 0.929688\n",
      "2017-01-10T21:11:27.472649: step 1288, loss 0.0357926, acc 0.992188\n",
      "2017-01-10T21:11:29.712093: step 1289, loss 0.123126, acc 0.960938\n",
      "2017-01-10T21:11:31.733370: step 1290, loss 0.125958, acc 0.953125\n",
      "2017-01-10T21:11:33.741282: step 1291, loss 0.30608, acc 0.90625\n",
      "2017-01-10T21:11:35.765748: step 1292, loss 0.22992, acc 0.9375\n",
      "2017-01-10T21:11:37.755018: step 1293, loss 0.237397, acc 0.929688\n",
      "2017-01-10T21:11:39.759029: step 1294, loss 0.186571, acc 0.945312\n",
      "2017-01-10T21:11:41.768745: step 1295, loss 0.226461, acc 0.921875\n",
      "2017-01-10T21:11:43.785701: step 1296, loss 0.264616, acc 0.921875\n",
      "2017-01-10T21:11:46.492615: step 1297, loss 0.235089, acc 0.9375\n",
      "2017-01-10T21:11:48.679336: step 1298, loss 0.310421, acc 0.890625\n",
      "2017-01-10T21:11:50.695345: step 1299, loss 0.250628, acc 0.914062\n",
      "2017-01-10T21:11:52.707398: step 1300, loss 0.276328, acc 0.914062\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:12:14.151865: step 1300, loss 0.123804, acc 0.9638\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1300\n",
      "\n",
      "2017-01-10T21:12:18.766847: step 1301, loss 0.278562, acc 0.914062\n",
      "2017-01-10T21:12:20.801778: step 1302, loss 0.127972, acc 0.945312\n",
      "2017-01-10T21:12:22.861258: step 1303, loss 0.258722, acc 0.921875\n",
      "2017-01-10T21:12:24.875697: step 1304, loss 0.306012, acc 0.9375\n",
      "2017-01-10T21:12:26.908504: step 1305, loss 0.124002, acc 0.960938\n",
      "2017-01-10T21:12:29.003225: step 1306, loss 0.14253, acc 0.945312\n",
      "2017-01-10T21:12:31.197534: step 1307, loss 0.248689, acc 0.914062\n",
      "2017-01-10T21:12:33.210095: step 1308, loss 0.279046, acc 0.929688\n",
      "2017-01-10T21:12:35.291253: step 1309, loss 0.225444, acc 0.9375\n",
      "2017-01-10T21:12:37.351530: step 1310, loss 0.122639, acc 0.96875\n",
      "2017-01-10T21:12:39.406249: step 1311, loss 0.181144, acc 0.960938\n",
      "2017-01-10T21:12:41.419450: step 1312, loss 0.309271, acc 0.914062\n",
      "2017-01-10T21:12:43.417759: step 1313, loss 0.171475, acc 0.945312\n",
      "2017-01-10T21:12:45.453966: step 1314, loss 0.207757, acc 0.945312\n",
      "2017-01-10T21:12:47.503259: step 1315, loss 0.19723, acc 0.953125\n",
      "2017-01-10T21:12:49.556858: step 1316, loss 0.222998, acc 0.929688\n",
      "2017-01-10T21:12:51.621124: step 1317, loss 0.207258, acc 0.921875\n",
      "2017-01-10T21:12:53.645268: step 1318, loss 0.23825, acc 0.921875\n",
      "2017-01-10T21:12:55.706421: step 1319, loss 0.20573, acc 0.945312\n",
      "2017-01-10T21:12:57.726474: step 1320, loss 0.257328, acc 0.90625\n",
      "2017-01-10T21:12:59.736548: step 1321, loss 0.179567, acc 0.945312\n",
      "2017-01-10T21:13:01.797287: step 1322, loss 0.185251, acc 0.929688\n",
      "2017-01-10T21:13:04.049904: step 1323, loss 0.0820543, acc 0.976562\n",
      "2017-01-10T21:13:06.069384: step 1324, loss 0.244967, acc 0.914062\n",
      "2017-01-10T21:13:08.103685: step 1325, loss 0.191858, acc 0.953125\n",
      "2017-01-10T21:13:10.160769: step 1326, loss 0.426759, acc 0.867188\n",
      "2017-01-10T21:13:12.151330: step 1327, loss 0.166064, acc 0.960938\n",
      "2017-01-10T21:13:14.205392: step 1328, loss 0.102729, acc 0.953125\n",
      "2017-01-10T21:13:16.234281: step 1329, loss 0.266232, acc 0.9375\n",
      "2017-01-10T21:13:18.280414: step 1330, loss 0.324226, acc 0.945312\n",
      "2017-01-10T21:13:20.372920: step 1331, loss 0.211203, acc 0.945312\n",
      "2017-01-10T21:13:22.425474: step 1332, loss 0.192893, acc 0.9375\n",
      "2017-01-10T21:13:24.457141: step 1333, loss 0.101364, acc 0.976562\n",
      "2017-01-10T21:13:26.503172: step 1334, loss 0.149468, acc 0.953125\n",
      "2017-01-10T21:13:28.544443: step 1335, loss 0.178553, acc 0.929688\n",
      "2017-01-10T21:13:30.580712: step 1336, loss 0.151558, acc 0.960938\n",
      "2017-01-10T21:13:32.639198: step 1337, loss 0.252797, acc 0.898438\n",
      "2017-01-10T21:13:34.892267: step 1338, loss 0.0977412, acc 0.96875\n",
      "2017-01-10T21:13:36.956875: step 1339, loss 0.154789, acc 0.9375\n",
      "2017-01-10T21:13:38.967011: step 1340, loss 0.0781225, acc 0.976562\n",
      "2017-01-10T21:13:40.971461: step 1341, loss 0.126218, acc 0.953125\n",
      "2017-01-10T21:13:43.067700: step 1342, loss 0.112668, acc 0.953125\n",
      "2017-01-10T21:13:45.276964: step 1343, loss 0.273835, acc 0.921875\n",
      "2017-01-10T21:13:47.507675: step 1344, loss 0.20089, acc 0.929688\n",
      "2017-01-10T21:13:49.594578: step 1345, loss 0.151101, acc 0.945312\n",
      "2017-01-10T21:13:52.953186: step 1346, loss 0.147767, acc 0.953125\n",
      "2017-01-10T21:13:55.030659: step 1347, loss 0.243349, acc 0.90625\n",
      "2017-01-10T21:13:57.044020: step 1348, loss 0.162275, acc 0.945312\n",
      "2017-01-10T21:13:59.088715: step 1349, loss 0.0872599, acc 0.96875\n",
      "2017-01-10T21:14:01.144293: step 1350, loss 0.134447, acc 0.953125\n",
      "2017-01-10T21:14:03.156192: step 1351, loss 0.273968, acc 0.921875\n",
      "2017-01-10T21:14:05.232228: step 1352, loss 0.14127, acc 0.960938\n",
      "2017-01-10T21:14:07.438823: step 1353, loss 0.153179, acc 0.9375\n",
      "2017-01-10T21:14:09.442885: step 1354, loss 0.146725, acc 0.9375\n",
      "2017-01-10T21:14:11.445407: step 1355, loss 0.34074, acc 0.921875\n",
      "2017-01-10T21:14:13.458097: step 1356, loss 0.22221, acc 0.953125\n",
      "2017-01-10T21:14:15.480510: step 1357, loss 0.12651, acc 0.953125\n",
      "2017-01-10T21:14:17.487433: step 1358, loss 0.0990992, acc 0.976562\n",
      "2017-01-10T21:14:19.560148: step 1359, loss 0.15192, acc 0.960938\n",
      "2017-01-10T21:14:21.661326: step 1360, loss 0.169643, acc 0.9375\n",
      "2017-01-10T21:14:23.694096: step 1361, loss 0.149293, acc 0.945312\n",
      "2017-01-10T21:14:25.685883: step 1362, loss 0.23788, acc 0.929688\n",
      "2017-01-10T21:14:27.731170: step 1363, loss 0.150892, acc 0.953125\n",
      "2017-01-10T21:14:29.764371: step 1364, loss 0.241482, acc 0.921875\n",
      "2017-01-10T21:14:31.798059: step 1365, loss 0.215084, acc 0.914062\n",
      "2017-01-10T21:14:33.821807: step 1366, loss 0.151701, acc 0.953125\n",
      "2017-01-10T21:14:35.852444: step 1367, loss 0.179091, acc 0.96875\n",
      "2017-01-10T21:14:37.856839: step 1368, loss 0.162899, acc 0.953125\n",
      "2017-01-10T21:14:40.055683: step 1369, loss 0.137785, acc 0.945312\n",
      "2017-01-10T21:14:42.079373: step 1370, loss 0.171557, acc 0.945312\n",
      "2017-01-10T21:14:44.108435: step 1371, loss 0.232847, acc 0.921875\n",
      "2017-01-10T21:14:46.123797: step 1372, loss 0.246729, acc 0.921875\n",
      "2017-01-10T21:14:48.128061: step 1373, loss 0.208502, acc 0.945312\n",
      "2017-01-10T21:14:50.228203: step 1374, loss 0.121409, acc 0.953125\n",
      "2017-01-10T21:14:52.306838: step 1375, loss 0.238222, acc 0.914062\n",
      "2017-01-10T21:14:54.336183: step 1376, loss 0.125016, acc 0.976562\n",
      "2017-01-10T21:14:56.318165: step 1377, loss 0.245478, acc 0.921875\n",
      "2017-01-10T21:14:58.315824: step 1378, loss 0.205496, acc 0.9375\n",
      "2017-01-10T21:15:00.318015: step 1379, loss 0.385885, acc 0.898438\n",
      "2017-01-10T21:15:02.380283: step 1380, loss 0.162074, acc 0.953125\n",
      "2017-01-10T21:15:04.426438: step 1381, loss 0.0999362, acc 0.96875\n",
      "2017-01-10T21:15:06.457301: step 1382, loss 0.161161, acc 0.945312\n",
      "2017-01-10T21:15:08.481780: step 1383, loss 0.119967, acc 0.976562\n",
      "2017-01-10T21:15:10.690897: step 1384, loss 0.1521, acc 0.953125\n",
      "2017-01-10T21:15:12.858235: step 1385, loss 0.124586, acc 0.953125\n",
      "2017-01-10T21:15:14.894479: step 1386, loss 0.173562, acc 0.9375\n",
      "2017-01-10T21:15:16.902373: step 1387, loss 0.145066, acc 0.953125\n",
      "2017-01-10T21:15:18.924554: step 1388, loss 0.214684, acc 0.90625\n",
      "2017-01-10T21:15:20.943186: step 1389, loss 0.22645, acc 0.9375\n",
      "2017-01-10T21:15:22.992776: step 1390, loss 0.236363, acc 0.953125\n",
      "2017-01-10T21:15:25.022013: step 1391, loss 0.121094, acc 0.96875\n",
      "2017-01-10T21:15:27.849003: step 1392, loss 0.185184, acc 0.9375\n",
      "2017-01-10T21:15:30.001869: step 1393, loss 0.191508, acc 0.929688\n",
      "2017-01-10T21:15:32.056374: step 1394, loss 0.138175, acc 0.953125\n",
      "2017-01-10T21:15:34.075344: step 1395, loss 0.122562, acc 0.960938\n",
      "2017-01-10T21:15:36.068974: step 1396, loss 0.291632, acc 0.921875\n",
      "2017-01-10T21:15:38.146425: step 1397, loss 0.140548, acc 0.953125\n",
      "2017-01-10T21:15:40.174071: step 1398, loss 0.22814, acc 0.921875\n",
      "2017-01-10T21:15:42.214404: step 1399, loss 0.1849, acc 0.929688\n",
      "2017-01-10T21:15:44.435041: step 1400, loss 0.14348, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:16:11.686047: step 1400, loss 0.122336, acc 0.96504\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1400\n",
      "\n",
      "2017-01-10T21:16:16.473318: step 1401, loss 0.0739008, acc 0.96875\n",
      "2017-01-10T21:16:18.635014: step 1402, loss 0.243771, acc 0.929688\n",
      "2017-01-10T21:16:20.670097: step 1403, loss 0.0671349, acc 0.976562\n",
      "2017-01-10T21:16:22.740205: step 1404, loss 0.204627, acc 0.945312\n",
      "2017-01-10T21:16:24.761461: step 1405, loss 0.161138, acc 0.945312\n",
      "2017-01-10T21:16:27.341236: step 1406, loss 0.0776472, acc 0.976562\n",
      "2017-01-10T21:16:29.767860: step 1407, loss 0.183362, acc 0.953125\n",
      "2017-01-10T21:16:31.851594: step 1408, loss 0.154033, acc 0.953125\n",
      "2017-01-10T21:16:33.895897: step 1409, loss 0.224426, acc 0.9375\n",
      "2017-01-10T21:16:35.908513: step 1410, loss 0.171064, acc 0.953125\n",
      "2017-01-10T21:16:37.960866: step 1411, loss 0.236067, acc 0.945312\n",
      "2017-01-10T21:16:39.991447: step 1412, loss 0.146922, acc 0.953125\n",
      "2017-01-10T21:16:41.997456: step 1413, loss 0.205626, acc 0.921875\n",
      "2017-01-10T21:16:43.994177: step 1414, loss 0.152276, acc 0.945312\n",
      "2017-01-10T21:16:46.041021: step 1415, loss 0.158949, acc 0.945312\n",
      "2017-01-10T21:16:48.081636: step 1416, loss 0.148534, acc 0.96875\n",
      "2017-01-10T21:16:50.357767: step 1417, loss 0.0332042, acc 0.984375\n",
      "2017-01-10T21:16:52.359079: step 1418, loss 0.155242, acc 0.96875\n",
      "2017-01-10T21:16:54.405206: step 1419, loss 0.212048, acc 0.9375\n",
      "2017-01-10T21:16:56.443703: step 1420, loss 0.0851225, acc 0.976562\n",
      "2017-01-10T21:16:58.468248: step 1421, loss 0.313899, acc 0.898438\n",
      "2017-01-10T21:17:00.508124: step 1422, loss 0.230653, acc 0.929688\n",
      "2017-01-10T21:17:02.555677: step 1423, loss 0.198621, acc 0.929688\n",
      "2017-01-10T21:17:04.583785: step 1424, loss 0.155769, acc 0.9375\n",
      "2017-01-10T21:17:06.602769: step 1425, loss 0.159836, acc 0.945312\n",
      "2017-01-10T21:17:08.601489: step 1426, loss 0.252558, acc 0.929688\n",
      "2017-01-10T21:17:10.645690: step 1427, loss 0.234937, acc 0.929688\n",
      "2017-01-10T21:17:12.679188: step 1428, loss 0.162556, acc 0.945312\n",
      "2017-01-10T21:17:14.679603: step 1429, loss 0.226062, acc 0.929688\n",
      "2017-01-10T21:17:16.718999: step 1430, loss 0.0979685, acc 0.976562\n",
      "2017-01-10T21:17:18.720695: step 1431, loss 0.233728, acc 0.9375\n",
      "2017-01-10T21:17:20.769958: step 1432, loss 0.0727662, acc 0.96875\n",
      "2017-01-10T21:17:22.945222: step 1433, loss 0.170259, acc 0.9375\n",
      "2017-01-10T21:17:24.919749: step 1434, loss 0.111413, acc 0.960938\n",
      "2017-01-10T21:17:26.963548: step 1435, loss 0.182424, acc 0.953125\n",
      "2017-01-10T21:17:28.981556: step 1436, loss 0.0971075, acc 0.976562\n",
      "2017-01-10T21:17:31.014792: step 1437, loss 0.195361, acc 0.953125\n",
      "2017-01-10T21:17:33.007870: step 1438, loss 0.17585, acc 0.9375\n",
      "2017-01-10T21:17:35.063985: step 1439, loss 0.123483, acc 0.960938\n",
      "2017-01-10T21:17:37.083675: step 1440, loss 0.18397, acc 0.9375\n",
      "2017-01-10T21:17:39.079499: step 1441, loss 0.275612, acc 0.921875\n",
      "2017-01-10T21:17:41.112078: step 1442, loss 0.0373704, acc 0.992188\n",
      "2017-01-10T21:17:43.152614: step 1443, loss 0.23254, acc 0.9375\n",
      "2017-01-10T21:17:45.174055: step 1444, loss 0.193822, acc 0.929688\n",
      "2017-01-10T21:17:47.240160: step 1445, loss 0.106127, acc 0.976562\n",
      "2017-01-10T21:17:49.267390: step 1446, loss 0.291199, acc 0.890625\n",
      "2017-01-10T21:17:51.340902: step 1447, loss 0.239537, acc 0.9375\n",
      "2017-01-10T21:17:53.471390: step 1448, loss 0.0970529, acc 0.96875\n",
      "2017-01-10T21:17:55.474045: step 1449, loss 0.131651, acc 0.953125\n",
      "2017-01-10T21:17:57.504813: step 1450, loss 0.232637, acc 0.921875\n",
      "2017-01-10T21:17:59.561084: step 1451, loss 0.170598, acc 0.976562\n",
      "2017-01-10T21:18:01.592546: step 1452, loss 0.155178, acc 0.945312\n",
      "2017-01-10T21:18:03.648845: step 1453, loss 0.0803913, acc 0.976562\n",
      "2017-01-10T21:18:05.690433: step 1454, loss 0.252383, acc 0.914062\n",
      "2017-01-10T21:18:07.730515: step 1455, loss 0.153885, acc 0.945312\n",
      "2017-01-10T21:18:09.781158: step 1456, loss 0.22961, acc 0.945312\n",
      "2017-01-10T21:18:11.835528: step 1457, loss 0.126526, acc 0.960938\n",
      "2017-01-10T21:18:13.889929: step 1458, loss 0.133327, acc 0.960938\n",
      "2017-01-10T21:18:15.960455: step 1459, loss 0.20191, acc 0.921875\n",
      "2017-01-10T21:18:17.970315: step 1460, loss 0.251257, acc 0.921875\n",
      "2017-01-10T21:18:20.043331: step 1461, loss 0.102317, acc 0.976562\n",
      "2017-01-10T21:18:22.147490: step 1462, loss 0.239476, acc 0.929688\n",
      "2017-01-10T21:18:24.204893: step 1463, loss 0.138755, acc 0.953125\n",
      "2017-01-10T21:18:26.407968: step 1464, loss 0.18353, acc 0.9375\n",
      "2017-01-10T21:18:28.463513: step 1465, loss 0.271192, acc 0.929688\n",
      "2017-01-10T21:18:30.458498: step 1466, loss 0.113799, acc 0.960938\n",
      "2017-01-10T21:18:32.495767: step 1467, loss 0.185461, acc 0.929688\n",
      "2017-01-10T21:18:34.570636: step 1468, loss 0.208297, acc 0.9375\n",
      "2017-01-10T21:18:36.559825: step 1469, loss 0.191108, acc 0.929688\n",
      "2017-01-10T21:18:38.608319: step 1470, loss 0.166598, acc 0.960938\n",
      "2017-01-10T21:18:40.651938: step 1471, loss 0.201084, acc 0.953125\n",
      "2017-01-10T21:18:42.687185: step 1472, loss 0.242049, acc 0.921875\n",
      "2017-01-10T21:18:44.748050: step 1473, loss 0.235294, acc 0.921875\n",
      "2017-01-10T21:18:46.743916: step 1474, loss 0.223558, acc 0.921875\n",
      "2017-01-10T21:18:48.778268: step 1475, loss 0.151073, acc 0.945312\n",
      "2017-01-10T21:18:50.845963: step 1476, loss 0.231806, acc 0.929688\n",
      "2017-01-10T21:18:52.858891: step 1477, loss 0.252102, acc 0.90625\n",
      "2017-01-10T21:18:54.895757: step 1478, loss 0.273457, acc 0.914062\n",
      "2017-01-10T21:18:57.012430: step 1479, loss 0.207822, acc 0.953125\n",
      "2017-01-10T21:18:59.045954: step 1480, loss 0.186824, acc 0.945312\n",
      "2017-01-10T21:19:01.106975: step 1481, loss 0.214528, acc 0.929688\n",
      "2017-01-10T21:19:03.121473: step 1482, loss 0.0980945, acc 0.96875\n",
      "2017-01-10T21:19:05.208602: step 1483, loss 0.186523, acc 0.9375\n",
      "2017-01-10T21:19:07.223297: step 1484, loss 0.156905, acc 0.945312\n",
      "2017-01-10T21:19:09.259841: step 1485, loss 0.240649, acc 0.9375\n",
      "2017-01-10T21:19:11.268133: step 1486, loss 0.207323, acc 0.945312\n",
      "2017-01-10T21:19:13.291115: step 1487, loss 0.206516, acc 0.9375\n",
      "2017-01-10T21:19:15.328131: step 1488, loss 0.174554, acc 0.945312\n",
      "2017-01-10T21:19:17.390099: step 1489, loss 0.23532, acc 0.9375\n",
      "2017-01-10T21:19:19.467897: step 1490, loss 0.230323, acc 0.929688\n",
      "2017-01-10T21:19:21.580839: step 1491, loss 0.218969, acc 0.914062\n",
      "2017-01-10T21:19:23.608286: step 1492, loss 0.145109, acc 0.960938\n",
      "2017-01-10T21:19:25.653858: step 1493, loss 0.166137, acc 0.960938\n",
      "2017-01-10T21:19:27.720196: step 1494, loss 0.11786, acc 0.960938\n",
      "2017-01-10T21:19:29.956463: step 1495, loss 0.137323, acc 0.953125\n",
      "2017-01-10T21:19:31.956209: step 1496, loss 0.170958, acc 0.953125\n",
      "2017-01-10T21:19:33.964111: step 1497, loss 0.0893303, acc 0.96875\n",
      "2017-01-10T21:19:35.992773: step 1498, loss 0.102636, acc 0.96875\n",
      "2017-01-10T21:19:37.997295: step 1499, loss 0.0871206, acc 0.96875\n",
      "2017-01-10T21:19:40.091149: step 1500, loss 0.145506, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:20:09.872862: step 1500, loss 0.11671, acc 0.9672\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1500\n",
      "\n",
      "2017-01-10T21:20:14.515787: step 1501, loss 0.0817174, acc 0.96875\n",
      "2017-01-10T21:20:16.548309: step 1502, loss 0.28593, acc 0.9375\n",
      "2017-01-10T21:20:18.564319: step 1503, loss 0.195569, acc 0.929688\n",
      "2017-01-10T21:20:20.673334: step 1504, loss 0.187198, acc 0.953125\n",
      "2017-01-10T21:20:22.730750: step 1505, loss 0.153101, acc 0.953125\n",
      "2017-01-10T21:20:24.784618: step 1506, loss 0.125918, acc 0.960938\n",
      "2017-01-10T21:20:26.858473: step 1507, loss 0.218317, acc 0.9375\n",
      "2017-01-10T21:20:28.958527: step 1508, loss 0.0758613, acc 0.976562\n",
      "2017-01-10T21:20:31.000064: step 1509, loss 0.194361, acc 0.921875\n",
      "2017-01-10T21:20:33.033745: step 1510, loss 0.1052, acc 0.945312\n",
      "2017-01-10T21:20:35.129321: step 1511, loss 0.0778417, acc 0.976562\n",
      "2017-01-10T21:20:37.164851: step 1512, loss 0.188734, acc 0.9375\n",
      "2017-01-10T21:20:39.363228: step 1513, loss 0.248071, acc 0.921875\n",
      "2017-01-10T21:20:41.464319: step 1514, loss 0.144159, acc 0.96875\n",
      "2017-01-10T21:20:43.502250: step 1515, loss 0.116936, acc 0.960938\n",
      "2017-01-10T21:20:45.532143: step 1516, loss 0.256852, acc 0.921875\n",
      "2017-01-10T21:20:47.592428: step 1517, loss 0.135889, acc 0.9375\n",
      "2017-01-10T21:20:49.689652: step 1518, loss 0.0931888, acc 0.96875\n",
      "2017-01-10T21:20:51.781338: step 1519, loss 0.145586, acc 0.976562\n",
      "2017-01-10T21:20:53.783323: step 1520, loss 0.224242, acc 0.921875\n",
      "2017-01-10T21:20:55.838739: step 1521, loss 0.184707, acc 0.9375\n",
      "2017-01-10T21:20:57.901888: step 1522, loss 0.15312, acc 0.953125\n",
      "2017-01-10T21:20:59.964576: step 1523, loss 0.111507, acc 0.96875\n",
      "2017-01-10T21:21:01.999041: step 1524, loss 0.129223, acc 0.96875\n",
      "2017-01-10T21:21:04.062338: step 1525, loss 0.249434, acc 0.929688\n",
      "2017-01-10T21:21:06.119329: step 1526, loss 0.183021, acc 0.953125\n",
      "2017-01-10T21:21:08.140773: step 1527, loss 0.121127, acc 0.96875\n",
      "2017-01-10T21:21:10.172367: step 1528, loss 0.203746, acc 0.921875\n",
      "2017-01-10T21:21:12.372877: step 1529, loss 0.2938, acc 0.914062\n",
      "2017-01-10T21:21:14.441281: step 1530, loss 0.25229, acc 0.945312\n",
      "2017-01-10T21:21:16.777674: step 1531, loss 0.196293, acc 0.921875\n",
      "2017-01-10T21:21:18.895645: step 1532, loss 0.302057, acc 0.921875\n",
      "2017-01-10T21:21:20.970030: step 1533, loss 0.197011, acc 0.960938\n",
      "2017-01-10T21:21:23.029234: step 1534, loss 0.34943, acc 0.921875\n",
      "2017-01-10T21:21:25.072600: step 1535, loss 0.156795, acc 0.945312\n",
      "2017-01-10T21:21:27.084019: step 1536, loss 0.123512, acc 0.96875\n",
      "2017-01-10T21:21:29.150135: step 1537, loss 0.22872, acc 0.9375\n",
      "2017-01-10T21:21:31.187947: step 1538, loss 0.152202, acc 0.953125\n",
      "2017-01-10T21:21:34.054982: step 1539, loss 0.158551, acc 0.9375\n",
      "2017-01-10T21:21:36.902747: step 1540, loss 0.170383, acc 0.9375\n",
      "2017-01-10T21:21:39.161919: step 1541, loss 0.13516, acc 0.960938\n",
      "2017-01-10T21:21:41.160446: step 1542, loss 0.228925, acc 0.929688\n",
      "2017-01-10T21:21:43.376687: step 1543, loss 0.082621, acc 0.96875\n",
      "2017-01-10T21:21:45.386330: step 1544, loss 0.160461, acc 0.945312\n",
      "2017-01-10T21:21:47.443042: step 1545, loss 0.183945, acc 0.953125\n",
      "2017-01-10T21:21:49.528733: step 1546, loss 0.188756, acc 0.945312\n",
      "2017-01-10T21:21:51.602346: step 1547, loss 0.199596, acc 0.945312\n",
      "2017-01-10T21:21:53.657671: step 1548, loss 0.15162, acc 0.96875\n",
      "2017-01-10T21:21:55.690825: step 1549, loss 0.0962458, acc 0.960938\n",
      "2017-01-10T21:21:57.749608: step 1550, loss 0.22837, acc 0.929688\n",
      "2017-01-10T21:21:59.752990: step 1551, loss 0.196847, acc 0.945312\n",
      "2017-01-10T21:22:01.844975: step 1552, loss 0.3207, acc 0.914062\n",
      "2017-01-10T21:22:03.916471: step 1553, loss 0.155869, acc 0.945312\n",
      "2017-01-10T21:22:05.966873: step 1554, loss 0.222706, acc 0.9375\n",
      "2017-01-10T21:22:08.001856: step 1555, loss 0.202999, acc 0.945312\n",
      "2017-01-10T21:22:10.044301: step 1556, loss 0.165231, acc 0.945312\n",
      "2017-01-10T21:22:12.059185: step 1557, loss 0.119675, acc 0.960938\n",
      "2017-01-10T21:22:14.135765: step 1558, loss 0.149375, acc 0.9375\n",
      "2017-01-10T21:22:16.357773: step 1559, loss 0.254233, acc 0.921875\n",
      "2017-01-10T21:22:18.392415: step 1560, loss 0.256644, acc 0.9375\n",
      "2017-01-10T21:22:20.410707: step 1561, loss 0.176988, acc 0.914062\n",
      "2017-01-10T21:22:22.445407: step 1562, loss 0.326483, acc 0.929688\n",
      "2017-01-10T21:22:24.466219: step 1563, loss 0.141024, acc 0.960938\n",
      "2017-01-10T21:22:26.519356: step 1564, loss 0.16217, acc 0.9375\n",
      "2017-01-10T21:22:28.540030: step 1565, loss 0.142164, acc 0.9375\n",
      "2017-01-10T21:22:30.569536: step 1566, loss 0.111424, acc 0.96875\n",
      "2017-01-10T21:22:32.608225: step 1567, loss 0.0952707, acc 0.976562\n",
      "2017-01-10T21:22:34.680798: step 1568, loss 0.122877, acc 0.96875\n",
      "2017-01-10T21:22:36.733287: step 1569, loss 0.0713804, acc 0.976562\n",
      "2017-01-10T21:22:38.768561: step 1570, loss 0.122736, acc 0.960938\n",
      "2017-01-10T21:22:40.770728: step 1571, loss 0.0968174, acc 0.976562\n",
      "2017-01-10T21:22:42.806024: step 1572, loss 0.141377, acc 0.953125\n",
      "2017-01-10T21:22:44.823102: step 1573, loss 0.123561, acc 0.960938\n",
      "2017-01-10T21:22:46.912091: step 1574, loss 0.0830869, acc 0.96875\n",
      "2017-01-10T21:22:48.992281: step 1575, loss 0.264851, acc 0.914062\n",
      "2017-01-10T21:22:51.016447: step 1576, loss 0.275276, acc 0.914062\n",
      "2017-01-10T21:22:53.050900: step 1577, loss 0.25185, acc 0.929688\n",
      "2017-01-10T21:22:55.056706: step 1578, loss 0.193142, acc 0.9375\n",
      "2017-01-10T21:22:57.060644: step 1579, loss 0.097831, acc 0.960938\n",
      "2017-01-10T21:22:59.107845: step 1580, loss 0.196762, acc 0.9375\n",
      "2017-01-10T21:23:01.169098: step 1581, loss 0.0812807, acc 0.96875\n",
      "2017-01-10T21:23:03.255191: step 1582, loss 0.171572, acc 0.960938\n",
      "2017-01-10T21:23:05.288758: step 1583, loss 0.0990772, acc 0.96875\n",
      "2017-01-10T21:23:07.337488: step 1584, loss 0.157671, acc 0.953125\n",
      "2017-01-10T21:23:09.382412: step 1585, loss 0.199474, acc 0.945312\n",
      "2017-01-10T21:23:11.425546: step 1586, loss 0.16357, acc 0.945312\n",
      "2017-01-10T21:23:13.430994: step 1587, loss 0.206994, acc 0.921875\n",
      "2017-01-10T21:23:15.460824: step 1588, loss 0.114699, acc 0.96875\n",
      "2017-01-10T21:23:17.521999: step 1589, loss 0.128303, acc 0.960938\n",
      "2017-01-10T21:23:19.770760: step 1590, loss 0.12236, acc 0.960938\n",
      "2017-01-10T21:23:21.839586: step 1591, loss 0.21039, acc 0.929688\n",
      "2017-01-10T21:23:23.898145: step 1592, loss 0.163137, acc 0.953125\n",
      "2017-01-10T21:23:25.932946: step 1593, loss 0.219993, acc 0.914062\n",
      "2017-01-10T21:23:27.984351: step 1594, loss 0.129663, acc 0.953125\n",
      "2017-01-10T21:23:30.014169: step 1595, loss 0.267797, acc 0.929688\n",
      "2017-01-10T21:23:32.065016: step 1596, loss 0.242932, acc 0.921875\n",
      "2017-01-10T21:23:34.118575: step 1597, loss 0.213829, acc 0.921875\n",
      "2017-01-10T21:23:36.142557: step 1598, loss 0.0866083, acc 0.96875\n",
      "2017-01-10T21:23:38.199007: step 1599, loss 0.22808, acc 0.9375\n",
      "2017-01-10T21:23:40.285014: step 1600, loss 0.126043, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:24:09.001113: step 1600, loss 0.115745, acc 0.96576\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1600\n",
      "\n",
      "2017-01-10T21:24:13.591900: step 1601, loss 0.107821, acc 0.96875\n",
      "2017-01-10T21:24:15.615221: step 1602, loss 0.163568, acc 0.945312\n",
      "2017-01-10T21:24:17.591433: step 1603, loss 0.276473, acc 0.914062\n",
      "2017-01-10T21:24:19.693622: step 1604, loss 0.284744, acc 0.945312\n",
      "2017-01-10T21:24:21.774704: step 1605, loss 0.150454, acc 0.953125\n",
      "2017-01-10T21:24:23.817316: step 1606, loss 0.180178, acc 0.945312\n",
      "2017-01-10T21:24:26.057441: step 1607, loss 0.106634, acc 0.960938\n",
      "2017-01-10T21:24:28.115860: step 1608, loss 0.153137, acc 0.960938\n",
      "2017-01-10T21:24:30.191955: step 1609, loss 0.256209, acc 0.929688\n",
      "2017-01-10T21:24:32.285972: step 1610, loss 0.145761, acc 0.953125\n",
      "2017-01-10T21:24:34.359143: step 1611, loss 0.191736, acc 0.9375\n",
      "2017-01-10T21:24:36.394636: step 1612, loss 0.105738, acc 0.953125\n",
      "2017-01-10T21:24:38.406937: step 1613, loss 0.240893, acc 0.929688\n",
      "2017-01-10T21:24:40.468577: step 1614, loss 0.220372, acc 0.90625\n",
      "2017-01-10T21:24:42.504824: step 1615, loss 0.107438, acc 0.984375\n",
      "2017-01-10T21:24:44.572649: step 1616, loss 0.20778, acc 0.9375\n",
      "2017-01-10T21:24:46.636301: step 1617, loss 0.114273, acc 0.960938\n",
      "2017-01-10T21:24:48.667663: step 1618, loss 0.141047, acc 0.945312\n",
      "2017-01-10T21:24:50.717097: step 1619, loss 0.124186, acc 0.96875\n",
      "2017-01-10T21:24:52.777839: step 1620, loss 0.201803, acc 0.921875\n",
      "2017-01-10T21:24:54.834219: step 1621, loss 0.233643, acc 0.9375\n",
      "2017-01-10T21:24:56.883543: step 1622, loss 0.221248, acc 0.929688\n",
      "2017-01-10T21:24:59.066480: step 1623, loss 0.211973, acc 0.929688\n",
      "2017-01-10T21:25:01.111829: step 1624, loss 0.0602061, acc 0.976562\n",
      "2017-01-10T21:25:03.133573: step 1625, loss 0.0945809, acc 0.96875\n",
      "2017-01-10T21:25:05.207624: step 1626, loss 0.11396, acc 0.960938\n",
      "2017-01-10T21:25:07.225805: step 1627, loss 0.170661, acc 0.945312\n",
      "2017-01-10T21:25:09.274193: step 1628, loss 0.160601, acc 0.945312\n",
      "2017-01-10T21:25:11.323671: step 1629, loss 0.0714834, acc 0.984375\n",
      "2017-01-10T21:25:13.370082: step 1630, loss 0.113962, acc 0.96875\n",
      "2017-01-10T21:25:15.425423: step 1631, loss 0.177486, acc 0.929688\n",
      "2017-01-10T21:25:17.425859: step 1632, loss 0.171603, acc 0.945312\n",
      "2017-01-10T21:25:19.493769: step 1633, loss 0.240714, acc 0.9375\n",
      "2017-01-10T21:25:21.574277: step 1634, loss 0.126059, acc 0.960938\n",
      "2017-01-10T21:25:23.606996: step 1635, loss 0.100763, acc 0.984375\n",
      "2017-01-10T21:25:25.679519: step 1636, loss 0.170354, acc 0.929688\n",
      "2017-01-10T21:25:27.694220: step 1637, loss 0.167806, acc 0.96875\n",
      "2017-01-10T21:25:29.804968: step 1638, loss 0.243753, acc 0.914062\n",
      "2017-01-10T21:25:31.887859: step 1639, loss 0.209948, acc 0.9375\n",
      "2017-01-10T21:25:33.936257: step 1640, loss 0.228334, acc 0.898438\n",
      "2017-01-10T21:25:35.937919: step 1641, loss 0.194221, acc 0.945312\n",
      "2017-01-10T21:25:37.957660: step 1642, loss 0.161409, acc 0.953125\n",
      "2017-01-10T21:25:39.954846: step 1643, loss 0.140693, acc 0.96875\n",
      "2017-01-10T21:25:41.972439: step 1644, loss 0.250191, acc 0.929688\n",
      "2017-01-10T21:25:44.028090: step 1645, loss 0.187233, acc 0.9375\n",
      "2017-01-10T21:25:46.289709: step 1646, loss 0.119945, acc 0.960938\n",
      "2017-01-10T21:25:48.347582: step 1647, loss 0.186335, acc 0.929688\n",
      "2017-01-10T21:25:50.424133: step 1648, loss 0.160504, acc 0.953125\n",
      "2017-01-10T21:25:52.453560: step 1649, loss 0.149617, acc 0.945312\n",
      "2017-01-10T21:25:54.492037: step 1650, loss 0.201107, acc 0.9375\n",
      "2017-01-10T21:25:56.567821: step 1651, loss 0.13697, acc 0.96875\n",
      "2017-01-10T21:25:58.626427: step 1652, loss 0.194614, acc 0.9375\n",
      "2017-01-10T21:26:00.691144: step 1653, loss 0.225322, acc 0.929688\n",
      "2017-01-10T21:26:04.390181: step 1654, loss 0.139021, acc 0.960938\n",
      "2017-01-10T21:26:07.105459: step 1655, loss 0.181086, acc 0.945312\n",
      "2017-01-10T21:26:09.160220: step 1656, loss 0.150706, acc 0.953125\n",
      "2017-01-10T21:26:11.209505: step 1657, loss 0.132687, acc 0.953125\n",
      "2017-01-10T21:26:13.253531: step 1658, loss 0.158341, acc 0.953125\n",
      "2017-01-10T21:26:15.306407: step 1659, loss 0.11154, acc 0.960938\n",
      "2017-01-10T21:26:17.346271: step 1660, loss 0.168859, acc 0.960938\n",
      "2017-01-10T21:26:19.425372: step 1661, loss 0.0825646, acc 0.96875\n",
      "2017-01-10T21:26:21.540078: step 1662, loss 0.0715781, acc 0.96875\n",
      "2017-01-10T21:26:23.606427: step 1663, loss 0.192604, acc 0.929688\n",
      "2017-01-10T21:26:25.644273: step 1664, loss 0.148436, acc 0.953125\n",
      "2017-01-10T21:26:27.719115: step 1665, loss 0.117641, acc 0.953125\n",
      "2017-01-10T21:26:29.782843: step 1666, loss 0.0892138, acc 0.960938\n",
      "2017-01-10T21:26:31.816058: step 1667, loss 0.159891, acc 0.9375\n",
      "2017-01-10T21:26:33.859707: step 1668, loss 0.104168, acc 0.976562\n",
      "2017-01-10T21:26:36.096063: step 1669, loss 0.145397, acc 0.960938\n",
      "2017-01-10T21:26:38.147207: step 1670, loss 0.217978, acc 0.929688\n",
      "2017-01-10T21:26:40.202313: step 1671, loss 0.245181, acc 0.9375\n",
      "2017-01-10T21:26:42.238658: step 1672, loss 0.149046, acc 0.953125\n",
      "2017-01-10T21:26:44.314176: step 1673, loss 0.208712, acc 0.921875\n",
      "2017-01-10T21:26:46.376137: step 1674, loss 0.156001, acc 0.945312\n",
      "2017-01-10T21:26:48.435857: step 1675, loss 0.216935, acc 0.921875\n",
      "2017-01-10T21:26:50.487274: step 1676, loss 0.183066, acc 0.953125\n",
      "2017-01-10T21:26:52.461761: step 1677, loss 0.210736, acc 0.929688\n",
      "2017-01-10T21:26:54.464679: step 1678, loss 0.166748, acc 0.953125\n",
      "2017-01-10T21:26:56.478309: step 1679, loss 0.0958777, acc 0.960938\n",
      "2017-01-10T21:26:58.546778: step 1680, loss 0.146431, acc 0.953125\n",
      "2017-01-10T21:27:00.587383: step 1681, loss 0.0385507, acc 0.992188\n",
      "2017-01-10T21:27:02.633457: step 1682, loss 0.199571, acc 0.929688\n",
      "2017-01-10T21:27:04.789959: step 1683, loss 0.169322, acc 0.945312\n",
      "2017-01-10T21:27:06.891380: step 1684, loss 0.16304, acc 0.960938\n",
      "2017-01-10T21:27:09.072506: step 1685, loss 0.115672, acc 0.96875\n",
      "2017-01-10T21:27:11.114326: step 1686, loss 0.188362, acc 0.945312\n",
      "2017-01-10T21:27:13.156571: step 1687, loss 0.350003, acc 0.890625\n",
      "2017-01-10T21:27:15.193130: step 1688, loss 0.127387, acc 0.953125\n",
      "2017-01-10T21:27:17.226744: step 1689, loss 0.171683, acc 0.945312\n",
      "2017-01-10T21:27:19.259015: step 1690, loss 0.236859, acc 0.9375\n",
      "2017-01-10T21:27:21.368115: step 1691, loss 0.17104, acc 0.945312\n",
      "2017-01-10T21:27:23.648812: step 1692, loss 0.291539, acc 0.921875\n",
      "2017-01-10T21:27:25.687199: step 1693, loss 0.104828, acc 0.953125\n",
      "2017-01-10T21:27:27.773438: step 1694, loss 0.117928, acc 0.9375\n",
      "2017-01-10T21:27:29.845249: step 1695, loss 0.153595, acc 0.953125\n",
      "2017-01-10T21:27:31.888730: step 1696, loss 0.200002, acc 0.953125\n",
      "2017-01-10T21:27:33.957343: step 1697, loss 0.274773, acc 0.9375\n",
      "2017-01-10T21:27:35.993397: step 1698, loss 0.169958, acc 0.9375\n",
      "2017-01-10T21:27:38.473701: step 1699, loss 0.301124, acc 0.90625\n",
      "2017-01-10T21:27:41.295593: step 1700, loss 0.0495213, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:28:13.064051: step 1700, loss 0.112321, acc 0.96688\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1700\n",
      "\n",
      "2017-01-10T21:28:17.884430: step 1701, loss 0.120653, acc 0.9375\n",
      "2017-01-10T21:28:19.936005: step 1702, loss 0.140146, acc 0.96875\n",
      "2017-01-10T21:28:21.939166: step 1703, loss 0.272232, acc 0.945312\n",
      "2017-01-10T21:28:23.919289: step 1704, loss 0.161202, acc 0.945312\n",
      "2017-01-10T21:28:25.920536: step 1705, loss 0.0853214, acc 0.960938\n",
      "2017-01-10T21:28:27.970805: step 1706, loss 0.0794235, acc 0.984375\n",
      "2017-01-10T21:28:29.995286: step 1707, loss 0.208337, acc 0.914062\n",
      "2017-01-10T21:28:32.014957: step 1708, loss 0.18548, acc 0.9375\n",
      "2017-01-10T21:28:34.081027: step 1709, loss 0.220158, acc 0.9375\n",
      "2017-01-10T21:28:36.124557: step 1710, loss 0.255958, acc 0.929688\n",
      "2017-01-10T21:28:38.170387: step 1711, loss 0.144535, acc 0.953125\n",
      "2017-01-10T21:28:40.201686: step 1712, loss 0.174731, acc 0.96875\n",
      "2017-01-10T21:28:42.204097: step 1713, loss 0.0843043, acc 0.96875\n",
      "2017-01-10T21:28:44.241155: step 1714, loss 0.0748685, acc 0.976562\n",
      "2017-01-10T21:28:46.277988: step 1715, loss 0.0785773, acc 0.960938\n",
      "2017-01-10T21:28:48.484412: step 1716, loss 0.0723761, acc 0.976562\n",
      "2017-01-10T21:28:50.580499: step 1717, loss 0.192784, acc 0.929688\n",
      "2017-01-10T21:28:52.622382: step 1718, loss 0.119393, acc 0.953125\n",
      "2017-01-10T21:28:54.581997: step 1719, loss 0.0490974, acc 0.984375\n",
      "2017-01-10T21:28:56.594646: step 1720, loss 0.146735, acc 0.945312\n",
      "2017-01-10T21:28:58.574812: step 1721, loss 0.173361, acc 0.9375\n",
      "2017-01-10T21:29:00.632842: step 1722, loss 0.107971, acc 0.953125\n",
      "2017-01-10T21:29:02.697087: step 1723, loss 0.18868, acc 0.945312\n",
      "2017-01-10T21:29:04.788958: step 1724, loss 0.0873253, acc 0.960938\n",
      "2017-01-10T21:29:06.874525: step 1725, loss 0.0818469, acc 0.984375\n",
      "2017-01-10T21:29:08.925722: step 1726, loss 0.183592, acc 0.929688\n",
      "2017-01-10T21:29:10.969860: step 1727, loss 0.118158, acc 0.96875\n",
      "2017-01-10T21:29:13.020526: step 1728, loss 0.0773262, acc 0.976562\n",
      "2017-01-10T21:29:15.096535: step 1729, loss 0.212347, acc 0.9375\n",
      "2017-01-10T21:29:17.189009: step 1730, loss 0.125404, acc 0.960938\n",
      "2017-01-10T21:29:19.301635: step 1731, loss 0.13085, acc 0.953125\n",
      "2017-01-10T21:29:21.580974: step 1732, loss 0.0711177, acc 0.976562\n",
      "2017-01-10T21:29:23.630607: step 1733, loss 0.149541, acc 0.960938\n",
      "2017-01-10T21:29:25.666943: step 1734, loss 0.0690318, acc 0.984375\n",
      "2017-01-10T21:29:27.711613: step 1735, loss 0.269091, acc 0.929688\n",
      "2017-01-10T21:29:29.696847: step 1736, loss 0.0556507, acc 0.984375\n",
      "2017-01-10T21:29:31.738476: step 1737, loss 0.180825, acc 0.953125\n",
      "2017-01-10T21:29:33.789085: step 1738, loss 0.16187, acc 0.960938\n",
      "2017-01-10T21:29:35.865028: step 1739, loss 0.192195, acc 0.929688\n",
      "2017-01-10T21:29:37.936338: step 1740, loss 0.162619, acc 0.945312\n",
      "2017-01-10T21:29:40.012378: step 1741, loss 0.134189, acc 0.96875\n",
      "2017-01-10T21:29:42.079673: step 1742, loss 0.162075, acc 0.9375\n",
      "2017-01-10T21:29:44.155389: step 1743, loss 0.11123, acc 0.960938\n",
      "2017-01-10T21:29:46.209240: step 1744, loss 0.0735382, acc 0.96875\n",
      "2017-01-10T21:29:48.237798: step 1745, loss 0.151168, acc 0.953125\n",
      "2017-01-10T21:29:50.356030: step 1746, loss 0.125006, acc 0.960938\n",
      "2017-01-10T21:29:52.495739: step 1747, loss 0.0820259, acc 0.976562\n",
      "2017-01-10T21:29:54.573405: step 1748, loss 0.145634, acc 0.945312\n",
      "2017-01-10T21:29:56.602280: step 1749, loss 0.0921932, acc 0.984375\n",
      "2017-01-10T21:29:58.631449: step 1750, loss 0.122527, acc 0.960938\n",
      "2017-01-10T21:30:00.707526: step 1751, loss 0.184791, acc 0.953125\n",
      "2017-01-10T21:30:02.752924: step 1752, loss 0.267293, acc 0.9375\n",
      "2017-01-10T21:30:04.846577: step 1753, loss 0.196653, acc 0.929688\n",
      "2017-01-10T21:30:06.908386: step 1754, loss 0.202146, acc 0.9375\n",
      "2017-01-10T21:30:08.897106: step 1755, loss 0.112413, acc 0.960938\n",
      "2017-01-10T21:30:10.968183: step 1756, loss 0.17464, acc 0.960938\n",
      "2017-01-10T21:30:13.019983: step 1757, loss 0.156031, acc 0.953125\n",
      "2017-01-10T21:30:14.945091: step 1758, loss 0.146329, acc 0.951923\n",
      "2017-01-10T21:30:17.093884: step 1759, loss 0.11046, acc 0.96875\n",
      "2017-01-10T21:30:19.153035: step 1760, loss 0.0730203, acc 0.984375\n",
      "2017-01-10T21:30:21.237081: step 1761, loss 0.0756486, acc 0.976562\n",
      "2017-01-10T21:30:23.298622: step 1762, loss 0.116329, acc 0.96875\n",
      "2017-01-10T21:30:25.500550: step 1763, loss 0.0937328, acc 0.96875\n",
      "2017-01-10T21:30:27.580712: step 1764, loss 0.135048, acc 0.96875\n",
      "2017-01-10T21:30:29.738069: step 1765, loss 0.119012, acc 0.960938\n",
      "2017-01-10T21:30:31.974367: step 1766, loss 0.224293, acc 0.921875\n",
      "2017-01-10T21:30:34.275266: step 1767, loss 0.0684033, acc 0.976562\n",
      "2017-01-10T21:30:36.362682: step 1768, loss 0.111096, acc 0.96875\n",
      "2017-01-10T21:30:38.676371: step 1769, loss 0.0974863, acc 0.976562\n",
      "2017-01-10T21:30:40.748082: step 1770, loss 0.0949364, acc 0.96875\n",
      "2017-01-10T21:30:42.802176: step 1771, loss 0.148416, acc 0.9375\n",
      "2017-01-10T21:30:44.830874: step 1772, loss 0.115593, acc 0.96875\n",
      "2017-01-10T21:30:46.842301: step 1773, loss 0.102004, acc 0.960938\n",
      "2017-01-10T21:30:48.878765: step 1774, loss 0.112608, acc 0.953125\n",
      "2017-01-10T21:30:50.966294: step 1775, loss 0.144609, acc 0.953125\n",
      "2017-01-10T21:30:52.985761: step 1776, loss 0.267895, acc 0.90625\n",
      "2017-01-10T21:30:55.062831: step 1777, loss 0.179803, acc 0.953125\n",
      "2017-01-10T21:30:57.297935: step 1778, loss 0.135793, acc 0.96875\n",
      "2017-01-10T21:30:59.366920: step 1779, loss 0.120822, acc 0.960938\n",
      "2017-01-10T21:31:01.442632: step 1780, loss 0.07551, acc 0.96875\n",
      "2017-01-10T21:31:03.502768: step 1781, loss 0.0759958, acc 0.984375\n",
      "2017-01-10T21:31:05.587145: step 1782, loss 0.0639459, acc 0.992188\n",
      "2017-01-10T21:31:07.658787: step 1783, loss 0.18456, acc 0.953125\n",
      "2017-01-10T21:31:09.675709: step 1784, loss 0.172051, acc 0.945312\n",
      "2017-01-10T21:31:11.702548: step 1785, loss 0.149775, acc 0.960938\n",
      "2017-01-10T21:31:13.754406: step 1786, loss 0.202546, acc 0.929688\n",
      "2017-01-10T21:31:15.791336: step 1787, loss 0.205826, acc 0.9375\n",
      "2017-01-10T21:31:17.857933: step 1788, loss 0.0493023, acc 0.984375\n",
      "2017-01-10T21:31:19.922978: step 1789, loss 0.0880341, acc 0.976562\n",
      "2017-01-10T21:31:22.024414: step 1790, loss 0.0696804, acc 0.960938\n",
      "2017-01-10T21:31:24.070132: step 1791, loss 0.119941, acc 0.960938\n",
      "2017-01-10T21:31:26.124546: step 1792, loss 0.164367, acc 0.960938\n",
      "2017-01-10T21:31:28.375818: step 1793, loss 0.194203, acc 0.9375\n",
      "2017-01-10T21:31:30.888876: step 1794, loss 0.0902916, acc 0.976562\n",
      "2017-01-10T21:31:32.914942: step 1795, loss 0.190071, acc 0.929688\n",
      "2017-01-10T21:31:34.988679: step 1796, loss 0.158603, acc 0.96875\n",
      "2017-01-10T21:31:37.039355: step 1797, loss 0.0963692, acc 0.976562\n",
      "2017-01-10T21:31:39.115398: step 1798, loss 0.0590007, acc 0.984375\n",
      "2017-01-10T21:31:41.169774: step 1799, loss 0.0936898, acc 0.984375\n",
      "2017-01-10T21:31:43.232720: step 1800, loss 0.139721, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:32:10.959898: step 1800, loss 0.108461, acc 0.96796\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1800\n",
      "\n",
      "2017-01-10T21:32:15.697545: step 1801, loss 0.104453, acc 0.96875\n",
      "2017-01-10T21:32:17.725338: step 1802, loss 0.0998594, acc 0.953125\n",
      "2017-01-10T21:32:19.830047: step 1803, loss 0.0929928, acc 0.960938\n",
      "2017-01-10T21:32:21.912716: step 1804, loss 0.107114, acc 0.960938\n",
      "2017-01-10T21:32:24.002192: step 1805, loss 0.183785, acc 0.9375\n",
      "2017-01-10T21:32:26.048480: step 1806, loss 0.200188, acc 0.9375\n",
      "2017-01-10T21:32:28.100249: step 1807, loss 0.199343, acc 0.9375\n",
      "2017-01-10T21:32:30.167492: step 1808, loss 0.120775, acc 0.945312\n",
      "2017-01-10T21:32:32.258591: step 1809, loss 0.0949098, acc 0.984375\n",
      "2017-01-10T21:32:34.292868: step 1810, loss 0.187307, acc 0.9375\n",
      "2017-01-10T21:32:36.331733: step 1811, loss 0.132717, acc 0.96875\n",
      "2017-01-10T21:32:38.531893: step 1812, loss 0.143899, acc 0.945312\n",
      "2017-01-10T21:32:40.597818: step 1813, loss 0.0898767, acc 0.984375\n",
      "2017-01-10T21:32:42.634216: step 1814, loss 0.0888421, acc 0.953125\n",
      "2017-01-10T21:32:44.687592: step 1815, loss 0.160502, acc 0.953125\n",
      "2017-01-10T21:32:46.725666: step 1816, loss 0.12054, acc 0.960938\n",
      "2017-01-10T21:32:48.725822: step 1817, loss 0.0706344, acc 0.976562\n",
      "2017-01-10T21:32:50.774567: step 1818, loss 0.245653, acc 0.929688\n",
      "2017-01-10T21:32:53.237167: step 1819, loss 0.164837, acc 0.953125\n",
      "2017-01-10T21:32:55.569282: step 1820, loss 0.117031, acc 0.976562\n",
      "2017-01-10T21:32:57.638826: step 1821, loss 0.129945, acc 0.945312\n",
      "2017-01-10T21:32:59.692574: step 1822, loss 0.117439, acc 0.960938\n",
      "2017-01-10T21:33:01.735779: step 1823, loss 0.19208, acc 0.945312\n",
      "2017-01-10T21:33:03.805900: step 1824, loss 0.128523, acc 0.953125\n",
      "2017-01-10T21:33:05.848649: step 1825, loss 0.129457, acc 0.976562\n",
      "2017-01-10T21:33:07.913332: step 1826, loss 0.211783, acc 0.929688\n",
      "2017-01-10T21:33:10.114169: step 1827, loss 0.055729, acc 0.984375\n",
      "2017-01-10T21:33:12.129295: step 1828, loss 0.0963624, acc 0.960938\n",
      "2017-01-10T21:33:14.151346: step 1829, loss 0.167348, acc 0.945312\n",
      "2017-01-10T21:33:16.193456: step 1830, loss 0.193099, acc 0.9375\n",
      "2017-01-10T21:33:18.254593: step 1831, loss 0.170623, acc 0.960938\n",
      "2017-01-10T21:33:20.349543: step 1832, loss 0.14695, acc 0.960938\n",
      "2017-01-10T21:33:22.449254: step 1833, loss 0.0833966, acc 0.96875\n",
      "2017-01-10T21:33:24.522243: step 1834, loss 0.0666783, acc 0.976562\n",
      "2017-01-10T21:33:26.559154: step 1835, loss 0.177308, acc 0.953125\n",
      "2017-01-10T21:33:28.637726: step 1836, loss 0.0804371, acc 0.96875\n",
      "2017-01-10T21:33:30.680350: step 1837, loss 0.0845588, acc 0.984375\n",
      "2017-01-10T21:33:32.737759: step 1838, loss 0.140486, acc 0.96875\n",
      "2017-01-10T21:33:34.817609: step 1839, loss 0.178313, acc 0.945312\n",
      "2017-01-10T21:33:36.878891: step 1840, loss 0.123649, acc 0.984375\n",
      "2017-01-10T21:33:38.939258: step 1841, loss 0.2425, acc 0.929688\n",
      "2017-01-10T21:33:41.006641: step 1842, loss 0.114736, acc 0.96875\n",
      "2017-01-10T21:33:43.263356: step 1843, loss 0.0928733, acc 0.960938\n",
      "2017-01-10T21:33:46.136785: step 1844, loss 0.300381, acc 0.960938\n",
      "2017-01-10T21:33:48.262715: step 1845, loss 0.135314, acc 0.960938\n",
      "2017-01-10T21:33:50.361800: step 1846, loss 0.149875, acc 0.953125\n",
      "2017-01-10T21:33:52.463206: step 1847, loss 0.0491969, acc 0.992188\n",
      "2017-01-10T21:33:54.547032: step 1848, loss 0.101692, acc 0.96875\n",
      "2017-01-10T21:33:56.599179: step 1849, loss 0.107855, acc 0.960938\n",
      "2017-01-10T21:33:58.640180: step 1850, loss 0.195965, acc 0.9375\n",
      "2017-01-10T21:34:00.685973: step 1851, loss 0.103249, acc 0.96875\n",
      "2017-01-10T21:34:02.725768: step 1852, loss 0.101846, acc 0.976562\n",
      "2017-01-10T21:34:04.813621: step 1853, loss 0.147376, acc 0.945312\n",
      "2017-01-10T21:34:06.861564: step 1854, loss 0.130209, acc 0.960938\n",
      "2017-01-10T21:34:08.905713: step 1855, loss 0.177479, acc 0.945312\n",
      "2017-01-10T21:34:10.967273: step 1856, loss 0.167184, acc 0.929688\n",
      "2017-01-10T21:34:13.044063: step 1857, loss 0.237796, acc 0.921875\n",
      "2017-01-10T21:34:15.260792: step 1858, loss 0.112609, acc 0.953125\n",
      "2017-01-10T21:34:17.347520: step 1859, loss 0.0543354, acc 0.984375\n",
      "2017-01-10T21:34:19.402533: step 1860, loss 0.110066, acc 0.96875\n",
      "2017-01-10T21:34:21.497999: step 1861, loss 0.108816, acc 0.945312\n",
      "2017-01-10T21:34:23.531160: step 1862, loss 0.173817, acc 0.945312\n",
      "2017-01-10T21:34:25.578411: step 1863, loss 0.105578, acc 0.960938\n",
      "2017-01-10T21:34:27.608855: step 1864, loss 0.101735, acc 0.96875\n",
      "2017-01-10T21:34:29.626302: step 1865, loss 0.106547, acc 0.96875\n",
      "2017-01-10T21:34:31.627433: step 1866, loss 0.101051, acc 0.960938\n",
      "2017-01-10T21:34:33.664471: step 1867, loss 0.147817, acc 0.945312\n",
      "2017-01-10T21:34:35.735090: step 1868, loss 0.123758, acc 0.960938\n",
      "2017-01-10T21:34:37.799110: step 1869, loss 0.102573, acc 0.953125\n",
      "2017-01-10T21:34:39.853324: step 1870, loss 0.101763, acc 0.96875\n",
      "2017-01-10T21:34:41.922465: step 1871, loss 0.155572, acc 0.953125\n",
      "2017-01-10T21:34:44.006106: step 1872, loss 0.230293, acc 0.90625\n",
      "2017-01-10T21:34:46.168577: step 1873, loss 0.146955, acc 0.96875\n",
      "2017-01-10T21:34:48.220274: step 1874, loss 0.217967, acc 0.921875\n",
      "2017-01-10T21:34:50.264764: step 1875, loss 0.114558, acc 0.960938\n",
      "2017-01-10T21:34:52.366681: step 1876, loss 0.129834, acc 0.953125\n",
      "2017-01-10T21:34:54.418586: step 1877, loss 0.175857, acc 0.929688\n",
      "2017-01-10T21:34:56.478084: step 1878, loss 0.176147, acc 0.9375\n",
      "2017-01-10T21:34:58.562461: step 1879, loss 0.154814, acc 0.960938\n",
      "2017-01-10T21:35:00.612547: step 1880, loss 0.167684, acc 0.976562\n",
      "2017-01-10T21:35:02.668908: step 1881, loss 0.146094, acc 0.945312\n",
      "2017-01-10T21:35:04.735941: step 1882, loss 0.224758, acc 0.921875\n",
      "2017-01-10T21:35:06.809047: step 1883, loss 0.0866266, acc 0.96875\n",
      "2017-01-10T21:35:08.849671: step 1884, loss 0.116135, acc 0.976562\n",
      "2017-01-10T21:35:10.942591: step 1885, loss 0.0969259, acc 0.984375\n",
      "2017-01-10T21:35:12.988780: step 1886, loss 0.144378, acc 0.960938\n",
      "2017-01-10T21:35:15.040842: step 1887, loss 0.0946229, acc 0.960938\n",
      "2017-01-10T21:35:17.087559: step 1888, loss 0.0768663, acc 0.96875\n",
      "2017-01-10T21:35:19.327598: step 1889, loss 0.147775, acc 0.945312\n",
      "2017-01-10T21:35:21.450186: step 1890, loss 0.0918018, acc 0.976562\n",
      "2017-01-10T21:35:23.528035: step 1891, loss 0.18943, acc 0.945312\n",
      "2017-01-10T21:35:25.596905: step 1892, loss 0.064459, acc 0.96875\n",
      "2017-01-10T21:35:27.621097: step 1893, loss 0.0775203, acc 0.96875\n",
      "2017-01-10T21:35:29.721511: step 1894, loss 0.134599, acc 0.96875\n",
      "2017-01-10T21:35:31.810716: step 1895, loss 0.131483, acc 0.9375\n",
      "2017-01-10T21:35:33.865392: step 1896, loss 0.120239, acc 0.96875\n",
      "2017-01-10T21:35:35.902979: step 1897, loss 0.119054, acc 0.96875\n",
      "2017-01-10T21:35:37.961749: step 1898, loss 0.186727, acc 0.9375\n",
      "2017-01-10T21:35:40.034039: step 1899, loss 0.162484, acc 0.953125\n",
      "2017-01-10T21:35:42.127599: step 1900, loss 0.199378, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:36:15.546172: step 1900, loss 0.106615, acc 0.96844\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-1900\n",
      "\n",
      "2017-01-10T21:36:20.484947: step 1901, loss 0.0980161, acc 0.96875\n",
      "2017-01-10T21:36:22.736166: step 1902, loss 0.0639632, acc 0.984375\n",
      "2017-01-10T21:36:24.928408: step 1903, loss 0.234084, acc 0.929688\n",
      "2017-01-10T21:36:26.974228: step 1904, loss 0.144066, acc 0.953125\n",
      "2017-01-10T21:36:29.161770: step 1905, loss 0.218046, acc 0.921875\n",
      "2017-01-10T21:36:31.192154: step 1906, loss 0.139671, acc 0.953125\n",
      "2017-01-10T21:36:33.266365: step 1907, loss 0.105788, acc 0.960938\n",
      "2017-01-10T21:36:35.341631: step 1908, loss 0.120785, acc 0.960938\n",
      "2017-01-10T21:36:37.371127: step 1909, loss 0.11497, acc 0.960938\n",
      "2017-01-10T21:36:39.403475: step 1910, loss 0.22694, acc 0.921875\n",
      "2017-01-10T21:36:41.446102: step 1911, loss 0.167244, acc 0.960938\n",
      "2017-01-10T21:36:43.503754: step 1912, loss 0.168856, acc 0.960938\n",
      "2017-01-10T21:36:46.451131: step 1913, loss 0.0806649, acc 0.976562\n",
      "2017-01-10T21:36:48.538253: step 1914, loss 0.0492784, acc 0.984375\n",
      "2017-01-10T21:36:50.657084: step 1915, loss 0.0986777, acc 0.960938\n",
      "2017-01-10T21:36:52.677364: step 1916, loss 0.146112, acc 0.945312\n",
      "2017-01-10T21:36:54.731049: step 1917, loss 0.224397, acc 0.929688\n",
      "2017-01-10T21:36:56.962427: step 1918, loss 0.0690071, acc 0.976562\n",
      "2017-01-10T21:36:59.067790: step 1919, loss 0.077885, acc 0.976562\n",
      "2017-01-10T21:37:01.136827: step 1920, loss 0.159641, acc 0.9375\n",
      "2017-01-10T21:37:03.234857: step 1921, loss 0.136429, acc 0.945312\n",
      "2017-01-10T21:37:05.343371: step 1922, loss 0.156206, acc 0.953125\n",
      "2017-01-10T21:37:07.368445: step 1923, loss 0.0419801, acc 0.984375\n",
      "2017-01-10T21:37:09.416090: step 1924, loss 0.17993, acc 0.929688\n",
      "2017-01-10T21:37:11.496070: step 1925, loss 0.111251, acc 0.960938\n",
      "2017-01-10T21:37:13.530472: step 1926, loss 0.214044, acc 0.9375\n",
      "2017-01-10T21:37:15.574666: step 1927, loss 0.157658, acc 0.953125\n",
      "2017-01-10T21:37:17.621915: step 1928, loss 0.151719, acc 0.953125\n",
      "2017-01-10T21:37:19.697842: step 1929, loss 0.0971331, acc 0.96875\n",
      "2017-01-10T21:37:21.785569: step 1930, loss 0.038247, acc 0.984375\n",
      "2017-01-10T21:37:23.822271: step 1931, loss 0.195478, acc 0.945312\n",
      "2017-01-10T21:37:25.866146: step 1932, loss 0.089442, acc 0.984375\n",
      "2017-01-10T21:37:28.087743: step 1933, loss 0.080048, acc 0.984375\n",
      "2017-01-10T21:37:30.346778: step 1934, loss 0.150531, acc 0.953125\n",
      "2017-01-10T21:37:32.435395: step 1935, loss 0.18401, acc 0.953125\n",
      "2017-01-10T21:37:34.519593: step 1936, loss 0.139958, acc 0.96875\n",
      "2017-01-10T21:37:36.613680: step 1937, loss 0.0928763, acc 0.96875\n",
      "2017-01-10T21:37:38.675145: step 1938, loss 0.102926, acc 0.960938\n",
      "2017-01-10T21:37:40.726930: step 1939, loss 0.131984, acc 0.953125\n",
      "2017-01-10T21:37:42.790787: step 1940, loss 0.113319, acc 0.96875\n",
      "2017-01-10T21:37:45.349332: step 1941, loss 0.246321, acc 0.929688\n",
      "2017-01-10T21:37:47.766197: step 1942, loss 0.114054, acc 0.96875\n",
      "2017-01-10T21:37:49.843188: step 1943, loss 0.0960675, acc 0.96875\n",
      "2017-01-10T21:37:51.980810: step 1944, loss 0.142579, acc 0.96875\n",
      "2017-01-10T21:37:54.061019: step 1945, loss 0.191279, acc 0.9375\n",
      "2017-01-10T21:37:56.151511: step 1946, loss 0.203965, acc 0.929688\n",
      "2017-01-10T21:37:58.191916: step 1947, loss 0.110938, acc 0.953125\n",
      "2017-01-10T21:38:00.460745: step 1948, loss 0.130771, acc 0.96875\n",
      "2017-01-10T21:38:02.520024: step 1949, loss 0.0751526, acc 0.976562\n",
      "2017-01-10T21:38:04.576849: step 1950, loss 0.0402345, acc 0.992188\n",
      "2017-01-10T21:38:06.630470: step 1951, loss 0.0763207, acc 0.976562\n",
      "2017-01-10T21:38:08.645464: step 1952, loss 0.0915094, acc 0.976562\n",
      "2017-01-10T21:38:10.726389: step 1953, loss 0.120545, acc 0.96875\n",
      "2017-01-10T21:38:12.787778: step 1954, loss 0.259382, acc 0.914062\n",
      "2017-01-10T21:38:14.861891: step 1955, loss 0.141304, acc 0.953125\n",
      "2017-01-10T21:38:16.926379: step 1956, loss 0.114449, acc 0.945312\n",
      "2017-01-10T21:38:18.984570: step 1957, loss 0.243588, acc 0.9375\n",
      "2017-01-10T21:38:21.044115: step 1958, loss 0.0982828, acc 0.976562\n",
      "2017-01-10T21:38:23.111683: step 1959, loss 0.150068, acc 0.960938\n",
      "2017-01-10T21:38:25.159033: step 1960, loss 0.0644241, acc 0.984375\n",
      "2017-01-10T21:38:27.223649: step 1961, loss 0.101681, acc 0.96875\n",
      "2017-01-10T21:38:29.318875: step 1962, loss 0.143173, acc 0.953125\n",
      "2017-01-10T21:38:31.409900: step 1963, loss 0.130125, acc 0.9375\n",
      "2017-01-10T21:38:33.587484: step 1964, loss 0.0791066, acc 0.976562\n",
      "2017-01-10T21:38:35.671685: step 1965, loss 0.105918, acc 0.976562\n",
      "2017-01-10T21:38:37.722496: step 1966, loss 0.130737, acc 0.960938\n",
      "2017-01-10T21:38:39.757945: step 1967, loss 0.0546499, acc 0.976562\n",
      "2017-01-10T21:38:41.844155: step 1968, loss 0.0610348, acc 0.984375\n",
      "2017-01-10T21:38:43.913891: step 1969, loss 0.132805, acc 0.96875\n",
      "2017-01-10T21:38:45.981006: step 1970, loss 0.180788, acc 0.929688\n",
      "2017-01-10T21:38:48.042050: step 1971, loss 0.173417, acc 0.945312\n",
      "2017-01-10T21:38:50.195593: step 1972, loss 0.0808202, acc 0.96875\n",
      "2017-01-10T21:38:52.285527: step 1973, loss 0.147752, acc 0.960938\n",
      "2017-01-10T21:38:54.349597: step 1974, loss 0.0925902, acc 0.96875\n",
      "2017-01-10T21:38:56.407731: step 1975, loss 0.0887302, acc 0.96875\n",
      "2017-01-10T21:38:58.494009: step 1976, loss 0.214449, acc 0.9375\n",
      "2017-01-10T21:39:00.540803: step 1977, loss 0.109568, acc 0.96875\n",
      "2017-01-10T21:39:02.597339: step 1978, loss 0.133849, acc 0.953125\n",
      "2017-01-10T21:39:04.869165: step 1979, loss 0.0747143, acc 0.976562\n",
      "2017-01-10T21:39:06.926207: step 1980, loss 0.075266, acc 0.976562\n",
      "2017-01-10T21:39:09.946839: step 1981, loss 0.112764, acc 0.96875\n",
      "2017-01-10T21:39:11.982284: step 1982, loss 0.134902, acc 0.953125\n",
      "2017-01-10T21:39:14.013498: step 1983, loss 0.282411, acc 0.914062\n",
      "2017-01-10T21:39:16.060145: step 1984, loss 0.173356, acc 0.929688\n",
      "2017-01-10T21:39:18.115836: step 1985, loss 0.260618, acc 0.929688\n",
      "2017-01-10T21:39:20.185424: step 1986, loss 0.117151, acc 0.96875\n",
      "2017-01-10T21:39:22.281359: step 1987, loss 0.169565, acc 0.9375\n",
      "2017-01-10T21:39:24.339570: step 1988, loss 0.127232, acc 0.945312\n",
      "2017-01-10T21:39:26.396654: step 1989, loss 0.0973115, acc 0.960938\n",
      "2017-01-10T21:39:28.448679: step 1990, loss 0.21112, acc 0.9375\n",
      "2017-01-10T21:39:30.482327: step 1991, loss 0.094135, acc 0.96875\n",
      "2017-01-10T21:39:32.515349: step 1992, loss 0.0508398, acc 0.992188\n",
      "2017-01-10T21:39:34.712072: step 1993, loss 0.0327477, acc 0.992188\n",
      "2017-01-10T21:39:36.935354: step 1994, loss 0.0927866, acc 0.96875\n",
      "2017-01-10T21:39:38.965526: step 1995, loss 0.166653, acc 0.945312\n",
      "2017-01-10T21:39:41.006189: step 1996, loss 0.0793729, acc 0.976562\n",
      "2017-01-10T21:39:43.072224: step 1997, loss 0.121733, acc 0.953125\n",
      "2017-01-10T21:39:45.146346: step 1998, loss 0.0993232, acc 0.96875\n",
      "2017-01-10T21:39:47.222783: step 1999, loss 0.0453326, acc 0.976562\n",
      "2017-01-10T21:39:49.436402: step 2000, loss 0.139652, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:40:17.597386: step 2000, loss 0.105378, acc 0.96824\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2000\n",
      "\n",
      "2017-01-10T21:40:22.198241: step 2001, loss 0.0837755, acc 0.976562\n",
      "2017-01-10T21:40:24.270130: step 2002, loss 0.118475, acc 0.953125\n",
      "2017-01-10T21:40:26.358710: step 2003, loss 0.167801, acc 0.96875\n",
      "2017-01-10T21:40:28.380331: step 2004, loss 0.0913971, acc 0.96875\n",
      "2017-01-10T21:40:30.424546: step 2005, loss 0.0467207, acc 1\n",
      "2017-01-10T21:40:32.499558: step 2006, loss 0.149313, acc 0.960938\n",
      "2017-01-10T21:40:34.732325: step 2007, loss 0.0897763, acc 0.976562\n",
      "2017-01-10T21:40:36.821375: step 2008, loss 0.141605, acc 0.960938\n",
      "2017-01-10T21:40:38.877187: step 2009, loss 0.211054, acc 0.945312\n",
      "2017-01-10T21:40:40.988786: step 2010, loss 0.166962, acc 0.9375\n",
      "2017-01-10T21:40:43.229670: step 2011, loss 0.110204, acc 0.960938\n",
      "2017-01-10T21:40:45.455471: step 2012, loss 0.139516, acc 0.960938\n",
      "2017-01-10T21:40:47.510784: step 2013, loss 0.0949722, acc 0.984375\n",
      "2017-01-10T21:40:49.897293: step 2014, loss 0.0831253, acc 0.96875\n",
      "2017-01-10T21:40:52.694186: step 2015, loss 0.0605221, acc 0.976562\n",
      "2017-01-10T21:40:54.758783: step 2016, loss 0.143529, acc 0.960938\n",
      "2017-01-10T21:40:56.802583: step 2017, loss 0.100777, acc 0.976562\n",
      "2017-01-10T21:40:58.830357: step 2018, loss 0.158325, acc 0.9375\n",
      "2017-01-10T21:41:00.907708: step 2019, loss 0.148021, acc 0.960938\n",
      "2017-01-10T21:41:02.975782: step 2020, loss 0.143979, acc 0.953125\n",
      "2017-01-10T21:41:05.119858: step 2021, loss 0.0857279, acc 0.96875\n",
      "2017-01-10T21:41:07.190798: step 2022, loss 0.154045, acc 0.9375\n",
      "2017-01-10T21:41:09.254548: step 2023, loss 0.0945514, acc 0.960938\n",
      "2017-01-10T21:41:11.302531: step 2024, loss 0.142997, acc 0.953125\n",
      "2017-01-10T21:41:13.380279: step 2025, loss 0.0489658, acc 0.984375\n",
      "2017-01-10T21:41:15.393196: step 2026, loss 0.150321, acc 0.960938\n",
      "2017-01-10T21:41:17.686646: step 2027, loss 0.107168, acc 0.960938\n",
      "2017-01-10T21:41:19.782406: step 2028, loss 0.199569, acc 0.945312\n",
      "2017-01-10T21:41:21.884531: step 2029, loss 0.16813, acc 0.945312\n",
      "2017-01-10T21:41:23.932211: step 2030, loss 0.139493, acc 0.960938\n",
      "2017-01-10T21:41:26.006000: step 2031, loss 0.106858, acc 0.960938\n",
      "2017-01-10T21:41:28.102582: step 2032, loss 0.114459, acc 0.976562\n",
      "2017-01-10T21:41:30.158969: step 2033, loss 0.181147, acc 0.945312\n",
      "2017-01-10T21:41:32.193205: step 2034, loss 0.0945276, acc 0.96875\n",
      "2017-01-10T21:41:34.228803: step 2035, loss 0.23036, acc 0.929688\n",
      "2017-01-10T21:41:36.281709: step 2036, loss 0.12227, acc 0.96875\n",
      "2017-01-10T21:41:38.317195: step 2037, loss 0.0764274, acc 0.976562\n",
      "2017-01-10T21:41:40.403142: step 2038, loss 0.0814246, acc 0.96875\n",
      "2017-01-10T21:41:42.417436: step 2039, loss 0.101377, acc 0.960938\n",
      "2017-01-10T21:41:44.460154: step 2040, loss 0.208439, acc 0.921875\n",
      "2017-01-10T21:41:46.538011: step 2041, loss 0.157313, acc 0.960938\n",
      "2017-01-10T21:41:48.603650: step 2042, loss 0.085917, acc 0.96875\n",
      "2017-01-10T21:41:50.785972: step 2043, loss 0.10397, acc 0.96875\n",
      "2017-01-10T21:41:52.771186: step 2044, loss 0.28673, acc 0.898438\n",
      "2017-01-10T21:41:54.826361: step 2045, loss 0.0746049, acc 0.984375\n",
      "2017-01-10T21:41:56.889277: step 2046, loss 0.155659, acc 0.9375\n",
      "2017-01-10T21:41:58.960892: step 2047, loss 0.0675683, acc 0.976562\n",
      "2017-01-10T21:42:01.066369: step 2048, loss 0.134881, acc 0.945312\n",
      "2017-01-10T21:42:03.097037: step 2049, loss 0.109705, acc 0.960938\n",
      "2017-01-10T21:42:05.198421: step 2050, loss 0.0746425, acc 0.96875\n",
      "2017-01-10T21:42:07.230978: step 2051, loss 0.349524, acc 0.953125\n",
      "2017-01-10T21:42:09.257137: step 2052, loss 0.190656, acc 0.929688\n",
      "2017-01-10T21:42:11.306112: step 2053, loss 0.0735512, acc 0.992188\n",
      "2017-01-10T21:42:13.337667: step 2054, loss 0.0859289, acc 0.96875\n",
      "2017-01-10T21:42:15.396108: step 2055, loss 0.134434, acc 0.945312\n",
      "2017-01-10T21:42:17.427588: step 2056, loss 0.103479, acc 0.96875\n",
      "2017-01-10T21:42:19.491895: step 2057, loss 0.0389004, acc 0.992188\n",
      "2017-01-10T21:42:21.760390: step 2058, loss 0.120267, acc 0.953125\n",
      "2017-01-10T21:42:23.799755: step 2059, loss 0.0917553, acc 0.96875\n",
      "2017-01-10T21:42:25.828922: step 2060, loss 0.0612944, acc 0.984375\n",
      "2017-01-10T21:42:27.887813: step 2061, loss 0.162358, acc 0.953125\n",
      "2017-01-10T21:42:29.964634: step 2062, loss 0.146521, acc 0.945312\n",
      "2017-01-10T21:42:32.014569: step 2063, loss 0.112946, acc 0.960938\n",
      "2017-01-10T21:42:34.052050: step 2064, loss 0.0909033, acc 0.96875\n",
      "2017-01-10T21:42:36.054057: step 2065, loss 0.197337, acc 0.953125\n",
      "2017-01-10T21:42:38.070675: step 2066, loss 0.049552, acc 0.992188\n",
      "2017-01-10T21:42:40.132713: step 2067, loss 0.12822, acc 0.960938\n",
      "2017-01-10T21:42:42.191838: step 2068, loss 0.170424, acc 0.953125\n",
      "2017-01-10T21:42:44.247789: step 2069, loss 0.0559292, acc 0.976562\n",
      "2017-01-10T21:42:46.317553: step 2070, loss 0.104779, acc 0.976562\n",
      "2017-01-10T21:42:48.338754: step 2071, loss 0.0776622, acc 0.976562\n",
      "2017-01-10T21:42:50.456630: step 2072, loss 0.0793165, acc 0.984375\n",
      "2017-01-10T21:42:52.576602: step 2073, loss 0.0478593, acc 0.984375\n",
      "2017-01-10T21:42:54.807453: step 2074, loss 0.169935, acc 0.945312\n",
      "2017-01-10T21:42:56.861532: step 2075, loss 0.21002, acc 0.921875\n",
      "2017-01-10T21:42:58.879561: step 2076, loss 0.140613, acc 0.945312\n",
      "2017-01-10T21:43:00.965134: step 2077, loss 0.260452, acc 0.929688\n",
      "2017-01-10T21:43:03.030836: step 2078, loss 0.113306, acc 0.96875\n",
      "2017-01-10T21:43:05.057345: step 2079, loss 0.127355, acc 0.960938\n",
      "2017-01-10T21:43:07.162120: step 2080, loss 0.0792722, acc 0.984375\n",
      "2017-01-10T21:43:09.196879: step 2081, loss 0.156199, acc 0.960938\n",
      "2017-01-10T21:43:11.284301: step 2082, loss 0.139899, acc 0.953125\n",
      "2017-01-10T21:43:13.368696: step 2083, loss 0.0891372, acc 0.976562\n",
      "2017-01-10T21:43:15.441157: step 2084, loss 0.0638841, acc 0.96875\n",
      "2017-01-10T21:43:17.515131: step 2085, loss 0.044318, acc 0.984375\n",
      "2017-01-10T21:43:19.562446: step 2086, loss 0.132349, acc 0.96875\n",
      "2017-01-10T21:43:21.649998: step 2087, loss 0.186503, acc 0.921875\n",
      "2017-01-10T21:43:23.739770: step 2088, loss 0.160171, acc 0.9375\n",
      "2017-01-10T21:43:25.974115: step 2089, loss 0.183852, acc 0.929688\n",
      "2017-01-10T21:43:28.065572: step 2090, loss 0.0651333, acc 0.984375\n",
      "2017-01-10T21:43:30.120528: step 2091, loss 0.256475, acc 0.921875\n",
      "2017-01-10T21:43:32.188272: step 2092, loss 0.107263, acc 0.953125\n",
      "2017-01-10T21:43:34.182668: step 2093, loss 0.140614, acc 0.960938\n",
      "2017-01-10T21:43:36.422632: step 2094, loss 0.113054, acc 0.960938\n",
      "2017-01-10T21:43:38.489462: step 2095, loss 0.138522, acc 0.960938\n",
      "2017-01-10T21:43:40.535794: step 2096, loss 0.114043, acc 0.960938\n",
      "2017-01-10T21:43:42.578756: step 2097, loss 0.07235, acc 0.984375\n",
      "2017-01-10T21:43:44.610226: step 2098, loss 0.144868, acc 0.945312\n",
      "2017-01-10T21:43:46.656312: step 2099, loss 0.0667066, acc 0.984375\n",
      "2017-01-10T21:43:48.727533: step 2100, loss 0.125385, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:44:11.738605: step 2100, loss 0.101892, acc 0.97008\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2100\n",
      "\n",
      "2017-01-10T21:44:16.281983: step 2101, loss 0.0880732, acc 0.976562\n",
      "2017-01-10T21:44:18.340235: step 2102, loss 0.0824285, acc 0.976562\n",
      "2017-01-10T21:44:20.426636: step 2103, loss 0.120074, acc 0.960938\n",
      "2017-01-10T21:44:22.535813: step 2104, loss 0.0677861, acc 0.984375\n",
      "2017-01-10T21:44:24.590014: step 2105, loss 0.0820054, acc 0.96875\n",
      "2017-01-10T21:44:26.633673: step 2106, loss 0.154654, acc 0.945312\n",
      "2017-01-10T21:44:28.688224: step 2107, loss 0.0880252, acc 0.976562\n",
      "2017-01-10T21:44:30.950683: step 2108, loss 0.169929, acc 0.953125\n",
      "2017-01-10T21:44:32.999021: step 2109, loss 0.123919, acc 0.953125\n",
      "2017-01-10T21:44:35.091546: step 2110, loss 0.0613001, acc 0.984375\n",
      "2017-01-10T21:44:37.132257: step 2111, loss 0.100194, acc 0.976562\n",
      "2017-01-10T21:44:39.172912: step 2112, loss 0.19129, acc 0.9375\n",
      "2017-01-10T21:44:41.226889: step 2113, loss 0.250445, acc 0.960938\n",
      "2017-01-10T21:44:43.269909: step 2114, loss 0.072679, acc 0.96875\n",
      "2017-01-10T21:44:45.321139: step 2115, loss 0.179476, acc 0.953125\n",
      "2017-01-10T21:44:47.399795: step 2116, loss 0.0781599, acc 0.984375\n",
      "2017-01-10T21:44:49.480023: step 2117, loss 0.104372, acc 0.960938\n",
      "2017-01-10T21:44:51.582066: step 2118, loss 0.114298, acc 0.960938\n",
      "2017-01-10T21:44:53.632483: step 2119, loss 0.0323621, acc 0.992188\n",
      "2017-01-10T21:44:55.692208: step 2120, loss 0.0683478, acc 0.96875\n",
      "2017-01-10T21:44:57.721100: step 2121, loss 0.111484, acc 0.984375\n",
      "2017-01-10T21:44:59.724228: step 2122, loss 0.101685, acc 0.96875\n",
      "2017-01-10T21:45:01.877436: step 2123, loss 0.0567297, acc 0.984375\n",
      "2017-01-10T21:45:03.973156: step 2124, loss 0.10878, acc 0.945312\n",
      "2017-01-10T21:45:06.027434: step 2125, loss 0.119578, acc 0.96875\n",
      "2017-01-10T21:45:08.033231: step 2126, loss 0.0763708, acc 0.976562\n",
      "2017-01-10T21:45:10.075281: step 2127, loss 0.165745, acc 0.9375\n",
      "2017-01-10T21:45:12.132676: step 2128, loss 0.145602, acc 0.945312\n",
      "2017-01-10T21:45:14.193850: step 2129, loss 0.161455, acc 0.953125\n",
      "2017-01-10T21:45:16.257122: step 2130, loss 0.117904, acc 0.96875\n",
      "2017-01-10T21:45:18.343617: step 2131, loss 0.0965618, acc 0.96875\n",
      "2017-01-10T21:45:20.467802: step 2132, loss 0.142599, acc 0.953125\n",
      "2017-01-10T21:45:22.523254: step 2133, loss 0.119897, acc 0.960938\n",
      "2017-01-10T21:45:24.522676: step 2134, loss 0.181244, acc 0.929688\n",
      "2017-01-10T21:45:26.574119: step 2135, loss 0.173364, acc 0.953125\n",
      "2017-01-10T21:45:28.613609: step 2136, loss 0.122381, acc 0.960938\n",
      "2017-01-10T21:45:30.669579: step 2137, loss 0.0844531, acc 0.96875\n",
      "2017-01-10T21:45:32.717184: step 2138, loss 0.101554, acc 0.960938\n",
      "2017-01-10T21:45:35.037504: step 2139, loss 0.076674, acc 0.960938\n",
      "2017-01-10T21:45:37.098670: step 2140, loss 0.0966434, acc 0.960938\n",
      "2017-01-10T21:45:39.254035: step 2141, loss 0.25237, acc 0.921875\n",
      "2017-01-10T21:45:41.896977: step 2142, loss 0.0910689, acc 0.976562\n",
      "2017-01-10T21:45:43.985242: step 2143, loss 0.104135, acc 0.96875\n",
      "2017-01-10T21:45:46.048672: step 2144, loss 0.121675, acc 0.96875\n",
      "2017-01-10T21:45:48.081009: step 2145, loss 0.17046, acc 0.953125\n",
      "2017-01-10T21:45:50.168658: step 2146, loss 0.159618, acc 0.960938\n",
      "2017-01-10T21:45:52.284161: step 2147, loss 0.147261, acc 0.945312\n",
      "2017-01-10T21:45:54.327950: step 2148, loss 0.118189, acc 0.976562\n",
      "2017-01-10T21:45:57.279227: step 2149, loss 0.115934, acc 0.976562\n",
      "2017-01-10T21:46:00.172519: step 2150, loss 0.111429, acc 0.96875\n",
      "2017-01-10T21:46:02.354548: step 2151, loss 0.113549, acc 0.960938\n",
      "2017-01-10T21:46:04.436290: step 2152, loss 0.0833422, acc 0.96875\n",
      "2017-01-10T21:46:06.713521: step 2153, loss 0.104637, acc 0.96875\n",
      "2017-01-10T21:46:08.795396: step 2154, loss 0.0939336, acc 0.96875\n",
      "2017-01-10T21:46:10.866132: step 2155, loss 0.179362, acc 0.953125\n",
      "2017-01-10T21:46:12.959675: step 2156, loss 0.0702443, acc 0.96875\n",
      "2017-01-10T21:46:15.027451: step 2157, loss 0.14003, acc 0.953125\n",
      "2017-01-10T21:46:17.107829: step 2158, loss 0.137929, acc 0.960938\n",
      "2017-01-10T21:46:19.141492: step 2159, loss 0.108564, acc 0.960938\n",
      "2017-01-10T21:46:21.228870: step 2160, loss 0.117955, acc 0.96875\n",
      "2017-01-10T21:46:23.324197: step 2161, loss 0.063712, acc 0.96875\n",
      "2017-01-10T21:46:25.377091: step 2162, loss 0.102638, acc 0.984375\n",
      "2017-01-10T21:46:27.463694: step 2163, loss 0.166073, acc 0.96875\n",
      "2017-01-10T21:46:29.519870: step 2164, loss 0.105935, acc 0.96875\n",
      "2017-01-10T21:46:31.567091: step 2165, loss 0.0817703, acc 0.960938\n",
      "2017-01-10T21:46:33.620030: step 2166, loss 0.195914, acc 0.953125\n",
      "2017-01-10T21:46:35.741836: step 2167, loss 0.123026, acc 0.953125\n",
      "2017-01-10T21:46:37.802658: step 2168, loss 0.0460233, acc 0.984375\n",
      "2017-01-10T21:46:39.981786: step 2169, loss 0.147299, acc 0.96875\n",
      "2017-01-10T21:46:42.027788: step 2170, loss 0.23559, acc 0.9375\n",
      "2017-01-10T21:46:44.092505: step 2171, loss 0.214393, acc 0.929688\n",
      "2017-01-10T21:46:46.107094: step 2172, loss 0.087595, acc 0.96875\n",
      "2017-01-10T21:46:48.175873: step 2173, loss 0.0966939, acc 0.96875\n",
      "2017-01-10T21:46:50.270663: step 2174, loss 0.0295948, acc 1\n",
      "2017-01-10T21:46:52.401327: step 2175, loss 0.1106, acc 0.960938\n",
      "2017-01-10T21:46:54.470099: step 2176, loss 0.132307, acc 0.96875\n",
      "2017-01-10T21:46:56.524570: step 2177, loss 0.133347, acc 0.960938\n",
      "2017-01-10T21:46:58.575089: step 2178, loss 0.15739, acc 0.921875\n",
      "2017-01-10T21:47:00.654790: step 2179, loss 0.0721442, acc 0.96875\n",
      "2017-01-10T21:47:02.707320: step 2180, loss 0.0989874, acc 0.96875\n",
      "2017-01-10T21:47:04.814949: step 2181, loss 0.05863, acc 0.984375\n",
      "2017-01-10T21:47:06.892453: step 2182, loss 0.136047, acc 0.960938\n",
      "2017-01-10T21:47:08.977591: step 2183, loss 0.0839999, acc 0.960938\n",
      "2017-01-10T21:47:11.427407: step 2184, loss 0.15807, acc 0.953125\n",
      "2017-01-10T21:47:13.457333: step 2185, loss 0.0842073, acc 0.96875\n",
      "2017-01-10T21:47:15.518846: step 2186, loss 0.13003, acc 0.960938\n",
      "2017-01-10T21:47:17.560412: step 2187, loss 0.168646, acc 0.953125\n",
      "2017-01-10T21:47:19.645488: step 2188, loss 0.0545945, acc 0.984375\n",
      "2017-01-10T21:47:21.762769: step 2189, loss 0.127377, acc 0.953125\n",
      "2017-01-10T21:47:23.856271: step 2190, loss 0.0679871, acc 0.976562\n",
      "2017-01-10T21:47:25.948036: step 2191, loss 0.162372, acc 0.953125\n",
      "2017-01-10T21:47:28.002298: step 2192, loss 0.0635981, acc 0.984375\n",
      "2017-01-10T21:47:30.068814: step 2193, loss 0.23923, acc 0.90625\n",
      "2017-01-10T21:47:32.106156: step 2194, loss 0.107116, acc 0.960938\n",
      "2017-01-10T21:47:34.168004: step 2195, loss 0.0836781, acc 0.960938\n",
      "2017-01-10T21:47:36.283738: step 2196, loss 0.139972, acc 0.953125\n",
      "2017-01-10T21:47:38.271069: step 2197, loss 0.138177, acc 0.953125\n",
      "2017-01-10T21:47:40.329556: step 2198, loss 0.0277483, acc 1\n",
      "2017-01-10T21:47:42.485351: step 2199, loss 0.0913616, acc 0.960938\n",
      "2017-01-10T21:47:44.555279: step 2200, loss 0.138839, acc 0.945312\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:48:20.443684: step 2200, loss 0.100879, acc 0.96988\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2200\n",
      "\n",
      "2017-01-10T21:48:25.926517: step 2201, loss 0.109048, acc 0.976562\n",
      "2017-01-10T21:48:27.975349: step 2202, loss 0.094069, acc 0.96875\n",
      "2017-01-10T21:48:30.006753: step 2203, loss 0.152211, acc 0.960938\n",
      "2017-01-10T21:48:32.098265: step 2204, loss 0.0954393, acc 0.960938\n",
      "2017-01-10T21:48:34.183308: step 2205, loss 0.107932, acc 0.96875\n",
      "2017-01-10T21:48:36.215203: step 2206, loss 0.120518, acc 0.945312\n",
      "2017-01-10T21:48:38.318227: step 2207, loss 0.115817, acc 0.976562\n",
      "2017-01-10T21:48:40.578945: step 2208, loss 0.0766044, acc 0.96875\n",
      "2017-01-10T21:48:42.838427: step 2209, loss 0.0755564, acc 0.976562\n",
      "2017-01-10T21:48:45.029498: step 2210, loss 0.125614, acc 0.945312\n",
      "2017-01-10T21:48:47.144067: step 2211, loss 0.0969612, acc 0.984375\n",
      "2017-01-10T21:48:49.403210: step 2212, loss 0.193002, acc 0.945312\n",
      "2017-01-10T21:48:51.682936: step 2213, loss 0.149215, acc 0.960938\n",
      "2017-01-10T21:48:53.745269: step 2214, loss 0.109331, acc 0.984375\n",
      "2017-01-10T21:48:55.789448: step 2215, loss 0.0496666, acc 0.984375\n",
      "2017-01-10T21:48:57.816140: step 2216, loss 0.135151, acc 0.945312\n",
      "2017-01-10T21:48:59.853192: step 2217, loss 0.0431433, acc 0.976562\n",
      "2017-01-10T21:49:01.885102: step 2218, loss 0.134137, acc 0.945312\n",
      "2017-01-10T21:49:03.944491: step 2219, loss 0.106151, acc 0.960938\n",
      "2017-01-10T21:49:05.976481: step 2220, loss 0.0763272, acc 0.984375\n",
      "2017-01-10T21:49:08.054082: step 2221, loss 0.0806636, acc 0.984375\n",
      "2017-01-10T21:49:10.115530: step 2222, loss 0.0715803, acc 0.976562\n",
      "2017-01-10T21:49:12.203483: step 2223, loss 0.0614712, acc 0.984375\n",
      "2017-01-10T21:49:14.274825: step 2224, loss 0.143606, acc 0.945312\n",
      "2017-01-10T21:49:16.358825: step 2225, loss 0.138441, acc 0.960938\n",
      "2017-01-10T21:49:18.395643: step 2226, loss 0.125423, acc 0.976562\n",
      "2017-01-10T21:49:20.532682: step 2227, loss 0.0939488, acc 0.96875\n",
      "2017-01-10T21:49:22.610808: step 2228, loss 0.0415996, acc 0.984375\n",
      "2017-01-10T21:49:24.889719: step 2229, loss 0.121584, acc 0.96875\n",
      "2017-01-10T21:49:26.907545: step 2230, loss 0.109266, acc 0.960938\n",
      "2017-01-10T21:49:28.956191: step 2231, loss 0.106295, acc 0.960938\n",
      "2017-01-10T21:49:31.108039: step 2232, loss 0.0915494, acc 0.976562\n",
      "2017-01-10T21:49:33.180214: step 2233, loss 0.0917926, acc 0.976562\n",
      "2017-01-10T21:49:35.255898: step 2234, loss 0.408787, acc 0.914062\n",
      "2017-01-10T21:49:37.311749: step 2235, loss 0.0859724, acc 0.976562\n",
      "2017-01-10T21:49:39.371931: step 2236, loss 0.123156, acc 0.945312\n",
      "2017-01-10T21:49:41.528158: step 2237, loss 0.167847, acc 0.945312\n",
      "2017-01-10T21:49:43.547366: step 2238, loss 0.0553799, acc 0.976562\n",
      "2017-01-10T21:49:45.581523: step 2239, loss 0.092208, acc 0.96875\n",
      "2017-01-10T21:49:47.645950: step 2240, loss 0.107855, acc 0.976562\n",
      "2017-01-10T21:49:49.759177: step 2241, loss 0.0809961, acc 0.976562\n",
      "2017-01-10T21:49:51.871513: step 2242, loss 0.126212, acc 0.976562\n",
      "2017-01-10T21:49:53.929316: step 2243, loss 0.108421, acc 0.960938\n",
      "2017-01-10T21:49:56.475781: step 2244, loss 0.134599, acc 0.960938\n",
      "2017-01-10T21:49:59.398658: step 2245, loss 0.0643003, acc 0.976562\n",
      "2017-01-10T21:50:01.436798: step 2246, loss 0.0791781, acc 0.96875\n",
      "2017-01-10T21:50:03.518791: step 2247, loss 0.12625, acc 0.96875\n",
      "2017-01-10T21:50:05.581760: step 2248, loss 0.296014, acc 0.898438\n",
      "2017-01-10T21:50:07.675266: step 2249, loss 0.147006, acc 0.953125\n",
      "2017-01-10T21:50:09.744822: step 2250, loss 0.400045, acc 0.960938\n",
      "2017-01-10T21:50:11.821050: step 2251, loss 0.117392, acc 0.976562\n",
      "2017-01-10T21:50:13.919515: step 2252, loss 0.104478, acc 0.976562\n",
      "2017-01-10T21:50:16.027255: step 2253, loss 0.08691, acc 0.960938\n",
      "2017-01-10T21:50:18.106457: step 2254, loss 0.139528, acc 0.945312\n",
      "2017-01-10T21:50:20.212717: step 2255, loss 0.0866587, acc 0.976562\n",
      "2017-01-10T21:50:22.334379: step 2256, loss 0.272075, acc 0.914062\n",
      "2017-01-10T21:50:24.405195: step 2257, loss 0.0930134, acc 0.976562\n",
      "2017-01-10T21:50:26.488365: step 2258, loss 0.260941, acc 0.90625\n",
      "2017-01-10T21:50:28.721420: step 2259, loss 0.085333, acc 0.960938\n",
      "2017-01-10T21:50:30.809074: step 2260, loss 0.178413, acc 0.953125\n",
      "2017-01-10T21:50:32.845484: step 2261, loss 0.0311451, acc 0.992188\n",
      "2017-01-10T21:50:34.954767: step 2262, loss 0.179595, acc 0.953125\n",
      "2017-01-10T21:50:37.030001: step 2263, loss 0.153012, acc 0.945312\n",
      "2017-01-10T21:50:39.041378: step 2264, loss 0.172081, acc 0.953125\n",
      "2017-01-10T21:50:41.200190: step 2265, loss 0.0872976, acc 0.96875\n",
      "2017-01-10T21:50:43.287239: step 2266, loss 0.182536, acc 0.9375\n",
      "2017-01-10T21:50:45.337282: step 2267, loss 0.148129, acc 0.945312\n",
      "2017-01-10T21:50:47.398989: step 2268, loss 0.0955836, acc 0.953125\n",
      "2017-01-10T21:50:49.461183: step 2269, loss 0.112812, acc 0.96875\n",
      "2017-01-10T21:50:51.546926: step 2270, loss 0.120106, acc 0.960938\n",
      "2017-01-10T21:50:53.632165: step 2271, loss 0.0467062, acc 0.992188\n",
      "2017-01-10T21:50:55.697797: step 2272, loss 0.097947, acc 0.976562\n",
      "2017-01-10T21:50:58.676676: step 2273, loss 0.166865, acc 0.96875\n",
      "2017-01-10T21:51:00.992305: step 2274, loss 0.13731, acc 0.976562\n",
      "2017-01-10T21:51:03.056960: step 2275, loss 0.0648606, acc 0.984375\n",
      "2017-01-10T21:51:05.151443: step 2276, loss 0.0752039, acc 0.96875\n",
      "2017-01-10T21:51:07.182115: step 2277, loss 0.0256379, acc 0.992188\n",
      "2017-01-10T21:51:09.266372: step 2278, loss 0.0646322, acc 0.984375\n",
      "2017-01-10T21:51:11.345509: step 2279, loss 0.113604, acc 0.976562\n",
      "2017-01-10T21:51:13.351726: step 2280, loss 0.123378, acc 0.9375\n",
      "2017-01-10T21:51:15.399612: step 2281, loss 0.110727, acc 0.960938\n",
      "2017-01-10T21:51:17.434965: step 2282, loss 0.1265, acc 0.960938\n",
      "2017-01-10T21:51:19.484735: step 2283, loss 0.20805, acc 0.9375\n",
      "2017-01-10T21:51:21.571187: step 2284, loss 0.108281, acc 0.96875\n",
      "2017-01-10T21:51:23.614952: step 2285, loss 0.32169, acc 0.976562\n",
      "2017-01-10T21:51:25.590723: step 2286, loss 0.141555, acc 0.9375\n",
      "2017-01-10T21:51:27.640159: step 2287, loss 0.178637, acc 0.921875\n",
      "2017-01-10T21:51:29.679538: step 2288, loss 0.10148, acc 0.976562\n",
      "2017-01-10T21:51:31.748037: step 2289, loss 0.106339, acc 0.96875\n",
      "2017-01-10T21:51:33.991783: step 2290, loss 0.0736024, acc 0.976562\n",
      "2017-01-10T21:51:36.046752: step 2291, loss 0.102706, acc 0.960938\n",
      "2017-01-10T21:51:38.081618: step 2292, loss 0.0745269, acc 0.976562\n",
      "2017-01-10T21:51:40.143444: step 2293, loss 0.101039, acc 0.976562\n",
      "2017-01-10T21:51:42.397415: step 2294, loss 0.118554, acc 0.945312\n",
      "2017-01-10T21:51:44.458213: step 2295, loss 0.161226, acc 0.96875\n",
      "2017-01-10T21:51:46.480636: step 2296, loss 0.0671267, acc 0.976562\n",
      "2017-01-10T21:51:48.539140: step 2297, loss 0.187998, acc 0.9375\n",
      "2017-01-10T21:51:50.619883: step 2298, loss 0.145827, acc 0.953125\n",
      "2017-01-10T21:51:52.644425: step 2299, loss 0.068963, acc 0.984375\n",
      "2017-01-10T21:51:54.724362: step 2300, loss 0.0613427, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:52:21.430906: step 2300, loss 0.098048, acc 0.97112\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2300\n",
      "\n",
      "2017-01-10T21:52:26.018064: step 2301, loss 0.126508, acc 0.953125\n",
      "2017-01-10T21:52:28.117703: step 2302, loss 0.0781448, acc 0.976562\n",
      "2017-01-10T21:52:30.193018: step 2303, loss 0.1248, acc 0.953125\n",
      "2017-01-10T21:52:32.255592: step 2304, loss 0.169308, acc 0.953125\n",
      "2017-01-10T21:52:34.380655: step 2305, loss 0.156395, acc 0.9375\n",
      "2017-01-10T21:52:36.486834: step 2306, loss 0.135052, acc 0.960938\n",
      "2017-01-10T21:52:38.810170: step 2307, loss 0.0679703, acc 0.96875\n",
      "2017-01-10T21:52:40.913771: step 2308, loss 0.101122, acc 0.960938\n",
      "2017-01-10T21:52:42.968543: step 2309, loss 0.0526792, acc 0.992188\n",
      "2017-01-10T21:52:45.046198: step 2310, loss 0.10792, acc 0.96875\n",
      "2017-01-10T21:52:47.144802: step 2311, loss 0.127618, acc 0.953125\n",
      "2017-01-10T21:52:49.206996: step 2312, loss 0.119853, acc 0.960938\n",
      "2017-01-10T21:52:51.311427: step 2313, loss 0.0242837, acc 0.992188\n",
      "2017-01-10T21:52:53.493742: step 2314, loss 0.100353, acc 0.960938\n",
      "2017-01-10T21:52:55.536266: step 2315, loss 0.281434, acc 0.921875\n",
      "2017-01-10T21:52:57.624977: step 2316, loss 0.0586563, acc 0.960938\n",
      "2017-01-10T21:52:59.674586: step 2317, loss 0.180838, acc 0.945312\n",
      "2017-01-10T21:53:01.735678: step 2318, loss 0.21942, acc 0.945312\n",
      "2017-01-10T21:53:03.852348: step 2319, loss 0.206749, acc 0.9375\n",
      "2017-01-10T21:53:05.911038: step 2320, loss 0.0119598, acc 1\n",
      "2017-01-10T21:53:07.973946: step 2321, loss 0.168638, acc 0.9375\n",
      "2017-01-10T21:53:10.033537: step 2322, loss 0.0485786, acc 0.984375\n",
      "2017-01-10T21:53:12.242318: step 2323, loss 0.134386, acc 0.960938\n",
      "2017-01-10T21:53:14.310628: step 2324, loss 0.0357561, acc 0.992188\n",
      "2017-01-10T21:53:16.382645: step 2325, loss 0.0919704, acc 0.96875\n",
      "2017-01-10T21:53:18.479281: step 2326, loss 0.0352936, acc 0.992188\n",
      "2017-01-10T21:53:20.595905: step 2327, loss 0.0911979, acc 0.96875\n",
      "2017-01-10T21:53:22.724646: step 2328, loss 0.0927124, acc 0.96875\n",
      "2017-01-10T21:53:24.785382: step 2329, loss 0.0533685, acc 0.96875\n",
      "2017-01-10T21:53:26.826418: step 2330, loss 0.0507262, acc 0.992188\n",
      "2017-01-10T21:53:28.905800: step 2331, loss 0.0572205, acc 0.976562\n",
      "2017-01-10T21:53:30.975366: step 2332, loss 0.0883428, acc 0.96875\n",
      "2017-01-10T21:53:33.025015: step 2333, loss 0.0980482, acc 0.96875\n",
      "2017-01-10T21:53:35.076477: step 2334, loss 0.0462358, acc 0.984375\n",
      "2017-01-10T21:53:37.150292: step 2335, loss 0.0702954, acc 0.976562\n",
      "2017-01-10T21:53:39.227178: step 2336, loss 0.0375252, acc 0.984375\n",
      "2017-01-10T21:53:41.314222: step 2337, loss 0.138854, acc 0.953125\n",
      "2017-01-10T21:53:43.579690: step 2338, loss 0.096144, acc 0.960938\n",
      "2017-01-10T21:53:45.658747: step 2339, loss 0.0709599, acc 0.984375\n",
      "2017-01-10T21:53:47.770303: step 2340, loss 0.146795, acc 0.945312\n",
      "2017-01-10T21:53:49.878240: step 2341, loss 0.124732, acc 0.953125\n",
      "2017-01-10T21:53:51.991688: step 2342, loss 0.0786569, acc 0.96875\n",
      "2017-01-10T21:53:54.062050: step 2343, loss 0.10958, acc 0.945312\n",
      "2017-01-10T21:53:56.134360: step 2344, loss 0.0602011, acc 0.984375\n",
      "2017-01-10T21:53:58.208027: step 2345, loss 0.0697073, acc 0.976562\n",
      "2017-01-10T21:54:00.267870: step 2346, loss 0.111388, acc 0.953125\n",
      "2017-01-10T21:54:02.370683: step 2347, loss 0.104139, acc 0.96875\n",
      "2017-01-10T21:54:04.428610: step 2348, loss 0.129235, acc 0.960938\n",
      "2017-01-10T21:54:06.503757: step 2349, loss 0.104802, acc 0.976562\n",
      "2017-01-10T21:54:08.582257: step 2350, loss 0.170197, acc 0.9375\n",
      "2017-01-10T21:54:10.624767: step 2351, loss 0.0747584, acc 0.984375\n",
      "2017-01-10T21:54:12.684403: step 2352, loss 0.174605, acc 0.960938\n",
      "2017-01-10T21:54:14.786570: step 2353, loss 0.0694359, acc 0.96875\n",
      "2017-01-10T21:54:16.940791: step 2354, loss 0.0945089, acc 0.976562\n",
      "2017-01-10T21:54:18.991835: step 2355, loss 0.0942911, acc 0.96875\n",
      "2017-01-10T21:54:21.101327: step 2356, loss 0.0433641, acc 0.984375\n",
      "2017-01-10T21:54:23.147187: step 2357, loss 0.0611356, acc 0.96875\n",
      "2017-01-10T21:54:25.198527: step 2358, loss 0.0597241, acc 0.984375\n",
      "2017-01-10T21:54:27.267435: step 2359, loss 0.098603, acc 0.984375\n",
      "2017-01-10T21:54:29.325626: step 2360, loss 0.130547, acc 0.953125\n",
      "2017-01-10T21:54:31.413590: step 2361, loss 0.0878462, acc 0.976562\n",
      "2017-01-10T21:54:33.456724: step 2362, loss 0.0390733, acc 0.984375\n",
      "2017-01-10T21:54:35.575731: step 2363, loss 0.120715, acc 0.96875\n",
      "2017-01-10T21:54:37.623812: step 2364, loss 0.173915, acc 0.945312\n",
      "2017-01-10T21:54:39.702930: step 2365, loss 0.145952, acc 0.960938\n",
      "2017-01-10T21:54:41.761682: step 2366, loss 0.120721, acc 0.976562\n",
      "2017-01-10T21:54:43.837486: step 2367, loss 0.0695416, acc 0.984375\n",
      "2017-01-10T21:54:45.895722: step 2368, loss 0.193639, acc 0.9375\n",
      "2017-01-10T21:54:48.272028: step 2369, loss 0.0870785, acc 0.976562\n",
      "2017-01-10T21:54:50.358872: step 2370, loss 0.0856721, acc 0.96875\n",
      "2017-01-10T21:54:52.471238: step 2371, loss 0.0328272, acc 0.992188\n",
      "2017-01-10T21:54:54.545400: step 2372, loss 0.0618207, acc 0.984375\n",
      "2017-01-10T21:54:56.620931: step 2373, loss 0.102264, acc 0.976562\n",
      "2017-01-10T21:54:58.713663: step 2374, loss 0.202479, acc 0.945312\n",
      "2017-01-10T21:55:00.763478: step 2375, loss 0.137745, acc 0.953125\n",
      "2017-01-10T21:55:02.829683: step 2376, loss 0.161969, acc 0.921875\n",
      "2017-01-10T21:55:04.915934: step 2377, loss 0.218271, acc 0.945312\n",
      "2017-01-10T21:55:06.990281: step 2378, loss 0.0699846, acc 0.960938\n",
      "2017-01-10T21:55:09.059827: step 2379, loss 0.10052, acc 0.960938\n",
      "2017-01-10T21:55:11.110955: step 2380, loss 0.176241, acc 0.953125\n",
      "2017-01-10T21:55:13.196325: step 2381, loss 0.0589849, acc 0.984375\n",
      "2017-01-10T21:55:15.274445: step 2382, loss 0.106634, acc 0.96875\n",
      "2017-01-10T21:55:17.371474: step 2383, loss 0.142645, acc 0.953125\n",
      "2017-01-10T21:55:19.454093: step 2384, loss 0.202934, acc 0.9375\n",
      "2017-01-10T21:55:21.664585: step 2385, loss 0.0913559, acc 0.976562\n",
      "2017-01-10T21:55:23.708914: step 2386, loss 0.129653, acc 0.953125\n",
      "2017-01-10T21:55:25.817299: step 2387, loss 0.0648042, acc 0.992188\n",
      "2017-01-10T21:55:27.880448: step 2388, loss 0.0864157, acc 0.96875\n",
      "2017-01-10T21:55:29.891495: step 2389, loss 0.108228, acc 0.96875\n",
      "2017-01-10T21:55:31.968517: step 2390, loss 0.244378, acc 0.945312\n",
      "2017-01-10T21:55:34.068689: step 2391, loss 0.101551, acc 0.945312\n",
      "2017-01-10T21:55:36.164438: step 2392, loss 0.0767651, acc 0.984375\n",
      "2017-01-10T21:55:38.207451: step 2393, loss 0.0838746, acc 0.984375\n",
      "2017-01-10T21:55:40.255783: step 2394, loss 0.108097, acc 0.96875\n",
      "2017-01-10T21:55:42.349866: step 2395, loss 0.0646067, acc 0.976562\n",
      "2017-01-10T21:55:44.414039: step 2396, loss 0.17892, acc 0.945312\n",
      "2017-01-10T21:55:46.758695: step 2397, loss 0.0625991, acc 0.976562\n",
      "2017-01-10T21:55:49.195954: step 2398, loss 0.0694212, acc 0.976562\n",
      "2017-01-10T21:55:51.338921: step 2399, loss 0.153603, acc 0.945312\n",
      "2017-01-10T21:55:53.453178: step 2400, loss 0.118722, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T21:56:20.168207: step 2400, loss 0.10025, acc 0.96976\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2400\n",
      "\n",
      "2017-01-10T21:56:25.140712: step 2401, loss 0.039297, acc 0.992188\n",
      "2017-01-10T21:56:27.229636: step 2402, loss 0.0777663, acc 0.96875\n",
      "2017-01-10T21:56:29.355246: step 2403, loss 0.119667, acc 0.960938\n",
      "2017-01-10T21:56:31.409490: step 2404, loss 0.183898, acc 0.953125\n",
      "2017-01-10T21:56:33.467814: step 2405, loss 0.0653087, acc 0.976562\n",
      "2017-01-10T21:56:35.606123: step 2406, loss 0.0646703, acc 0.984375\n",
      "2017-01-10T21:56:37.678981: step 2407, loss 0.0929925, acc 0.976562\n",
      "2017-01-10T21:56:39.758804: step 2408, loss 0.0625685, acc 0.984375\n",
      "2017-01-10T21:56:41.837866: step 2409, loss 0.100878, acc 0.960938\n",
      "2017-01-10T21:56:43.914493: step 2410, loss 0.0621518, acc 0.984375\n",
      "2017-01-10T21:56:45.983752: step 2411, loss 0.143676, acc 0.953125\n",
      "2017-01-10T21:56:48.213924: step 2412, loss 0.0425096, acc 0.984375\n",
      "2017-01-10T21:56:50.278551: step 2413, loss 0.117534, acc 0.953125\n",
      "2017-01-10T21:56:52.395435: step 2414, loss 0.0395573, acc 0.992188\n",
      "2017-01-10T21:56:54.457220: step 2415, loss 0.10182, acc 0.96875\n",
      "2017-01-10T21:56:56.621565: step 2416, loss 0.186849, acc 0.945312\n",
      "2017-01-10T21:56:58.755396: step 2417, loss 0.114573, acc 0.976562\n",
      "2017-01-10T21:57:00.832311: step 2418, loss 0.168295, acc 0.960938\n",
      "2017-01-10T21:57:03.234138: step 2419, loss 0.0946738, acc 0.984375\n",
      "2017-01-10T21:57:06.018874: step 2420, loss 0.133949, acc 0.96875\n",
      "2017-01-10T21:57:08.111329: step 2421, loss 0.0143598, acc 1\n",
      "2017-01-10T21:57:10.186644: step 2422, loss 0.052183, acc 0.992188\n",
      "2017-01-10T21:57:12.243009: step 2423, loss 0.122156, acc 0.945312\n",
      "2017-01-10T21:57:14.308455: step 2424, loss 0.0867151, acc 0.96875\n",
      "2017-01-10T21:57:16.392063: step 2425, loss 0.102413, acc 0.984375\n",
      "2017-01-10T21:57:18.451337: step 2426, loss 0.0928254, acc 0.96875\n",
      "2017-01-10T21:57:20.545336: step 2427, loss 0.0758635, acc 0.96875\n",
      "2017-01-10T21:57:22.705837: step 2428, loss 0.0878102, acc 0.96875\n",
      "2017-01-10T21:57:24.740515: step 2429, loss 0.0594488, acc 0.984375\n",
      "2017-01-10T21:57:26.801845: step 2430, loss 0.121723, acc 0.960938\n",
      "2017-01-10T21:57:29.010162: step 2431, loss 0.061056, acc 0.984375\n",
      "2017-01-10T21:57:31.109647: step 2432, loss 0.145825, acc 0.953125\n",
      "2017-01-10T21:57:33.198970: step 2433, loss 0.0872107, acc 0.96875\n",
      "2017-01-10T21:57:35.279617: step 2434, loss 0.114649, acc 0.960938\n",
      "2017-01-10T21:57:37.381477: step 2435, loss 0.0959848, acc 0.96875\n",
      "2017-01-10T21:57:39.478485: step 2436, loss 0.0477238, acc 0.984375\n",
      "2017-01-10T21:57:41.554724: step 2437, loss 0.250287, acc 0.945312\n",
      "2017-01-10T21:57:43.600331: step 2438, loss 0.145836, acc 0.945312\n",
      "2017-01-10T21:57:45.660908: step 2439, loss 0.0845501, acc 0.96875\n",
      "2017-01-10T21:57:47.731537: step 2440, loss 0.103096, acc 0.960938\n",
      "2017-01-10T21:57:49.834743: step 2441, loss 0.223664, acc 0.9375\n",
      "2017-01-10T21:57:51.941843: step 2442, loss 0.117512, acc 0.960938\n",
      "2017-01-10T21:57:54.013456: step 2443, loss 0.075909, acc 0.976562\n",
      "2017-01-10T21:57:56.080467: step 2444, loss 0.0931157, acc 0.96875\n",
      "2017-01-10T21:57:58.141259: step 2445, loss 0.0495963, acc 0.984375\n",
      "2017-01-10T21:58:00.234282: step 2446, loss 0.0648022, acc 0.984375\n",
      "2017-01-10T21:58:02.428574: step 2447, loss 0.0754549, acc 0.984375\n",
      "2017-01-10T21:58:04.503588: step 2448, loss 0.111579, acc 0.96875\n",
      "2017-01-10T21:58:06.594865: step 2449, loss 0.204285, acc 0.9375\n",
      "2017-01-10T21:58:08.670209: step 2450, loss 0.0584045, acc 0.984375\n",
      "2017-01-10T21:58:10.765352: step 2451, loss 0.0781043, acc 0.976562\n",
      "2017-01-10T21:58:12.884507: step 2452, loss 0.145745, acc 0.945312\n",
      "2017-01-10T21:58:14.948671: step 2453, loss 0.215196, acc 0.921875\n",
      "2017-01-10T21:58:16.987756: step 2454, loss 0.187876, acc 0.929688\n",
      "2017-01-10T21:58:19.063495: step 2455, loss 0.100367, acc 0.976562\n",
      "2017-01-10T21:58:21.146885: step 2456, loss 0.12634, acc 0.976562\n",
      "2017-01-10T21:58:23.203057: step 2457, loss 0.0503455, acc 0.992188\n",
      "2017-01-10T21:58:25.239168: step 2458, loss 0.103834, acc 0.960938\n",
      "2017-01-10T21:58:27.324925: step 2459, loss 0.0841522, acc 0.96875\n",
      "2017-01-10T21:58:29.409734: step 2460, loss 0.0741589, acc 0.96875\n",
      "2017-01-10T21:58:31.437918: step 2461, loss 0.106974, acc 0.96875\n",
      "2017-01-10T21:58:33.710010: step 2462, loss 0.155701, acc 0.960938\n",
      "2017-01-10T21:58:35.841823: step 2463, loss 0.103097, acc 0.976562\n",
      "2017-01-10T21:58:37.938566: step 2464, loss 0.107338, acc 0.984375\n",
      "2017-01-10T21:58:39.976720: step 2465, loss 0.0704479, acc 0.976562\n",
      "2017-01-10T21:58:42.038772: step 2466, loss 0.190335, acc 0.945312\n",
      "2017-01-10T21:58:44.133430: step 2467, loss 0.171879, acc 0.960938\n",
      "2017-01-10T21:58:46.203473: step 2468, loss 0.229194, acc 0.96875\n",
      "2017-01-10T21:58:48.307255: step 2469, loss 0.17424, acc 0.945312\n",
      "2017-01-10T21:58:50.426654: step 2470, loss 0.104737, acc 0.953125\n",
      "2017-01-10T21:58:52.563078: step 2471, loss 0.120075, acc 0.96875\n",
      "2017-01-10T21:58:54.603656: step 2472, loss 0.186822, acc 0.945312\n",
      "2017-01-10T21:58:56.686705: step 2473, loss 0.0702818, acc 0.984375\n",
      "2017-01-10T21:58:58.715685: step 2474, loss 0.168974, acc 0.945312\n",
      "2017-01-10T21:59:00.807771: step 2475, loss 0.129339, acc 0.960938\n",
      "2017-01-10T21:59:02.876335: step 2476, loss 0.0476314, acc 0.984375\n",
      "2017-01-10T21:59:05.096514: step 2477, loss 0.282326, acc 0.929688\n",
      "2017-01-10T21:59:07.184495: step 2478, loss 0.0686769, acc 0.976562\n",
      "2017-01-10T21:59:09.259828: step 2479, loss 0.0923413, acc 0.976562\n",
      "2017-01-10T21:59:11.309168: step 2480, loss 0.152431, acc 0.945312\n",
      "2017-01-10T21:59:13.405899: step 2481, loss 0.0736812, acc 0.976562\n",
      "2017-01-10T21:59:15.473523: step 2482, loss 0.121471, acc 0.960938\n",
      "2017-01-10T21:59:17.529019: step 2483, loss 0.132647, acc 0.960938\n",
      "2017-01-10T21:59:19.540649: step 2484, loss 0.0681787, acc 0.976562\n",
      "2017-01-10T21:59:21.624248: step 2485, loss 0.0636056, acc 0.976562\n",
      "2017-01-10T21:59:23.700444: step 2486, loss 0.183099, acc 0.921875\n",
      "2017-01-10T21:59:25.774495: step 2487, loss 0.141751, acc 0.953125\n",
      "2017-01-10T21:59:27.781715: step 2488, loss 0.146268, acc 0.953125\n",
      "2017-01-10T21:59:29.844682: step 2489, loss 0.111411, acc 0.96875\n",
      "2017-01-10T21:59:31.927724: step 2490, loss 0.161862, acc 0.945312\n",
      "2017-01-10T21:59:34.002474: step 2491, loss 0.0627923, acc 0.96875\n",
      "2017-01-10T21:59:36.103908: step 2492, loss 0.0878802, acc 0.953125\n",
      "2017-01-10T21:59:38.313979: step 2493, loss 0.105903, acc 0.945312\n",
      "2017-01-10T21:59:40.400687: step 2494, loss 0.0965779, acc 0.976562\n",
      "2017-01-10T21:59:42.485386: step 2495, loss 0.0344581, acc 0.992188\n",
      "2017-01-10T21:59:44.568397: step 2496, loss 0.118036, acc 0.960938\n",
      "2017-01-10T21:59:46.633816: step 2497, loss 0.0683795, acc 0.96875\n",
      "2017-01-10T21:59:48.714863: step 2498, loss 0.117563, acc 0.953125\n",
      "2017-01-10T21:59:50.846019: step 2499, loss 0.127035, acc 0.953125\n",
      "2017-01-10T21:59:52.899055: step 2500, loss 0.157242, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:00:18.449519: step 2500, loss 0.0953376, acc 0.9718\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2500\n",
      "\n",
      "2017-01-10T22:00:23.450312: step 2501, loss 0.105537, acc 0.96875\n",
      "2017-01-10T22:00:25.539420: step 2502, loss 0.104906, acc 0.960938\n",
      "2017-01-10T22:00:27.641974: step 2503, loss 0.0892942, acc 0.976562\n",
      "2017-01-10T22:00:29.751727: step 2504, loss 0.0575529, acc 0.96875\n",
      "2017-01-10T22:00:31.828966: step 2505, loss 0.111732, acc 0.96875\n",
      "2017-01-10T22:00:33.923322: step 2506, loss 0.0918556, acc 0.96875\n",
      "2017-01-10T22:00:36.001164: step 2507, loss 0.0814497, acc 0.984375\n",
      "2017-01-10T22:00:38.073252: step 2508, loss 0.114889, acc 0.96875\n",
      "2017-01-10T22:00:40.163419: step 2509, loss 0.0836079, acc 0.976562\n",
      "2017-01-10T22:00:42.211025: step 2510, loss 0.0668261, acc 0.976562\n",
      "2017-01-10T22:00:44.304064: step 2511, loss 0.0776681, acc 0.976562\n",
      "2017-01-10T22:00:46.530577: step 2512, loss 0.152947, acc 0.960938\n",
      "2017-01-10T22:00:48.597771: step 2513, loss 0.111777, acc 0.960938\n",
      "2017-01-10T22:00:50.734784: step 2514, loss 0.0951471, acc 0.96875\n",
      "2017-01-10T22:00:52.790381: step 2515, loss 0.0327674, acc 0.992188\n",
      "2017-01-10T22:00:54.848390: step 2516, loss 0.0983233, acc 0.984375\n",
      "2017-01-10T22:00:56.912858: step 2517, loss 0.0823088, acc 0.976562\n",
      "2017-01-10T22:00:58.970322: step 2518, loss 0.0501584, acc 0.992188\n",
      "2017-01-10T22:01:01.086433: step 2519, loss 0.0764704, acc 0.976562\n",
      "2017-01-10T22:01:03.180635: step 2520, loss 0.163239, acc 0.953125\n",
      "2017-01-10T22:01:05.271039: step 2521, loss 0.0992917, acc 0.960938\n",
      "2017-01-10T22:01:07.334788: step 2522, loss 0.175005, acc 0.953125\n",
      "2017-01-10T22:01:09.424496: step 2523, loss 0.137535, acc 0.96875\n",
      "2017-01-10T22:01:11.472620: step 2524, loss 0.0736523, acc 0.960938\n",
      "2017-01-10T22:01:13.573611: step 2525, loss 0.0912481, acc 0.96875\n",
      "2017-01-10T22:01:15.681790: step 2526, loss 0.0593008, acc 0.976562\n",
      "2017-01-10T22:01:17.904877: step 2527, loss 0.071918, acc 0.976562\n",
      "2017-01-10T22:01:20.029178: step 2528, loss 0.150077, acc 0.945312\n",
      "2017-01-10T22:01:22.157382: step 2529, loss 0.0551082, acc 0.984375\n",
      "2017-01-10T22:01:24.228029: step 2530, loss 0.184421, acc 0.960938\n",
      "2017-01-10T22:01:26.267664: step 2531, loss 0.0845396, acc 0.984375\n",
      "2017-01-10T22:01:28.538656: step 2532, loss 0.0452065, acc 0.984375\n",
      "2017-01-10T22:01:30.809290: step 2533, loss 0.0837366, acc 0.976562\n",
      "2017-01-10T22:01:32.895559: step 2534, loss 0.229439, acc 0.921875\n",
      "2017-01-10T22:01:34.976572: step 2535, loss 0.117354, acc 0.960938\n",
      "2017-01-10T22:01:37.044065: step 2536, loss 0.082809, acc 0.976562\n",
      "2017-01-10T22:01:39.129275: step 2537, loss 0.17847, acc 0.960938\n",
      "2017-01-10T22:01:41.213471: step 2538, loss 0.164192, acc 0.9375\n",
      "2017-01-10T22:01:43.286418: step 2539, loss 0.0864347, acc 0.96875\n",
      "2017-01-10T22:01:46.644120: step 2540, loss 0.12854, acc 0.953125\n",
      "2017-01-10T22:01:49.546452: step 2541, loss 0.0577618, acc 0.96875\n",
      "2017-01-10T22:01:51.728657: step 2542, loss 0.0546258, acc 0.976562\n",
      "2017-01-10T22:01:53.810909: step 2543, loss 0.0452741, acc 0.984375\n",
      "2017-01-10T22:01:55.943053: step 2544, loss 0.115667, acc 0.976562\n",
      "2017-01-10T22:01:58.057296: step 2545, loss 0.10015, acc 0.96875\n",
      "2017-01-10T22:02:00.144508: step 2546, loss 0.0949276, acc 0.96875\n",
      "2017-01-10T22:02:02.250131: step 2547, loss 0.106498, acc 0.960938\n",
      "2017-01-10T22:02:04.314033: step 2548, loss 0.0785093, acc 0.96875\n",
      "2017-01-10T22:02:06.425613: step 2549, loss 0.118707, acc 0.96875\n",
      "2017-01-10T22:02:08.515569: step 2550, loss 0.0596882, acc 0.976562\n",
      "2017-01-10T22:02:10.612384: step 2551, loss 0.144278, acc 0.960938\n",
      "2017-01-10T22:02:12.678297: step 2552, loss 0.167981, acc 0.929688\n",
      "2017-01-10T22:02:14.759425: step 2553, loss 0.123434, acc 0.960938\n",
      "2017-01-10T22:02:16.780509: step 2554, loss 0.0864235, acc 0.976562\n",
      "2017-01-10T22:02:18.871690: step 2555, loss 0.0917352, acc 0.960938\n",
      "2017-01-10T22:02:20.983387: step 2556, loss 0.12856, acc 0.976562\n",
      "2017-01-10T22:02:23.146767: step 2557, loss 0.0331209, acc 0.992188\n",
      "2017-01-10T22:02:25.228681: step 2558, loss 0.127799, acc 0.96875\n",
      "2017-01-10T22:02:27.297802: step 2559, loss 0.0518677, acc 0.992188\n",
      "2017-01-10T22:02:29.380311: step 2560, loss 0.0758729, acc 0.96875\n",
      "2017-01-10T22:02:31.471773: step 2561, loss 0.285024, acc 0.945312\n",
      "2017-01-10T22:02:33.566100: step 2562, loss 0.101047, acc 0.96875\n",
      "2017-01-10T22:02:35.661648: step 2563, loss 0.111892, acc 0.976562\n",
      "2017-01-10T22:02:37.750313: step 2564, loss 0.0271876, acc 0.992188\n",
      "2017-01-10T22:02:39.791575: step 2565, loss 0.0913678, acc 0.96875\n",
      "2017-01-10T22:02:41.904984: step 2566, loss 0.149502, acc 0.960938\n",
      "2017-01-10T22:02:43.965042: step 2567, loss 0.115325, acc 0.960938\n",
      "2017-01-10T22:02:46.049595: step 2568, loss 0.0375441, acc 0.984375\n",
      "2017-01-10T22:02:48.106020: step 2569, loss 0.0682533, acc 0.984375\n",
      "2017-01-10T22:02:50.209969: step 2570, loss 0.0801793, acc 0.976562\n",
      "2017-01-10T22:02:52.290805: step 2571, loss 0.250937, acc 0.9375\n",
      "2017-01-10T22:02:54.767737: step 2572, loss 0.0954188, acc 0.96875\n",
      "2017-01-10T22:02:56.811198: step 2573, loss 0.183982, acc 0.945312\n",
      "2017-01-10T22:02:58.921715: step 2574, loss 0.0463559, acc 0.984375\n",
      "2017-01-10T22:03:01.015493: step 2575, loss 0.0277478, acc 0.992188\n",
      "2017-01-10T22:03:03.098572: step 2576, loss 0.0649208, acc 0.976562\n",
      "2017-01-10T22:03:05.207834: step 2577, loss 0.0382908, acc 0.984375\n",
      "2017-01-10T22:03:07.317250: step 2578, loss 0.202856, acc 0.945312\n",
      "2017-01-10T22:03:09.828979: step 2579, loss 0.0407701, acc 0.984375\n",
      "2017-01-10T22:03:12.404133: step 2580, loss 0.0485906, acc 0.992188\n",
      "2017-01-10T22:03:14.479557: step 2581, loss 0.0767369, acc 0.976562\n",
      "2017-01-10T22:03:16.543610: step 2582, loss 0.177053, acc 0.953125\n",
      "2017-01-10T22:03:18.634126: step 2583, loss 0.151967, acc 0.960938\n",
      "2017-01-10T22:03:20.761707: step 2584, loss 0.0304337, acc 0.984375\n",
      "2017-01-10T22:03:22.857227: step 2585, loss 0.148673, acc 0.945312\n",
      "2017-01-10T22:03:24.924448: step 2586, loss 0.111862, acc 0.96875\n",
      "2017-01-10T22:03:27.176578: step 2587, loss 0.124908, acc 0.953125\n",
      "2017-01-10T22:03:29.253695: step 2588, loss 0.0811409, acc 0.984375\n",
      "2017-01-10T22:03:31.313215: step 2589, loss 0.108754, acc 0.960938\n",
      "2017-01-10T22:03:33.385585: step 2590, loss 0.0539743, acc 0.984375\n",
      "2017-01-10T22:03:35.502588: step 2591, loss 0.112655, acc 0.960938\n",
      "2017-01-10T22:03:37.573763: step 2592, loss 0.213036, acc 0.9375\n",
      "2017-01-10T22:03:39.652633: step 2593, loss 0.0785067, acc 0.976562\n",
      "2017-01-10T22:03:41.716147: step 2594, loss 0.161911, acc 0.9375\n",
      "2017-01-10T22:03:43.778651: step 2595, loss 0.0568487, acc 0.976562\n",
      "2017-01-10T22:03:45.859431: step 2596, loss 0.0475672, acc 0.992188\n",
      "2017-01-10T22:03:47.943962: step 2597, loss 0.110247, acc 0.953125\n",
      "2017-01-10T22:03:50.078939: step 2598, loss 0.0383073, acc 0.992188\n",
      "2017-01-10T22:03:52.244222: step 2599, loss 0.0660893, acc 0.984375\n",
      "2017-01-10T22:03:54.300144: step 2600, loss 0.0740193, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:04:27.987955: step 2600, loss 0.094385, acc 0.97196\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2600\n",
      "\n",
      "2017-01-10T22:04:33.981996: step 2601, loss 0.0727628, acc 0.976562\n",
      "2017-01-10T22:04:37.281882: step 2602, loss 0.144747, acc 0.953125\n",
      "2017-01-10T22:04:39.530406: step 2603, loss 0.0429697, acc 0.992188\n",
      "2017-01-10T22:04:41.784573: step 2604, loss 0.107418, acc 0.96875\n",
      "2017-01-10T22:04:44.077196: step 2605, loss 0.218294, acc 0.9375\n",
      "2017-01-10T22:04:46.306637: step 2606, loss 0.0890091, acc 0.96875\n",
      "2017-01-10T22:04:48.408725: step 2607, loss 0.141617, acc 0.953125\n",
      "2017-01-10T22:04:50.555496: step 2608, loss 0.109628, acc 0.976562\n",
      "2017-01-10T22:04:52.764394: step 2609, loss 0.106657, acc 0.96875\n",
      "2017-01-10T22:04:55.006499: step 2610, loss 0.0333655, acc 0.992188\n",
      "2017-01-10T22:04:57.265554: step 2611, loss 0.0645397, acc 0.984375\n",
      "2017-01-10T22:04:59.583669: step 2612, loss 0.126032, acc 0.953125\n",
      "2017-01-10T22:05:01.869527: step 2613, loss 0.0583977, acc 0.984375\n",
      "2017-01-10T22:05:04.275845: step 2614, loss 0.21849, acc 0.945312\n",
      "2017-01-10T22:05:06.711611: step 2615, loss 0.111333, acc 0.96875\n",
      "2017-01-10T22:05:09.059327: step 2616, loss 0.101757, acc 0.96875\n",
      "2017-01-10T22:05:11.368381: step 2617, loss 0.148146, acc 0.945312\n",
      "2017-01-10T22:05:13.801768: step 2618, loss 0.0228088, acc 0.984375\n",
      "2017-01-10T22:05:16.213527: step 2619, loss 0.0592174, acc 0.984375\n",
      "2017-01-10T22:05:18.540300: step 2620, loss 0.0530524, acc 0.992188\n",
      "2017-01-10T22:05:21.177435: step 2621, loss 0.0711257, acc 0.96875\n",
      "2017-01-10T22:05:23.879016: step 2622, loss 0.115518, acc 0.96875\n",
      "2017-01-10T22:05:26.171076: step 2623, loss 0.0934692, acc 0.960938\n",
      "2017-01-10T22:05:28.781353: step 2624, loss 0.0993474, acc 0.96875\n",
      "2017-01-10T22:05:30.917587: step 2625, loss 0.0888587, acc 0.976562\n",
      "2017-01-10T22:05:32.984691: step 2626, loss 0.0649451, acc 0.976562\n",
      "2017-01-10T22:05:35.079062: step 2627, loss 0.0776793, acc 0.96875\n",
      "2017-01-10T22:05:37.277058: step 2628, loss 0.0454503, acc 0.992188\n",
      "2017-01-10T22:05:39.380817: step 2629, loss 0.103832, acc 0.960938\n",
      "2017-01-10T22:05:41.431546: step 2630, loss 0.0483558, acc 0.976562\n",
      "2017-01-10T22:05:43.499497: step 2631, loss 0.158762, acc 0.953125\n",
      "2017-01-10T22:05:45.586326: step 2632, loss 0.0742982, acc 0.976562\n",
      "2017-01-10T22:05:47.707583: step 2633, loss 0.0339373, acc 0.984375\n",
      "2017-01-10T22:05:49.811571: step 2634, loss 0.0539954, acc 0.976562\n",
      "2017-01-10T22:05:51.912356: step 2635, loss 0.0927314, acc 0.976562\n",
      "2017-01-10T22:05:54.006439: step 2636, loss 0.0725393, acc 0.960938\n",
      "2017-01-10T22:05:56.075978: step 2637, loss 0.0106239, acc 1\n",
      "2017-01-10T22:05:58.158917: step 2638, loss 0.0797833, acc 0.96875\n",
      "2017-01-10T22:06:00.254181: step 2639, loss 0.153211, acc 0.945312\n",
      "2017-01-10T22:06:02.326514: step 2640, loss 0.164307, acc 0.945312\n",
      "2017-01-10T22:06:04.421771: step 2641, loss 0.024445, acc 0.992188\n",
      "2017-01-10T22:06:06.504723: step 2642, loss 0.0689146, acc 0.976562\n",
      "2017-01-10T22:06:08.593153: step 2643, loss 0.105565, acc 0.960938\n",
      "2017-01-10T22:06:10.790808: step 2644, loss 0.0802618, acc 0.976562\n",
      "2017-01-10T22:06:12.916905: step 2645, loss 0.103455, acc 0.953125\n",
      "2017-01-10T22:06:15.095404: step 2646, loss 0.117974, acc 0.96875\n",
      "2017-01-10T22:06:17.231650: step 2647, loss 0.148413, acc 0.945312\n",
      "2017-01-10T22:06:19.487058: step 2648, loss 0.107544, acc 0.960938\n",
      "2017-01-10T22:06:21.627569: step 2649, loss 0.109031, acc 0.96875\n",
      "2017-01-10T22:06:23.818056: step 2650, loss 0.222704, acc 0.945312\n",
      "2017-01-10T22:06:25.935316: step 2651, loss 0.0779082, acc 0.976562\n",
      "2017-01-10T22:06:28.036328: step 2652, loss 0.054044, acc 0.96875\n",
      "2017-01-10T22:06:30.355180: step 2653, loss 0.0453757, acc 0.992188\n",
      "2017-01-10T22:06:32.463668: step 2654, loss 0.0342593, acc 0.992188\n",
      "2017-01-10T22:06:34.510680: step 2655, loss 0.100129, acc 0.976562\n",
      "2017-01-10T22:06:36.595507: step 2656, loss 0.158528, acc 0.953125\n",
      "2017-01-10T22:06:38.648591: step 2657, loss 0.207222, acc 0.953125\n",
      "2017-01-10T22:06:40.840632: step 2658, loss 0.0658929, acc 0.976562\n",
      "2017-01-10T22:06:42.950590: step 2659, loss 0.0703556, acc 0.976562\n",
      "2017-01-10T22:06:45.808727: step 2660, loss 0.120002, acc 0.96875\n",
      "2017-01-10T22:06:48.021423: step 2661, loss 0.0769596, acc 0.984375\n",
      "2017-01-10T22:06:50.112392: step 2662, loss 0.0735256, acc 0.984375\n",
      "2017-01-10T22:06:52.211480: step 2663, loss 0.118886, acc 0.960938\n",
      "2017-01-10T22:06:54.296022: step 2664, loss 0.0226006, acc 0.992188\n",
      "2017-01-10T22:06:56.393661: step 2665, loss 0.144573, acc 0.960938\n",
      "2017-01-10T22:06:58.520463: step 2666, loss 0.073284, acc 0.976562\n",
      "2017-01-10T22:07:00.624824: step 2667, loss 0.129739, acc 0.953125\n",
      "2017-01-10T22:07:02.763272: step 2668, loss 0.0973766, acc 0.976562\n",
      "2017-01-10T22:07:04.921362: step 2669, loss 0.0815286, acc 0.976562\n",
      "2017-01-10T22:07:07.056554: step 2670, loss 0.252462, acc 0.945312\n",
      "2017-01-10T22:07:09.325515: step 2671, loss 0.101581, acc 0.976562\n",
      "2017-01-10T22:07:11.466738: step 2672, loss 0.0908374, acc 0.96875\n",
      "2017-01-10T22:07:13.807096: step 2673, loss 0.262027, acc 0.953125\n",
      "2017-01-10T22:07:15.891223: step 2674, loss 0.122859, acc 0.96875\n",
      "2017-01-10T22:07:17.986548: step 2675, loss 0.0871529, acc 0.96875\n",
      "2017-01-10T22:07:20.165139: step 2676, loss 0.181016, acc 0.960938\n",
      "2017-01-10T22:07:22.294225: step 2677, loss 0.0163355, acc 0.992188\n",
      "2017-01-10T22:07:24.399920: step 2678, loss 0.0821814, acc 0.984375\n",
      "2017-01-10T22:07:26.498103: step 2679, loss 0.102169, acc 0.96875\n",
      "2017-01-10T22:07:28.606390: step 2680, loss 0.191461, acc 0.945312\n",
      "2017-01-10T22:07:30.674516: step 2681, loss 0.0413238, acc 0.984375\n",
      "2017-01-10T22:07:32.745006: step 2682, loss 0.147444, acc 0.960938\n",
      "2017-01-10T22:07:35.000652: step 2683, loss 0.113907, acc 0.953125\n",
      "2017-01-10T22:07:37.090173: step 2684, loss 0.125461, acc 0.960938\n",
      "2017-01-10T22:07:39.172319: step 2685, loss 0.150029, acc 0.960938\n",
      "2017-01-10T22:07:41.280172: step 2686, loss 0.151761, acc 0.953125\n",
      "2017-01-10T22:07:43.373591: step 2687, loss 0.181733, acc 0.929688\n",
      "2017-01-10T22:07:45.615429: step 2688, loss 0.0640473, acc 0.976562\n",
      "2017-01-10T22:07:47.724943: step 2689, loss 0.0889014, acc 0.96875\n",
      "2017-01-10T22:07:49.894155: step 2690, loss 0.12483, acc 0.945312\n",
      "2017-01-10T22:07:52.317412: step 2691, loss 0.0856809, acc 0.96875\n",
      "2017-01-10T22:07:54.832317: step 2692, loss 0.099417, acc 0.96875\n",
      "2017-01-10T22:07:57.143887: step 2693, loss 0.111886, acc 0.960938\n",
      "2017-01-10T22:07:59.323573: step 2694, loss 0.108606, acc 0.976562\n",
      "2017-01-10T22:08:01.422023: step 2695, loss 0.0935676, acc 0.953125\n",
      "2017-01-10T22:08:03.516859: step 2696, loss 0.0710119, acc 0.976562\n",
      "2017-01-10T22:08:05.619175: step 2697, loss 0.0897712, acc 0.984375\n",
      "2017-01-10T22:08:07.726931: step 2698, loss 0.0644689, acc 0.984375\n",
      "2017-01-10T22:08:09.859313: step 2699, loss 0.0456423, acc 0.976562\n",
      "2017-01-10T22:08:12.018649: step 2700, loss 0.103246, acc 0.945312\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:08:47.315637: step 2700, loss 0.092438, acc 0.9724\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2700\n",
      "\n",
      "2017-01-10T22:08:53.755817: step 2701, loss 0.130039, acc 0.953125\n",
      "2017-01-10T22:08:57.213817: step 2702, loss 0.0472764, acc 0.992188\n",
      "2017-01-10T22:08:59.408880: step 2703, loss 0.0821445, acc 0.960938\n",
      "2017-01-10T22:09:01.976102: step 2704, loss 0.105593, acc 0.976562\n",
      "2017-01-10T22:09:04.167778: step 2705, loss 0.0701791, acc 0.976562\n",
      "2017-01-10T22:09:06.294416: step 2706, loss 0.0864639, acc 0.96875\n",
      "2017-01-10T22:09:09.290691: step 2707, loss 0.149943, acc 0.953125\n",
      "2017-01-10T22:09:11.509954: step 2708, loss 0.12967, acc 0.960938\n",
      "2017-01-10T22:09:13.588427: step 2709, loss 0.107758, acc 0.976562\n",
      "2017-01-10T22:09:16.483398: step 2710, loss 0.143182, acc 0.953125\n",
      "2017-01-10T22:09:18.776561: step 2711, loss 0.123239, acc 0.960938\n",
      "2017-01-10T22:09:21.070410: step 2712, loss 0.0655661, acc 0.96875\n",
      "2017-01-10T22:09:23.314747: step 2713, loss 0.162372, acc 0.960938\n",
      "2017-01-10T22:09:25.794299: step 2714, loss 0.101028, acc 0.96875\n",
      "2017-01-10T22:09:27.984034: step 2715, loss 0.0716799, acc 0.976562\n",
      "2017-01-10T22:09:30.324380: step 2716, loss 0.121482, acc 0.96875\n",
      "2017-01-10T22:09:33.123100: step 2717, loss 0.095364, acc 0.960938\n",
      "2017-01-10T22:09:35.760150: step 2718, loss 0.0206838, acc 0.984375\n",
      "2017-01-10T22:09:38.135279: step 2719, loss 0.0942271, acc 0.992188\n",
      "2017-01-10T22:09:40.513864: step 2720, loss 0.13164, acc 0.953125\n",
      "2017-01-10T22:09:42.927470: step 2721, loss 0.0524654, acc 0.984375\n",
      "2017-01-10T22:09:45.900449: step 2722, loss 0.0745329, acc 0.984375\n",
      "2017-01-10T22:09:48.874024: step 2723, loss 0.168506, acc 0.953125\n",
      "2017-01-10T22:09:51.379385: step 2724, loss 0.173049, acc 0.953125\n",
      "2017-01-10T22:09:54.902578: step 2725, loss 0.1613, acc 0.953125\n",
      "2017-01-10T22:09:58.251443: step 2726, loss 0.0445258, acc 0.992188\n",
      "2017-01-10T22:10:01.368588: step 2727, loss 0.0510982, acc 0.984375\n",
      "2017-01-10T22:10:04.143116: step 2728, loss 0.0999692, acc 0.960938\n",
      "2017-01-10T22:10:06.646078: step 2729, loss 0.0569612, acc 0.984375\n",
      "2017-01-10T22:10:08.948542: step 2730, loss 0.0403058, acc 0.984375\n",
      "2017-01-10T22:10:11.523492: step 2731, loss 0.103319, acc 0.96875\n",
      "2017-01-10T22:10:13.751172: step 2732, loss 0.0843738, acc 0.96875\n",
      "2017-01-10T22:10:16.044853: step 2733, loss 0.105532, acc 0.953125\n",
      "2017-01-10T22:10:18.124190: step 2734, loss 0.047288, acc 0.984375\n",
      "2017-01-10T22:10:20.234313: step 2735, loss 0.131685, acc 0.960938\n",
      "2017-01-10T22:10:23.523693: step 2736, loss 0.059893, acc 0.96875\n",
      "2017-01-10T22:10:25.663931: step 2737, loss 0.219776, acc 0.9375\n",
      "2017-01-10T22:10:27.904357: step 2738, loss 0.22265, acc 0.929688\n",
      "2017-01-10T22:10:30.635790: step 2739, loss 0.0535822, acc 0.976562\n",
      "2017-01-10T22:10:33.207733: step 2740, loss 0.0758281, acc 0.976562\n",
      "2017-01-10T22:10:35.457963: step 2741, loss 0.138583, acc 0.953125\n",
      "2017-01-10T22:10:37.710876: step 2742, loss 0.105241, acc 0.960938\n",
      "2017-01-10T22:10:39.775383: step 2743, loss 0.158172, acc 0.929688\n",
      "2017-01-10T22:10:41.940644: step 2744, loss 0.0947134, acc 0.953125\n",
      "2017-01-10T22:10:44.581068: step 2745, loss 0.10227, acc 0.960938\n",
      "2017-01-10T22:10:46.692168: step 2746, loss 0.135005, acc 0.953125\n",
      "2017-01-10T22:10:48.731093: step 2747, loss 0.0434259, acc 0.984375\n",
      "2017-01-10T22:10:50.828204: step 2748, loss 0.0402013, acc 0.984375\n",
      "2017-01-10T22:10:53.100070: step 2749, loss 0.0605041, acc 0.984375\n",
      "2017-01-10T22:10:55.174908: step 2750, loss 0.155444, acc 0.9375\n",
      "2017-01-10T22:10:57.612110: step 2751, loss 0.0677372, acc 0.976562\n",
      "2017-01-10T22:11:00.733893: step 2752, loss 0.157384, acc 0.96875\n",
      "2017-01-10T22:11:03.287103: step 2753, loss 0.0360272, acc 0.984375\n",
      "2017-01-10T22:11:06.196745: step 2754, loss 0.117539, acc 0.953125\n",
      "2017-01-10T22:11:08.838394: step 2755, loss 0.091418, acc 0.96875\n",
      "2017-01-10T22:11:10.922732: step 2756, loss 0.0935526, acc 0.976562\n",
      "2017-01-10T22:11:13.952133: step 2757, loss 0.0893634, acc 0.976562\n",
      "2017-01-10T22:11:17.556301: step 2758, loss 0.0907514, acc 0.960938\n",
      "2017-01-10T22:11:20.292161: step 2759, loss 0.119659, acc 0.976562\n",
      "2017-01-10T22:11:24.117360: step 2760, loss 0.140344, acc 0.976562\n",
      "2017-01-10T22:11:26.773355: step 2761, loss 0.0895876, acc 0.960938\n",
      "2017-01-10T22:11:29.263441: step 2762, loss 0.114734, acc 0.96875\n",
      "2017-01-10T22:11:32.213315: step 2763, loss 0.104273, acc 0.960938\n",
      "2017-01-10T22:11:34.805275: step 2764, loss 0.0505247, acc 0.976562\n",
      "2017-01-10T22:11:36.915914: step 2765, loss 0.0583203, acc 0.96875\n",
      "2017-01-10T22:11:39.526742: step 2766, loss 0.0648934, acc 0.976562\n",
      "2017-01-10T22:11:41.609520: step 2767, loss 0.14479, acc 0.953125\n",
      "2017-01-10T22:11:43.671183: step 2768, loss 0.090502, acc 0.96875\n",
      "2017-01-10T22:11:45.731366: step 2769, loss 0.165388, acc 0.929688\n",
      "2017-01-10T22:11:47.819466: step 2770, loss 0.0385977, acc 0.992188\n",
      "2017-01-10T22:11:50.186295: step 2771, loss 0.0206016, acc 0.992188\n",
      "2017-01-10T22:11:52.352776: step 2772, loss 0.0767544, acc 0.96875\n",
      "2017-01-10T22:11:54.401325: step 2773, loss 0.0402993, acc 0.992188\n",
      "2017-01-10T22:11:56.498520: step 2774, loss 0.13398, acc 0.945312\n",
      "2017-01-10T22:11:59.409158: step 2775, loss 0.130711, acc 0.9375\n",
      "2017-01-10T22:12:01.902983: step 2776, loss 0.0746607, acc 0.976562\n",
      "2017-01-10T22:12:04.684509: step 2777, loss 0.0849009, acc 0.96875\n",
      "2017-01-10T22:12:06.915183: step 2778, loss 0.112061, acc 0.96875\n",
      "2017-01-10T22:12:10.124177: step 2779, loss 0.114068, acc 0.953125\n",
      "2017-01-10T22:12:12.413582: step 2780, loss 0.19257, acc 0.929688\n",
      "2017-01-10T22:12:14.480504: step 2781, loss 0.0609979, acc 0.976562\n",
      "2017-01-10T22:12:16.552962: step 2782, loss 0.100915, acc 0.953125\n",
      "2017-01-10T22:12:18.602157: step 2783, loss 0.124626, acc 0.953125\n",
      "2017-01-10T22:12:20.755616: step 2784, loss 0.0727029, acc 0.976562\n",
      "2017-01-10T22:12:22.888971: step 2785, loss 0.15085, acc 0.953125\n",
      "2017-01-10T22:12:26.384294: step 2786, loss 0.0541545, acc 0.976562\n",
      "2017-01-10T22:12:28.871715: step 2787, loss 0.212282, acc 0.921875\n",
      "2017-01-10T22:12:31.783737: step 2788, loss 0.158262, acc 0.953125\n",
      "2017-01-10T22:12:33.883474: step 2789, loss 0.0242403, acc 0.992188\n",
      "2017-01-10T22:12:35.945796: step 2790, loss 0.172301, acc 0.953125\n",
      "2017-01-10T22:12:38.062567: step 2791, loss 0.144638, acc 0.960938\n",
      "2017-01-10T22:12:40.130613: step 2792, loss 0.0664135, acc 0.984375\n",
      "2017-01-10T22:12:43.190389: step 2793, loss 0.0972493, acc 0.960938\n",
      "2017-01-10T22:12:45.250769: step 2794, loss 0.122172, acc 0.96875\n",
      "2017-01-10T22:12:47.344198: step 2795, loss 0.0631434, acc 0.976562\n",
      "2017-01-10T22:12:49.431614: step 2796, loss 0.0565944, acc 0.960938\n",
      "2017-01-10T22:12:51.879468: step 2797, loss 0.10996, acc 0.96875\n",
      "2017-01-10T22:12:54.758151: step 2798, loss 0.129632, acc 0.953125\n",
      "2017-01-10T22:12:57.607790: step 2799, loss 0.0836665, acc 0.96875\n",
      "2017-01-10T22:13:01.503472: step 2800, loss 0.0593506, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:13:30.141071: step 2800, loss 0.0894272, acc 0.97384\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2800\n",
      "\n",
      "2017-01-10T22:13:35.186489: step 2801, loss 0.120686, acc 0.960938\n",
      "2017-01-10T22:13:37.232257: step 2802, loss 0.183877, acc 0.945312\n",
      "2017-01-10T22:13:39.306839: step 2803, loss 0.107599, acc 0.96875\n",
      "2017-01-10T22:13:41.418231: step 2804, loss 0.0736189, acc 0.96875\n",
      "2017-01-10T22:13:43.724462: step 2805, loss 0.152467, acc 0.953125\n",
      "2017-01-10T22:13:45.901649: step 2806, loss 0.0293474, acc 0.992188\n",
      "2017-01-10T22:13:47.992999: step 2807, loss 0.119349, acc 0.953125\n",
      "2017-01-10T22:13:50.350604: step 2808, loss 0.0169685, acc 1\n",
      "2017-01-10T22:13:52.481724: step 2809, loss 0.0745525, acc 0.976562\n",
      "2017-01-10T22:13:54.557362: step 2810, loss 0.144965, acc 0.960938\n",
      "2017-01-10T22:13:56.613596: step 2811, loss 0.0566911, acc 0.976562\n",
      "2017-01-10T22:13:58.688001: step 2812, loss 0.119704, acc 0.96875\n",
      "2017-01-10T22:14:00.770923: step 2813, loss 0.0908944, acc 0.96875\n",
      "2017-01-10T22:14:03.578059: step 2814, loss 0.0677702, acc 0.992188\n",
      "2017-01-10T22:14:06.080595: step 2815, loss 0.133483, acc 0.945312\n",
      "2017-01-10T22:14:08.183122: step 2816, loss 0.104374, acc 0.96875\n",
      "2017-01-10T22:14:10.242614: step 2817, loss 0.063483, acc 0.984375\n",
      "2017-01-10T22:14:12.314556: step 2818, loss 0.155485, acc 0.960938\n",
      "2017-01-10T22:14:14.407888: step 2819, loss 0.126777, acc 0.953125\n",
      "2017-01-10T22:14:16.663181: step 2820, loss 0.196943, acc 0.953125\n",
      "2017-01-10T22:14:18.734093: step 2821, loss 0.0604411, acc 0.992188\n",
      "2017-01-10T22:14:20.904587: step 2822, loss 0.124881, acc 0.96875\n",
      "2017-01-10T22:14:23.176146: step 2823, loss 0.0478564, acc 0.984375\n",
      "2017-01-10T22:14:25.239762: step 2824, loss 0.0535926, acc 0.984375\n",
      "2017-01-10T22:14:27.332236: step 2825, loss 0.0493328, acc 0.984375\n",
      "2017-01-10T22:14:29.409909: step 2826, loss 0.0803513, acc 0.984375\n",
      "2017-01-10T22:14:31.454482: step 2827, loss 0.158998, acc 0.953125\n",
      "2017-01-10T22:14:33.519183: step 2828, loss 0.122846, acc 0.96875\n",
      "2017-01-10T22:14:35.611393: step 2829, loss 0.268309, acc 0.921875\n",
      "2017-01-10T22:14:37.711954: step 2830, loss 0.0860359, acc 0.96875\n",
      "2017-01-10T22:14:39.784579: step 2831, loss 0.102446, acc 0.976562\n",
      "2017-01-10T22:14:41.880262: step 2832, loss 0.0610215, acc 0.984375\n",
      "2017-01-10T22:14:43.961108: step 2833, loss 0.0704959, acc 0.976562\n",
      "2017-01-10T22:14:46.046098: step 2834, loss 0.046262, acc 0.976562\n",
      "2017-01-10T22:14:48.212672: step 2835, loss 0.157846, acc 0.960938\n",
      "2017-01-10T22:14:50.310622: step 2836, loss 0.0654805, acc 0.976562\n",
      "2017-01-10T22:14:52.429746: step 2837, loss 0.0962583, acc 0.96875\n",
      "2017-01-10T22:14:54.511762: step 2838, loss 0.0910632, acc 0.976562\n",
      "2017-01-10T22:14:56.592083: step 2839, loss 0.0694492, acc 0.992188\n",
      "2017-01-10T22:14:58.669709: step 2840, loss 0.0287298, acc 0.992188\n",
      "2017-01-10T22:15:00.745615: step 2841, loss 0.1841, acc 0.945312\n",
      "2017-01-10T22:15:02.836656: step 2842, loss 0.0690229, acc 0.96875\n",
      "2017-01-10T22:15:04.947123: step 2843, loss 0.0957398, acc 0.96875\n",
      "2017-01-10T22:15:07.019493: step 2844, loss 0.087599, acc 0.960938\n",
      "2017-01-10T22:15:09.082649: step 2845, loss 0.0980294, acc 0.976562\n",
      "2017-01-10T22:15:11.137861: step 2846, loss 0.123274, acc 0.960938\n",
      "2017-01-10T22:15:13.903426: step 2847, loss 0.0333684, acc 0.992188\n",
      "2017-01-10T22:15:16.380931: step 2848, loss 0.150411, acc 0.945312\n",
      "2017-01-10T22:15:18.535104: step 2849, loss 0.134736, acc 0.960938\n",
      "2017-01-10T22:15:20.942597: step 2850, loss 0.156725, acc 0.945312\n",
      "2017-01-10T22:15:23.079195: step 2851, loss 0.101198, acc 0.984375\n",
      "2017-01-10T22:15:25.159520: step 2852, loss 0.132943, acc 0.960938\n",
      "2017-01-10T22:15:27.856788: step 2853, loss 0.0681881, acc 0.96875\n",
      "2017-01-10T22:15:30.247918: step 2854, loss 0.0633444, acc 0.976562\n",
      "2017-01-10T22:15:32.328668: step 2855, loss 0.10371, acc 0.953125\n",
      "2017-01-10T22:15:34.445681: step 2856, loss 0.0638703, acc 0.984375\n",
      "2017-01-10T22:15:36.533918: step 2857, loss 0.100245, acc 0.953125\n",
      "2017-01-10T22:15:38.609932: step 2858, loss 0.133381, acc 0.96875\n",
      "2017-01-10T22:15:40.706127: step 2859, loss 0.120038, acc 0.953125\n",
      "2017-01-10T22:15:42.896807: step 2860, loss 0.071431, acc 0.976562\n",
      "2017-01-10T22:15:45.055634: step 2861, loss 0.0350342, acc 0.992188\n",
      "2017-01-10T22:15:47.582154: step 2862, loss 0.138279, acc 0.953125\n",
      "2017-01-10T22:15:49.867732: step 2863, loss 0.336835, acc 0.929688\n",
      "2017-01-10T22:15:52.965730: step 2864, loss 0.0877353, acc 0.976562\n",
      "2017-01-10T22:15:55.480581: step 2865, loss 0.0674509, acc 0.992188\n",
      "2017-01-10T22:15:57.582927: step 2866, loss 0.109206, acc 0.96875\n",
      "2017-01-10T22:15:59.827194: step 2867, loss 0.0168652, acc 0.992188\n",
      "2017-01-10T22:16:02.317285: step 2868, loss 0.0738408, acc 0.976562\n",
      "2017-01-10T22:16:05.267135: step 2869, loss 0.159689, acc 0.945312\n",
      "2017-01-10T22:16:07.658757: step 2870, loss 0.0318695, acc 0.992188\n",
      "2017-01-10T22:16:10.494840: step 2871, loss 0.0806433, acc 0.976562\n",
      "2017-01-10T22:16:13.750412: step 2872, loss 0.0832646, acc 0.96875\n",
      "2017-01-10T22:16:16.066258: step 2873, loss 0.0158876, acc 1\n",
      "2017-01-10T22:16:18.120144: step 2874, loss 0.159189, acc 0.929688\n",
      "2017-01-10T22:16:20.614134: step 2875, loss 0.0872663, acc 0.976562\n",
      "2017-01-10T22:16:23.095871: step 2876, loss 0.0739782, acc 0.976562\n",
      "2017-01-10T22:16:25.354298: step 2877, loss 0.0929096, acc 0.96875\n",
      "2017-01-10T22:16:28.543285: step 2878, loss 0.140577, acc 0.945312\n",
      "2017-01-10T22:16:30.905008: step 2879, loss 0.0876148, acc 0.960938\n",
      "2017-01-10T22:16:33.055839: step 2880, loss 0.0562204, acc 0.984375\n",
      "2017-01-10T22:16:35.284887: step 2881, loss 0.0385795, acc 0.984375\n",
      "2017-01-10T22:16:37.488186: step 2882, loss 0.0255244, acc 0.984375\n",
      "2017-01-10T22:16:39.655106: step 2883, loss 0.0258136, acc 0.992188\n",
      "2017-01-10T22:16:42.100091: step 2884, loss 0.100612, acc 0.96875\n",
      "2017-01-10T22:16:44.760771: step 2885, loss 0.118684, acc 0.960938\n",
      "2017-01-10T22:16:47.778289: step 2886, loss 0.0867132, acc 0.976562\n",
      "2017-01-10T22:16:50.418938: step 2887, loss 0.080412, acc 0.976562\n",
      "2017-01-10T22:16:52.581087: step 2888, loss 0.0931213, acc 0.976562\n",
      "2017-01-10T22:16:54.712806: step 2889, loss 0.144489, acc 0.953125\n",
      "2017-01-10T22:16:56.893158: step 2890, loss 0.128152, acc 0.96875\n",
      "2017-01-10T22:16:59.028343: step 2891, loss 0.0818756, acc 0.96875\n",
      "2017-01-10T22:17:02.160810: step 2892, loss 0.0810553, acc 0.976562\n",
      "2017-01-10T22:17:04.908293: step 2893, loss 0.1392, acc 0.960938\n",
      "2017-01-10T22:17:07.452320: step 2894, loss 0.0434481, acc 0.984375\n",
      "2017-01-10T22:17:09.639185: step 2895, loss 0.0747756, acc 0.96875\n",
      "2017-01-10T22:17:12.089816: step 2896, loss 0.0594411, acc 0.976562\n",
      "2017-01-10T22:17:14.666919: step 2897, loss 0.119794, acc 0.960938\n",
      "2017-01-10T22:17:17.154886: step 2898, loss 0.163848, acc 0.960938\n",
      "2017-01-10T22:17:19.868814: step 2899, loss 0.0926718, acc 0.96875\n",
      "2017-01-10T22:17:23.363211: step 2900, loss 0.0553638, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:18:01.460361: step 2900, loss 0.0893767, acc 0.97416\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-2900\n",
      "\n",
      "2017-01-10T22:18:06.713403: step 2901, loss 0.064803, acc 0.984375\n",
      "2017-01-10T22:18:08.858718: step 2902, loss 0.157745, acc 0.953125\n",
      "2017-01-10T22:18:10.982188: step 2903, loss 0.158533, acc 0.945312\n",
      "2017-01-10T22:18:13.127409: step 2904, loss 0.132912, acc 0.960938\n",
      "2017-01-10T22:18:15.284246: step 2905, loss 0.237032, acc 0.929688\n",
      "2017-01-10T22:18:17.376417: step 2906, loss 0.0856984, acc 0.96875\n",
      "2017-01-10T22:18:19.476559: step 2907, loss 0.0790094, acc 0.960938\n",
      "2017-01-10T22:18:21.643110: step 2908, loss 0.144661, acc 0.984375\n",
      "2017-01-10T22:18:23.766166: step 2909, loss 0.117579, acc 0.953125\n",
      "2017-01-10T22:18:25.890246: step 2910, loss 0.0808496, acc 0.984375\n",
      "2017-01-10T22:18:27.970023: step 2911, loss 0.0231867, acc 0.992188\n",
      "2017-01-10T22:18:30.104016: step 2912, loss 0.152284, acc 0.953125\n",
      "2017-01-10T22:18:32.168774: step 2913, loss 0.0733996, acc 0.96875\n",
      "2017-01-10T22:18:34.249616: step 2914, loss 0.0759453, acc 0.976562\n",
      "2017-01-10T22:18:36.379118: step 2915, loss 0.115514, acc 0.960938\n",
      "2017-01-10T22:18:38.657236: step 2916, loss 0.080156, acc 0.976562\n",
      "2017-01-10T22:18:40.776638: step 2917, loss 0.0942871, acc 0.96875\n",
      "2017-01-10T22:18:42.857178: step 2918, loss 0.12583, acc 0.960938\n",
      "2017-01-10T22:18:44.959905: step 2919, loss 0.0955452, acc 0.96875\n",
      "2017-01-10T22:18:47.081542: step 2920, loss 0.26643, acc 0.929688\n",
      "2017-01-10T22:18:49.205313: step 2921, loss 0.0717511, acc 0.976562\n",
      "2017-01-10T22:18:51.303808: step 2922, loss 0.0806628, acc 0.96875\n",
      "2017-01-10T22:18:53.411699: step 2923, loss 0.0576033, acc 0.984375\n",
      "2017-01-10T22:18:55.475970: step 2924, loss 0.0417892, acc 0.984375\n",
      "2017-01-10T22:18:57.573700: step 2925, loss 0.0795767, acc 0.96875\n",
      "2017-01-10T22:18:59.686598: step 2926, loss 0.100615, acc 0.976562\n",
      "2017-01-10T22:19:01.786092: step 2927, loss 0.100182, acc 0.96875\n",
      "2017-01-10T22:19:03.928514: step 2928, loss 0.142664, acc 0.960938\n",
      "2017-01-10T22:19:06.178425: step 2929, loss 0.10695, acc 0.976562\n",
      "2017-01-10T22:19:08.280895: step 2930, loss 0.0965126, acc 0.96875\n",
      "2017-01-10T22:19:10.589081: step 2931, loss 0.0763875, acc 0.984375\n",
      "2017-01-10T22:19:12.681473: step 2932, loss 0.0449657, acc 0.984375\n",
      "2017-01-10T22:19:14.777793: step 2933, loss 0.0167133, acc 0.992188\n",
      "2017-01-10T22:19:16.915503: step 2934, loss 0.123162, acc 0.984375\n",
      "2017-01-10T22:19:19.002601: step 2935, loss 0.0662203, acc 0.976562\n",
      "2017-01-10T22:19:21.595116: step 2936, loss 0.0424508, acc 0.992188\n",
      "2017-01-10T22:19:24.113168: step 2937, loss 0.0839916, acc 0.96875\n",
      "2017-01-10T22:19:26.231813: step 2938, loss 0.126915, acc 0.976562\n",
      "2017-01-10T22:19:28.330912: step 2939, loss 0.111793, acc 0.96875\n",
      "2017-01-10T22:19:30.430522: step 2940, loss 0.0689054, acc 0.992188\n",
      "2017-01-10T22:19:32.576403: step 2941, loss 0.108045, acc 0.960938\n",
      "2017-01-10T22:19:34.667488: step 2942, loss 0.173617, acc 0.945312\n",
      "2017-01-10T22:19:36.763969: step 2943, loss 0.0827496, acc 0.960938\n",
      "2017-01-10T22:19:38.844356: step 2944, loss 0.0827464, acc 0.976562\n",
      "2017-01-10T22:19:40.927007: step 2945, loss 0.120928, acc 0.96875\n",
      "2017-01-10T22:19:43.236891: step 2946, loss 0.0673973, acc 0.976562\n",
      "2017-01-10T22:19:45.330700: step 2947, loss 0.0692662, acc 0.96875\n",
      "2017-01-10T22:19:47.440719: step 2948, loss 0.0398182, acc 0.992188\n",
      "2017-01-10T22:19:49.509045: step 2949, loss 0.186649, acc 0.976562\n",
      "2017-01-10T22:19:51.649585: step 2950, loss 0.0489032, acc 0.984375\n",
      "2017-01-10T22:19:53.770915: step 2951, loss 0.0559448, acc 0.984375\n",
      "2017-01-10T22:19:55.888020: step 2952, loss 0.0740298, acc 0.976562\n",
      "2017-01-10T22:19:58.003707: step 2953, loss 0.128529, acc 0.960938\n",
      "2017-01-10T22:20:00.118929: step 2954, loss 0.137159, acc 0.96875\n",
      "2017-01-10T22:20:02.201386: step 2955, loss 0.446495, acc 0.96875\n",
      "2017-01-10T22:20:04.326714: step 2956, loss 0.112589, acc 0.960938\n",
      "2017-01-10T22:20:06.434662: step 2957, loss 0.0860697, acc 0.984375\n",
      "2017-01-10T22:20:08.549728: step 2958, loss 0.0554251, acc 0.976562\n",
      "2017-01-10T22:20:10.649924: step 2959, loss 0.0840129, acc 0.976562\n",
      "2017-01-10T22:20:12.718734: step 2960, loss 0.03321, acc 0.992188\n",
      "2017-01-10T22:20:14.990541: step 2961, loss 0.0704892, acc 0.96875\n",
      "2017-01-10T22:20:17.102270: step 2962, loss 0.158227, acc 0.960938\n",
      "2017-01-10T22:20:19.202279: step 2963, loss 0.0673612, acc 0.96875\n",
      "2017-01-10T22:20:21.349907: step 2964, loss 0.116481, acc 0.960938\n",
      "2017-01-10T22:20:23.462814: step 2965, loss 0.111221, acc 0.960938\n",
      "2017-01-10T22:20:25.548496: step 2966, loss 0.160254, acc 0.9375\n",
      "2017-01-10T22:20:27.794439: step 2967, loss 0.0589894, acc 0.984375\n",
      "2017-01-10T22:20:30.032596: step 2968, loss 0.0818472, acc 0.96875\n",
      "2017-01-10T22:20:32.394754: step 2969, loss 0.106606, acc 0.960938\n",
      "2017-01-10T22:20:34.485583: step 2970, loss 0.0721287, acc 0.96875\n",
      "2017-01-10T22:20:36.885591: step 2971, loss 0.106819, acc 0.953125\n",
      "2017-01-10T22:20:38.980314: step 2972, loss 0.0558503, acc 0.984375\n",
      "2017-01-10T22:20:41.086796: step 2973, loss 0.111472, acc 0.953125\n",
      "2017-01-10T22:20:43.181855: step 2974, loss 0.080176, acc 0.976562\n",
      "2017-01-10T22:20:45.273889: step 2975, loss 0.089654, acc 0.96875\n",
      "2017-01-10T22:20:47.565226: step 2976, loss 0.0940736, acc 0.976562\n",
      "2017-01-10T22:20:49.672061: step 2977, loss 0.0666416, acc 0.976562\n",
      "2017-01-10T22:20:51.782090: step 2978, loss 0.124529, acc 0.96875\n",
      "2017-01-10T22:20:53.901611: step 2979, loss 0.064832, acc 0.984375\n",
      "2017-01-10T22:20:55.999607: step 2980, loss 0.189937, acc 0.9375\n",
      "2017-01-10T22:20:58.086923: step 2981, loss 0.124977, acc 0.945312\n",
      "2017-01-10T22:21:00.206717: step 2982, loss 0.0848847, acc 0.96875\n",
      "2017-01-10T22:21:02.271858: step 2983, loss 0.16264, acc 0.9375\n",
      "2017-01-10T22:21:04.396048: step 2984, loss 0.112991, acc 0.96875\n",
      "2017-01-10T22:21:06.511802: step 2985, loss 0.259474, acc 0.9375\n",
      "2017-01-10T22:21:08.590329: step 2986, loss 0.205743, acc 0.9375\n",
      "2017-01-10T22:21:10.653521: step 2987, loss 0.0652826, acc 0.984375\n",
      "2017-01-10T22:21:12.722966: step 2988, loss 0.0515776, acc 0.984375\n",
      "2017-01-10T22:21:14.835129: step 2989, loss 0.0848573, acc 0.96875\n",
      "2017-01-10T22:21:17.567097: step 2990, loss 0.0924684, acc 0.96875\n",
      "2017-01-10T22:21:19.885772: step 2991, loss 0.0939383, acc 0.976562\n",
      "2017-01-10T22:21:22.052226: step 2992, loss 0.150547, acc 0.945312\n",
      "2017-01-10T22:21:24.163651: step 2993, loss 0.0673365, acc 0.976562\n",
      "2017-01-10T22:21:26.257086: step 2994, loss 0.0509747, acc 0.976562\n",
      "2017-01-10T22:21:28.397188: step 2995, loss 0.112729, acc 0.960938\n",
      "2017-01-10T22:21:30.525004: step 2996, loss 0.183118, acc 0.960938\n",
      "2017-01-10T22:21:33.497988: step 2997, loss 0.0502574, acc 0.976562\n",
      "2017-01-10T22:21:36.531126: step 2998, loss 0.173918, acc 0.960938\n",
      "2017-01-10T22:21:38.736812: step 2999, loss 0.0677042, acc 0.96875\n",
      "2017-01-10T22:21:40.863457: step 3000, loss 0.0607108, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:22:13.455102: step 3000, loss 0.0877194, acc 0.9744\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3000\n",
      "\n",
      "2017-01-10T22:22:18.221359: step 3001, loss 0.122942, acc 0.976562\n",
      "2017-01-10T22:22:20.443851: step 3002, loss 0.0958264, acc 0.976562\n",
      "2017-01-10T22:22:22.739821: step 3003, loss 0.0951732, acc 0.976562\n",
      "2017-01-10T22:22:25.093243: step 3004, loss 0.0904416, acc 0.96875\n",
      "2017-01-10T22:22:27.180267: step 3005, loss 0.08587, acc 0.96875\n",
      "2017-01-10T22:22:29.203919: step 3006, loss 0.131187, acc 0.96875\n",
      "2017-01-10T22:22:31.356999: step 3007, loss 0.0556189, acc 0.992188\n",
      "2017-01-10T22:22:33.535437: step 3008, loss 0.0811314, acc 0.976562\n",
      "2017-01-10T22:22:35.718161: step 3009, loss 0.0946012, acc 0.976562\n",
      "2017-01-10T22:22:38.245211: step 3010, loss 0.0470221, acc 0.984375\n",
      "2017-01-10T22:22:40.887053: step 3011, loss 0.109619, acc 0.96875\n",
      "2017-01-10T22:22:43.016763: step 3012, loss 0.105336, acc 0.96875\n",
      "2017-01-10T22:22:45.081832: step 3013, loss 0.121822, acc 0.96875\n",
      "2017-01-10T22:22:47.188488: step 3014, loss 0.111842, acc 0.960938\n",
      "2017-01-10T22:22:49.322283: step 3015, loss 0.0650635, acc 0.984375\n",
      "2017-01-10T22:22:51.462697: step 3016, loss 0.227092, acc 0.976562\n",
      "2017-01-10T22:22:53.588001: step 3017, loss 0.087313, acc 0.984375\n",
      "2017-01-10T22:22:55.702585: step 3018, loss 0.127284, acc 0.960938\n",
      "2017-01-10T22:22:57.986470: step 3019, loss 0.0912863, acc 0.976562\n",
      "2017-01-10T22:23:00.089454: step 3020, loss 0.122697, acc 0.945312\n",
      "2017-01-10T22:23:02.178562: step 3021, loss 0.184964, acc 0.96875\n",
      "2017-01-10T22:23:04.289092: step 3022, loss 0.0664633, acc 0.984375\n",
      "2017-01-10T22:23:06.423471: step 3023, loss 0.0869355, acc 0.984375\n",
      "2017-01-10T22:23:08.531943: step 3024, loss 0.111219, acc 0.960938\n",
      "2017-01-10T22:23:10.666201: step 3025, loss 0.0666727, acc 0.96875\n",
      "2017-01-10T22:23:12.718432: step 3026, loss 0.0934258, acc 0.984375\n",
      "2017-01-10T22:23:14.821285: step 3027, loss 0.0900009, acc 0.976562\n",
      "2017-01-10T22:23:16.947423: step 3028, loss 0.0968236, acc 0.976562\n",
      "2017-01-10T22:23:19.445359: step 3029, loss 0.11078, acc 0.976562\n",
      "2017-01-10T22:23:21.870733: step 3030, loss 0.14367, acc 0.96875\n",
      "2017-01-10T22:23:24.017918: step 3031, loss 0.148143, acc 0.960938\n",
      "2017-01-10T22:23:26.229276: step 3032, loss 0.0406318, acc 0.984375\n",
      "2017-01-10T22:23:28.756953: step 3033, loss 0.0354941, acc 0.992188\n",
      "2017-01-10T22:23:31.976615: step 3034, loss 0.0612378, acc 0.976562\n",
      "2017-01-10T22:23:35.226413: step 3035, loss 0.0540926, acc 0.984375\n",
      "2017-01-10T22:23:38.340095: step 3036, loss 0.085444, acc 0.976562\n",
      "2017-01-10T22:23:40.992038: step 3037, loss 0.123018, acc 0.96875\n",
      "2017-01-10T22:23:43.123657: step 3038, loss 0.0239834, acc 0.992188\n",
      "2017-01-10T22:23:45.715475: step 3039, loss 0.130386, acc 0.976562\n",
      "2017-01-10T22:23:48.107631: step 3040, loss 0.215004, acc 0.929688\n",
      "2017-01-10T22:23:50.583056: step 3041, loss 0.0795988, acc 0.96875\n",
      "2017-01-10T22:23:53.038368: step 3042, loss 0.0685106, acc 0.976562\n",
      "2017-01-10T22:23:55.164872: step 3043, loss 0.118065, acc 0.960938\n",
      "2017-01-10T22:23:57.568451: step 3044, loss 0.0796777, acc 0.96875\n",
      "2017-01-10T22:23:59.860488: step 3045, loss 0.117733, acc 0.976562\n",
      "2017-01-10T22:24:02.181472: step 3046, loss 0.111079, acc 0.960938\n",
      "2017-01-10T22:24:04.473610: step 3047, loss 0.135037, acc 0.960938\n",
      "2017-01-10T22:24:06.788419: step 3048, loss 0.113613, acc 0.96875\n",
      "2017-01-10T22:24:09.151980: step 3049, loss 0.0459521, acc 0.984375\n",
      "2017-01-10T22:24:11.738986: step 3050, loss 0.0929051, acc 0.960938\n",
      "2017-01-10T22:24:13.852788: step 3051, loss 0.0860217, acc 0.976562\n",
      "2017-01-10T22:24:15.983424: step 3052, loss 0.0947318, acc 0.976562\n",
      "2017-01-10T22:24:19.225379: step 3053, loss 0.0532479, acc 0.992188\n",
      "2017-01-10T22:24:21.341322: step 3054, loss 0.0941697, acc 0.96875\n",
      "2017-01-10T22:24:23.435421: step 3055, loss 0.102613, acc 0.960938\n",
      "2017-01-10T22:24:25.947236: step 3056, loss 0.172385, acc 0.953125\n",
      "2017-01-10T22:24:28.284293: step 3057, loss 0.058099, acc 0.984375\n",
      "2017-01-10T22:24:30.387588: step 3058, loss 0.0928188, acc 0.960938\n",
      "2017-01-10T22:24:32.460948: step 3059, loss 0.122406, acc 0.96875\n",
      "2017-01-10T22:24:34.766327: step 3060, loss 0.0816533, acc 0.984375\n",
      "2017-01-10T22:24:36.870852: step 3061, loss 0.0651713, acc 0.976562\n",
      "2017-01-10T22:24:38.972821: step 3062, loss 0.0722294, acc 0.976562\n",
      "2017-01-10T22:24:41.065862: step 3063, loss 0.0240407, acc 0.992188\n",
      "2017-01-10T22:24:43.181552: step 3064, loss 0.145279, acc 0.953125\n",
      "2017-01-10T22:24:45.530081: step 3065, loss 0.111497, acc 0.960938\n",
      "2017-01-10T22:24:47.799804: step 3066, loss 0.0731395, acc 0.96875\n",
      "2017-01-10T22:24:50.012162: step 3067, loss 0.151509, acc 0.953125\n",
      "2017-01-10T22:24:52.179397: step 3068, loss 0.00792787, acc 1\n",
      "2017-01-10T22:24:54.283768: step 3069, loss 0.134508, acc 0.945312\n",
      "2017-01-10T22:24:56.385049: step 3070, loss 0.0434062, acc 0.992188\n",
      "2017-01-10T22:24:58.503499: step 3071, loss 0.128407, acc 0.953125\n",
      "2017-01-10T22:25:00.630377: step 3072, loss 0.0734624, acc 0.976562\n",
      "2017-01-10T22:25:02.742607: step 3073, loss 0.0468016, acc 0.976562\n",
      "2017-01-10T22:25:04.898067: step 3074, loss 0.171638, acc 0.960938\n",
      "2017-01-10T22:25:07.187707: step 3075, loss 0.14198, acc 0.945312\n",
      "2017-01-10T22:25:09.293809: step 3076, loss 0.0760601, acc 0.984375\n",
      "2017-01-10T22:25:11.383010: step 3077, loss 0.0693755, acc 0.976562\n",
      "2017-01-10T22:25:13.473383: step 3078, loss 0.09986, acc 0.96875\n",
      "2017-01-10T22:25:15.588391: step 3079, loss 0.0154446, acc 0.992188\n",
      "2017-01-10T22:25:17.690754: step 3080, loss 0.120562, acc 0.976562\n",
      "2017-01-10T22:25:19.901063: step 3081, loss 0.0886913, acc 0.984375\n",
      "2017-01-10T22:25:21.987901: step 3082, loss 0.0897726, acc 0.96875\n",
      "2017-01-10T22:25:24.059472: step 3083, loss 0.0615027, acc 0.992188\n",
      "2017-01-10T22:25:26.118087: step 3084, loss 0.0795581, acc 0.976562\n",
      "2017-01-10T22:25:28.223881: step 3085, loss 0.12346, acc 0.960938\n",
      "2017-01-10T22:25:30.300730: step 3086, loss 0.0681689, acc 0.976562\n",
      "2017-01-10T22:25:32.381522: step 3087, loss 0.0933211, acc 0.96875\n",
      "2017-01-10T22:25:34.464710: step 3088, loss 0.155967, acc 0.976562\n",
      "2017-01-10T22:25:36.575908: step 3089, loss 0.138563, acc 0.953125\n",
      "2017-01-10T22:25:38.750430: step 3090, loss 0.0700745, acc 0.984375\n",
      "2017-01-10T22:25:40.903411: step 3091, loss 0.0823663, acc 0.960938\n",
      "2017-01-10T22:25:43.009379: step 3092, loss 0.0824326, acc 0.96875\n",
      "2017-01-10T22:25:45.122756: step 3093, loss 0.0269351, acc 0.992188\n",
      "2017-01-10T22:25:47.209329: step 3094, loss 0.107954, acc 0.96875\n",
      "2017-01-10T22:25:49.306044: step 3095, loss 0.0876507, acc 0.976562\n",
      "2017-01-10T22:25:51.566345: step 3096, loss 0.144067, acc 0.953125\n",
      "2017-01-10T22:25:53.635884: step 3097, loss 0.0841239, acc 0.976562\n",
      "2017-01-10T22:25:55.746309: step 3098, loss 0.071478, acc 0.984375\n",
      "2017-01-10T22:25:57.851813: step 3099, loss 0.184491, acc 0.945312\n",
      "2017-01-10T22:25:59.911720: step 3100, loss 0.0673776, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:26:34.070576: step 3100, loss 0.0871772, acc 0.97508\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3100\n",
      "\n",
      "2017-01-10T22:26:38.656330: step 3101, loss 0.205954, acc 0.929688\n",
      "2017-01-10T22:26:40.792834: step 3102, loss 0.0614847, acc 0.984375\n",
      "2017-01-10T22:26:42.926270: step 3103, loss 0.0432275, acc 0.976562\n",
      "2017-01-10T22:26:45.272719: step 3104, loss 0.178059, acc 0.96875\n",
      "2017-01-10T22:26:48.392249: step 3105, loss 0.0492868, acc 0.976562\n",
      "2017-01-10T22:26:50.667821: step 3106, loss 0.0702043, acc 0.976562\n",
      "2017-01-10T22:26:52.808808: step 3107, loss 0.167899, acc 0.953125\n",
      "2017-01-10T22:26:54.931497: step 3108, loss 0.0374082, acc 0.992188\n",
      "2017-01-10T22:26:57.032392: step 3109, loss 0.169705, acc 0.96875\n",
      "2017-01-10T22:26:59.161671: step 3110, loss 0.0715498, acc 0.96875\n",
      "2017-01-10T22:27:01.316034: step 3111, loss 0.0532948, acc 0.976562\n",
      "2017-01-10T22:27:03.481157: step 3112, loss 0.105416, acc 0.960938\n",
      "2017-01-10T22:27:05.632129: step 3113, loss 0.0687398, acc 0.976562\n",
      "2017-01-10T22:27:07.758502: step 3114, loss 0.0395454, acc 0.984375\n",
      "2017-01-10T22:27:09.916809: step 3115, loss 0.0522608, acc 0.984375\n",
      "2017-01-10T22:27:12.045632: step 3116, loss 0.134057, acc 0.945312\n",
      "2017-01-10T22:27:14.188480: step 3117, loss 0.0946298, acc 0.96875\n",
      "2017-01-10T22:27:16.433862: step 3118, loss 0.110902, acc 0.960938\n",
      "2017-01-10T22:27:18.559227: step 3119, loss 0.102472, acc 0.960938\n",
      "2017-01-10T22:27:20.787831: step 3120, loss 0.0958332, acc 0.953125\n",
      "2017-01-10T22:27:22.911407: step 3121, loss 0.0875243, acc 0.96875\n",
      "2017-01-10T22:27:25.010550: step 3122, loss 0.135405, acc 0.960938\n",
      "2017-01-10T22:27:27.172501: step 3123, loss 0.0801169, acc 0.984375\n",
      "2017-01-10T22:27:29.489205: step 3124, loss 0.0939888, acc 0.976562\n",
      "2017-01-10T22:27:31.630047: step 3125, loss 0.106322, acc 0.96875\n",
      "2017-01-10T22:27:33.769387: step 3126, loss 0.132, acc 0.96875\n",
      "2017-01-10T22:27:35.883135: step 3127, loss 0.0713646, acc 0.984375\n",
      "2017-01-10T22:27:38.016539: step 3128, loss 0.10135, acc 0.976562\n",
      "2017-01-10T22:27:40.173438: step 3129, loss 0.0803086, acc 0.984375\n",
      "2017-01-10T22:27:42.329290: step 3130, loss 0.0132814, acc 1\n",
      "2017-01-10T22:27:45.042015: step 3131, loss 0.0505202, acc 0.992188\n",
      "2017-01-10T22:27:47.495635: step 3132, loss 0.0954291, acc 0.96875\n",
      "2017-01-10T22:27:49.878955: step 3133, loss 0.0739443, acc 0.96875\n",
      "2017-01-10T22:27:52.068373: step 3134, loss 0.0572855, acc 0.984375\n",
      "2017-01-10T22:27:54.166496: step 3135, loss 0.110653, acc 0.960938\n",
      "2017-01-10T22:27:56.345708: step 3136, loss 0.0378444, acc 0.992188\n",
      "2017-01-10T22:27:58.435089: step 3137, loss 0.0492358, acc 0.984375\n",
      "2017-01-10T22:28:00.574580: step 3138, loss 0.0734254, acc 0.96875\n",
      "2017-01-10T22:28:02.734555: step 3139, loss 0.0175272, acc 1\n",
      "2017-01-10T22:28:04.885590: step 3140, loss 0.134867, acc 0.953125\n",
      "2017-01-10T22:28:07.025844: step 3141, loss 0.0790998, acc 0.976562\n",
      "2017-01-10T22:28:09.129967: step 3142, loss 0.0816616, acc 0.96875\n",
      "2017-01-10T22:28:11.242689: step 3143, loss 0.0714463, acc 0.976562\n",
      "2017-01-10T22:28:13.371030: step 3144, loss 0.104453, acc 0.976562\n",
      "2017-01-10T22:28:15.458295: step 3145, loss 0.161462, acc 0.945312\n",
      "2017-01-10T22:28:17.577270: step 3146, loss 0.0796002, acc 0.96875\n",
      "2017-01-10T22:28:19.762566: step 3147, loss 0.0550256, acc 0.992188\n",
      "2017-01-10T22:28:22.177672: step 3148, loss 0.105929, acc 0.960938\n",
      "2017-01-10T22:28:24.284367: step 3149, loss 0.130181, acc 0.960938\n",
      "2017-01-10T22:28:26.356322: step 3150, loss 0.0494373, acc 0.984375\n",
      "2017-01-10T22:28:28.458480: step 3151, loss 0.0601893, acc 0.992188\n",
      "2017-01-10T22:28:30.585031: step 3152, loss 0.0355547, acc 0.992188\n",
      "2017-01-10T22:28:32.724955: step 3153, loss 0.089056, acc 0.960938\n",
      "2017-01-10T22:28:35.059832: step 3154, loss 0.0417173, acc 0.984375\n",
      "2017-01-10T22:28:37.307937: step 3155, loss 0.0946774, acc 0.96875\n",
      "2017-01-10T22:28:39.552765: step 3156, loss 0.160392, acc 0.945312\n",
      "2017-01-10T22:28:41.795919: step 3157, loss 0.0713889, acc 0.976562\n",
      "2017-01-10T22:28:44.491607: step 3158, loss 0.124, acc 0.953125\n",
      "2017-01-10T22:28:47.137524: step 3159, loss 0.182484, acc 0.960938\n",
      "2017-01-10T22:28:49.870086: step 3160, loss 0.0977489, acc 0.96875\n",
      "2017-01-10T22:28:52.864624: step 3161, loss 0.1408, acc 0.9375\n",
      "2017-01-10T22:28:55.491878: step 3162, loss 0.0535917, acc 0.976562\n",
      "2017-01-10T22:28:57.676372: step 3163, loss 0.151025, acc 0.945312\n",
      "2017-01-10T22:28:59.857458: step 3164, loss 0.0567991, acc 0.976562\n",
      "2017-01-10T22:29:02.039072: step 3165, loss 0.0835484, acc 0.976562\n",
      "2017-01-10T22:29:04.212289: step 3166, loss 0.128966, acc 0.960938\n",
      "2017-01-10T22:29:06.374337: step 3167, loss 0.0407213, acc 0.984375\n",
      "2017-01-10T22:29:08.514123: step 3168, loss 0.0389163, acc 0.992188\n",
      "2017-01-10T22:29:10.673833: step 3169, loss 0.0548636, acc 0.992188\n",
      "2017-01-10T22:29:12.799931: step 3170, loss 0.113366, acc 0.976562\n",
      "2017-01-10T22:29:14.966445: step 3171, loss 0.181022, acc 0.960938\n",
      "2017-01-10T22:29:17.125941: step 3172, loss 0.061814, acc 0.976562\n",
      "2017-01-10T22:29:19.288540: step 3173, loss 0.145655, acc 0.960938\n",
      "2017-01-10T22:29:21.576199: step 3174, loss 0.206709, acc 0.960938\n",
      "2017-01-10T22:29:23.731946: step 3175, loss 0.0826935, acc 0.96875\n",
      "2017-01-10T22:29:26.003640: step 3176, loss 0.11339, acc 0.960938\n",
      "2017-01-10T22:29:28.215874: step 3177, loss 0.0622767, acc 0.984375\n",
      "2017-01-10T22:29:30.364309: step 3178, loss 0.0736799, acc 0.984375\n",
      "2017-01-10T22:29:32.521072: step 3179, loss 0.130304, acc 0.960938\n",
      "2017-01-10T22:29:34.694140: step 3180, loss 0.0384559, acc 0.984375\n",
      "2017-01-10T22:29:36.857209: step 3181, loss 0.11882, acc 0.96875\n",
      "2017-01-10T22:29:38.983757: step 3182, loss 0.162017, acc 0.9375\n",
      "2017-01-10T22:29:41.129110: step 3183, loss 0.0758347, acc 0.960938\n",
      "2017-01-10T22:29:43.274282: step 3184, loss 0.100784, acc 0.96875\n",
      "2017-01-10T22:29:45.471537: step 3185, loss 0.146198, acc 0.960938\n",
      "2017-01-10T22:29:47.658165: step 3186, loss 0.201667, acc 0.960938\n",
      "2017-01-10T22:29:49.857147: step 3187, loss 0.15279, acc 0.953125\n",
      "2017-01-10T22:29:52.310310: step 3188, loss 0.144182, acc 0.960938\n",
      "2017-01-10T22:29:54.432545: step 3189, loss 0.0984268, acc 0.96875\n",
      "2017-01-10T22:29:56.594163: step 3190, loss 0.0997518, acc 0.984375\n",
      "2017-01-10T22:29:58.935637: step 3191, loss 0.104795, acc 0.976562\n",
      "2017-01-10T22:30:01.126828: step 3192, loss 0.0567563, acc 0.96875\n",
      "2017-01-10T22:30:03.307531: step 3193, loss 0.0414737, acc 0.976562\n",
      "2017-01-10T22:30:05.485938: step 3194, loss 0.0928618, acc 0.976562\n",
      "2017-01-10T22:30:08.235570: step 3195, loss 0.0849581, acc 0.96875\n",
      "2017-01-10T22:30:10.677658: step 3196, loss 0.0788207, acc 0.96875\n",
      "2017-01-10T22:30:12.820763: step 3197, loss 0.105736, acc 0.960938\n",
      "2017-01-10T22:30:14.960579: step 3198, loss 0.096233, acc 0.960938\n",
      "2017-01-10T22:30:17.119521: step 3199, loss 0.0309789, acc 0.984375\n",
      "2017-01-10T22:30:19.279666: step 3200, loss 0.142153, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:30:55.918150: step 3200, loss 0.0867535, acc 0.97472\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3200\n",
      "\n",
      "2017-01-10T22:31:01.356404: step 3201, loss 0.0446817, acc 0.984375\n",
      "2017-01-10T22:31:03.779848: step 3202, loss 0.090335, acc 0.96875\n",
      "2017-01-10T22:31:06.110092: step 3203, loss 0.0912941, acc 0.96875\n",
      "2017-01-10T22:31:08.342927: step 3204, loss 0.111513, acc 0.984375\n",
      "2017-01-10T22:31:10.534590: step 3205, loss 0.0349241, acc 0.984375\n",
      "2017-01-10T22:31:12.744719: step 3206, loss 0.131839, acc 0.960938\n",
      "2017-01-10T22:31:14.912610: step 3207, loss 0.0793923, acc 0.984375\n",
      "2017-01-10T22:31:17.078306: step 3208, loss 0.0787592, acc 0.976562\n",
      "2017-01-10T22:31:19.180418: step 3209, loss 0.105642, acc 0.96875\n",
      "2017-01-10T22:31:21.336650: step 3210, loss 0.0403156, acc 0.984375\n",
      "2017-01-10T22:31:23.479534: step 3211, loss 0.0507389, acc 0.976562\n",
      "2017-01-10T22:31:25.631017: step 3212, loss 0.0664902, acc 0.984375\n",
      "2017-01-10T22:31:27.802144: step 3213, loss 0.0930463, acc 0.96875\n",
      "2017-01-10T22:31:30.213456: step 3214, loss 0.0848696, acc 0.96875\n",
      "2017-01-10T22:31:32.447926: step 3215, loss 0.0241595, acc 0.992188\n",
      "2017-01-10T22:31:34.616291: step 3216, loss 0.0871232, acc 0.976562\n",
      "2017-01-10T22:31:36.878514: step 3217, loss 0.0663766, acc 0.984375\n",
      "2017-01-10T22:31:39.065190: step 3218, loss 0.0534462, acc 0.984375\n",
      "2017-01-10T22:31:41.236854: step 3219, loss 0.0774908, acc 0.96875\n",
      "2017-01-10T22:31:43.417774: step 3220, loss 0.148324, acc 0.960938\n",
      "2017-01-10T22:31:46.563161: step 3221, loss 0.125187, acc 0.953125\n",
      "2017-01-10T22:31:48.793377: step 3222, loss 0.136184, acc 0.960938\n",
      "2017-01-10T22:31:50.986405: step 3223, loss 0.0730485, acc 0.96875\n",
      "2017-01-10T22:31:53.101493: step 3224, loss 0.0860104, acc 0.984375\n",
      "2017-01-10T22:31:55.240209: step 3225, loss 0.0831457, acc 0.96875\n",
      "2017-01-10T22:31:57.514160: step 3226, loss 0.0398275, acc 0.984375\n",
      "2017-01-10T22:32:00.391745: step 3227, loss 0.0326725, acc 0.992188\n",
      "2017-01-10T22:32:04.047019: step 3228, loss 0.139716, acc 0.945312\n",
      "2017-01-10T22:32:10.529917: step 3229, loss 0.0826903, acc 0.976562\n",
      "2017-01-10T22:32:15.339125: step 3230, loss 0.0573784, acc 0.992188\n",
      "2017-01-10T22:32:17.620745: step 3231, loss 0.116496, acc 0.96875\n",
      "2017-01-10T22:32:19.865951: step 3232, loss 0.0376042, acc 0.984375\n",
      "2017-01-10T22:32:22.153857: step 3233, loss 0.0304048, acc 0.992188\n",
      "2017-01-10T22:32:24.393542: step 3234, loss 0.055833, acc 0.984375\n",
      "2017-01-10T22:32:26.574034: step 3235, loss 0.0406576, acc 0.992188\n",
      "2017-01-10T22:32:28.934780: step 3236, loss 0.112865, acc 0.96875\n",
      "2017-01-10T22:32:32.140126: step 3237, loss 0.0799683, acc 0.976562\n",
      "2017-01-10T22:32:34.364615: step 3238, loss 0.109605, acc 0.960938\n",
      "2017-01-10T22:32:36.513586: step 3239, loss 0.0784155, acc 0.960938\n",
      "2017-01-10T22:32:38.635655: step 3240, loss 0.0301721, acc 0.992188\n",
      "2017-01-10T22:32:40.753093: step 3241, loss 0.0351655, acc 0.992188\n",
      "2017-01-10T22:32:42.875162: step 3242, loss 0.0525439, acc 0.976562\n",
      "2017-01-10T22:32:44.966302: step 3243, loss 0.0628736, acc 0.984375\n",
      "2017-01-10T22:32:47.101510: step 3244, loss 0.119111, acc 0.976562\n",
      "2017-01-10T22:32:49.315735: step 3245, loss 0.10314, acc 0.960938\n",
      "2017-01-10T22:32:51.425491: step 3246, loss 0.0911955, acc 0.960938\n",
      "2017-01-10T22:32:53.526309: step 3247, loss 0.060579, acc 0.984375\n",
      "2017-01-10T22:32:55.614307: step 3248, loss 0.13264, acc 0.960938\n",
      "2017-01-10T22:32:57.718046: step 3249, loss 0.0160686, acc 1\n",
      "2017-01-10T22:32:59.787873: step 3250, loss 0.0725966, acc 0.96875\n",
      "2017-01-10T22:33:01.887934: step 3251, loss 0.0734369, acc 0.976562\n",
      "2017-01-10T22:33:03.990497: step 3252, loss 0.10373, acc 0.953125\n",
      "2017-01-10T22:33:06.093559: step 3253, loss 0.0315839, acc 0.992188\n",
      "2017-01-10T22:33:08.220655: step 3254, loss 0.0434761, acc 0.984375\n",
      "2017-01-10T22:33:10.285358: step 3255, loss 0.155455, acc 0.945312\n",
      "2017-01-10T22:33:12.419477: step 3256, loss 0.0640857, acc 0.984375\n",
      "2017-01-10T22:33:14.507691: step 3257, loss 0.0997494, acc 0.96875\n",
      "2017-01-10T22:33:16.667640: step 3258, loss 0.263676, acc 0.914062\n",
      "2017-01-10T22:33:18.750744: step 3259, loss 0.0604679, acc 0.976562\n",
      "2017-01-10T22:33:21.005611: step 3260, loss 0.125163, acc 0.976562\n",
      "2017-01-10T22:33:23.208698: step 3261, loss 0.127652, acc 0.960938\n",
      "2017-01-10T22:33:25.363770: step 3262, loss 0.0732781, acc 0.976562\n",
      "2017-01-10T22:33:27.833270: step 3263, loss 0.0492147, acc 0.984375\n",
      "2017-01-10T22:33:30.630271: step 3264, loss 0.0472014, acc 0.984375\n",
      "2017-01-10T22:33:32.713571: step 3265, loss 0.214699, acc 0.9375\n",
      "2017-01-10T22:33:34.842215: step 3266, loss 0.117388, acc 0.96875\n",
      "2017-01-10T22:33:36.855131: step 3267, loss 0.0820999, acc 0.976562\n",
      "2017-01-10T22:33:38.952969: step 3268, loss 0.00764414, acc 1\n",
      "2017-01-10T22:33:41.028936: step 3269, loss 0.0786509, acc 0.976562\n",
      "2017-01-10T22:33:43.120013: step 3270, loss 0.121001, acc 0.976562\n",
      "2017-01-10T22:33:45.219332: step 3271, loss 0.0771001, acc 0.976562\n",
      "2017-01-10T22:33:47.312172: step 3272, loss 0.110755, acc 0.953125\n",
      "2017-01-10T22:33:49.400312: step 3273, loss 0.0586101, acc 0.976562\n",
      "2017-01-10T22:33:51.669928: step 3274, loss 0.100502, acc 0.96875\n",
      "2017-01-10T22:33:53.749489: step 3275, loss 0.165593, acc 0.945312\n",
      "2017-01-10T22:33:55.848675: step 3276, loss 0.0695369, acc 0.976562\n",
      "2017-01-10T22:33:57.921656: step 3277, loss 0.155722, acc 0.945312\n",
      "2017-01-10T22:34:00.041328: step 3278, loss 0.0628863, acc 0.992188\n",
      "2017-01-10T22:34:02.166431: step 3279, loss 0.0237564, acc 0.984375\n",
      "2017-01-10T22:34:04.268667: step 3280, loss 0.114048, acc 0.976562\n",
      "2017-01-10T22:34:06.330345: step 3281, loss 0.0851385, acc 0.976562\n",
      "2017-01-10T22:34:08.428313: step 3282, loss 0.0867044, acc 0.96875\n",
      "2017-01-10T22:34:10.491433: step 3283, loss 0.160103, acc 0.960938\n",
      "2017-01-10T22:34:12.624744: step 3284, loss 0.117356, acc 0.960938\n",
      "2017-01-10T22:34:14.719112: step 3285, loss 0.0341357, acc 0.992188\n",
      "2017-01-10T22:34:16.782805: step 3286, loss 0.225782, acc 0.945312\n",
      "2017-01-10T22:34:18.896908: step 3287, loss 0.0601642, acc 0.976562\n",
      "2017-01-10T22:34:21.029428: step 3288, loss 0.0695481, acc 0.96875\n",
      "2017-01-10T22:34:23.347431: step 3289, loss 0.0989943, acc 0.960938\n",
      "2017-01-10T22:34:25.426767: step 3290, loss 0.0636143, acc 0.976562\n",
      "2017-01-10T22:34:27.513618: step 3291, loss 0.0871711, acc 0.976562\n",
      "2017-01-10T22:34:29.620106: step 3292, loss 0.0778052, acc 0.976562\n",
      "2017-01-10T22:34:31.697754: step 3293, loss 0.0511451, acc 0.984375\n",
      "2017-01-10T22:34:33.824839: step 3294, loss 0.155415, acc 0.945312\n",
      "2017-01-10T22:34:35.889539: step 3295, loss 0.0843596, acc 0.984375\n",
      "2017-01-10T22:34:37.972857: step 3296, loss 0.0448968, acc 0.984375\n",
      "2017-01-10T22:34:40.103605: step 3297, loss 0.128651, acc 0.960938\n",
      "2017-01-10T22:34:42.216103: step 3298, loss 0.0910494, acc 0.976562\n",
      "2017-01-10T22:34:44.281515: step 3299, loss 0.0638916, acc 0.984375\n",
      "2017-01-10T22:34:46.346954: step 3300, loss 0.114979, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:35:15.908385: step 3300, loss 0.0840237, acc 0.97516\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3300\n",
      "\n",
      "2017-01-10T22:35:20.478314: step 3301, loss 0.0860011, acc 0.96875\n",
      "2017-01-10T22:35:22.576147: step 3302, loss 0.0702184, acc 0.984375\n",
      "2017-01-10T22:35:24.686420: step 3303, loss 0.125113, acc 0.960938\n",
      "2017-01-10T22:35:26.770269: step 3304, loss 0.129816, acc 0.960938\n",
      "2017-01-10T22:35:28.846841: step 3305, loss 0.101076, acc 0.960938\n",
      "2017-01-10T22:35:31.085479: step 3306, loss 0.13995, acc 0.960938\n",
      "2017-01-10T22:35:33.197671: step 3307, loss 0.0990004, acc 0.984375\n",
      "2017-01-10T22:35:35.336722: step 3308, loss 0.043601, acc 0.992188\n",
      "2017-01-10T22:35:37.431574: step 3309, loss 0.0964674, acc 0.96875\n",
      "2017-01-10T22:35:39.534988: step 3310, loss 0.133212, acc 0.960938\n",
      "2017-01-10T22:35:41.606428: step 3311, loss 0.0950353, acc 0.96875\n",
      "2017-01-10T22:35:43.680132: step 3312, loss 0.15838, acc 0.960938\n",
      "2017-01-10T22:35:46.024280: step 3313, loss 0.0604707, acc 0.976562\n",
      "2017-01-10T22:35:48.110950: step 3314, loss 0.117868, acc 0.96875\n",
      "2017-01-10T22:35:50.216411: step 3315, loss 0.115867, acc 0.960938\n",
      "2017-01-10T22:35:52.378365: step 3316, loss 0.073854, acc 0.984375\n",
      "2017-01-10T22:35:54.492256: step 3317, loss 0.126097, acc 0.96875\n",
      "2017-01-10T22:35:56.606520: step 3318, loss 0.0721686, acc 0.984375\n",
      "2017-01-10T22:35:58.669247: step 3319, loss 0.126208, acc 0.953125\n",
      "2017-01-10T22:36:00.984553: step 3320, loss 0.0782509, acc 0.976562\n",
      "2017-01-10T22:36:04.752937: step 3321, loss 0.0728106, acc 0.976562\n",
      "2017-01-10T22:36:07.501798: step 3322, loss 0.0840414, acc 0.96875\n",
      "2017-01-10T22:36:09.588106: step 3323, loss 0.132194, acc 0.960938\n",
      "2017-01-10T22:36:11.693187: step 3324, loss 0.0452597, acc 0.976562\n",
      "2017-01-10T22:36:13.797981: step 3325, loss 0.0289567, acc 0.992188\n",
      "2017-01-10T22:36:15.913478: step 3326, loss 0.105855, acc 0.960938\n",
      "2017-01-10T22:36:18.507044: step 3327, loss 0.0480906, acc 0.992188\n",
      "2017-01-10T22:36:20.606161: step 3328, loss 0.129669, acc 0.960938\n",
      "2017-01-10T22:36:22.719887: step 3329, loss 0.0699352, acc 0.976562\n",
      "2017-01-10T22:36:24.834772: step 3330, loss 0.0821318, acc 0.96875\n",
      "2017-01-10T22:36:26.986634: step 3331, loss 0.0778176, acc 0.976562\n",
      "2017-01-10T22:36:29.077652: step 3332, loss 0.126436, acc 0.976562\n",
      "2017-01-10T22:36:31.241167: step 3333, loss 0.0274554, acc 0.992188\n",
      "2017-01-10T22:36:33.645936: step 3334, loss 0.141122, acc 0.960938\n",
      "2017-01-10T22:36:36.268841: step 3335, loss 0.110033, acc 0.945312\n",
      "2017-01-10T22:36:38.327237: step 3336, loss 0.0933314, acc 0.976562\n",
      "2017-01-10T22:36:40.770550: step 3337, loss 0.1016, acc 0.976562\n",
      "2017-01-10T22:36:42.872521: step 3338, loss 0.0471409, acc 0.984375\n",
      "2017-01-10T22:36:44.990722: step 3339, loss 0.075241, acc 0.984375\n",
      "2017-01-10T22:36:47.093300: step 3340, loss 0.136655, acc 0.984375\n",
      "2017-01-10T22:36:49.193491: step 3341, loss 0.0740946, acc 0.976562\n",
      "2017-01-10T22:36:51.308315: step 3342, loss 0.088027, acc 0.976562\n",
      "2017-01-10T22:36:53.393720: step 3343, loss 0.0941688, acc 0.96875\n",
      "2017-01-10T22:36:55.472738: step 3344, loss 0.0730676, acc 0.984375\n",
      "2017-01-10T22:36:57.540527: step 3345, loss 0.12007, acc 0.960938\n",
      "2017-01-10T22:36:59.638065: step 3346, loss 0.0485973, acc 0.984375\n",
      "2017-01-10T22:37:01.738904: step 3347, loss 0.0760482, acc 0.976562\n",
      "2017-01-10T22:37:03.800169: step 3348, loss 0.0806633, acc 0.976562\n",
      "2017-01-10T22:37:05.916970: step 3349, loss 0.093273, acc 0.976562\n",
      "2017-01-10T22:37:08.143437: step 3350, loss 0.109083, acc 0.96875\n",
      "2017-01-10T22:37:10.233286: step 3351, loss 0.185312, acc 0.953125\n",
      "2017-01-10T22:37:12.319506: step 3352, loss 0.120844, acc 0.960938\n",
      "2017-01-10T22:37:14.446921: step 3353, loss 0.0972057, acc 0.984375\n",
      "2017-01-10T22:37:16.477730: step 3354, loss 0.0842362, acc 0.976562\n",
      "2017-01-10T22:37:18.537806: step 3355, loss 0.109983, acc 0.960938\n",
      "2017-01-10T22:37:20.661703: step 3356, loss 0.162046, acc 0.960938\n",
      "2017-01-10T22:37:22.780533: step 3357, loss 0.107436, acc 0.960938\n",
      "2017-01-10T22:37:24.945706: step 3358, loss 0.100997, acc 0.96875\n",
      "2017-01-10T22:37:27.034189: step 3359, loss 0.0618702, acc 0.992188\n",
      "2017-01-10T22:37:29.100537: step 3360, loss 0.0726776, acc 0.976562\n",
      "2017-01-10T22:37:31.174098: step 3361, loss 0.0817315, acc 0.984375\n",
      "2017-01-10T22:37:33.265524: step 3362, loss 0.122832, acc 0.960938\n",
      "2017-01-10T22:37:35.367287: step 3363, loss 0.0792611, acc 0.984375\n",
      "2017-01-10T22:37:37.453568: step 3364, loss 0.123885, acc 0.960938\n",
      "2017-01-10T22:37:39.671658: step 3365, loss 0.10787, acc 0.976562\n",
      "2017-01-10T22:37:41.749900: step 3366, loss 0.0696972, acc 0.976562\n",
      "2017-01-10T22:37:43.830947: step 3367, loss 0.15601, acc 0.953125\n",
      "2017-01-10T22:37:45.915015: step 3368, loss 0.07848, acc 0.96875\n",
      "2017-01-10T22:37:47.983285: step 3369, loss 0.131845, acc 0.953125\n",
      "2017-01-10T22:37:50.093732: step 3370, loss 0.0456006, acc 0.984375\n",
      "2017-01-10T22:37:52.211790: step 3371, loss 0.0445098, acc 0.984375\n",
      "2017-01-10T22:37:54.310863: step 3372, loss 0.105246, acc 0.976562\n",
      "2017-01-10T22:37:56.399953: step 3373, loss 0.103786, acc 0.96875\n",
      "2017-01-10T22:37:58.479924: step 3374, loss 0.114762, acc 0.960938\n",
      "2017-01-10T22:38:00.565066: step 3375, loss 0.121547, acc 0.953125\n",
      "2017-01-10T22:38:02.645880: step 3376, loss 0.16845, acc 0.953125\n",
      "2017-01-10T22:38:04.782606: step 3377, loss 0.105422, acc 0.960938\n",
      "2017-01-10T22:38:06.845508: step 3378, loss 0.0321098, acc 0.992188\n",
      "2017-01-10T22:38:08.927540: step 3379, loss 0.126977, acc 0.960938\n",
      "2017-01-10T22:38:11.125330: step 3380, loss 0.0603778, acc 0.984375\n",
      "2017-01-10T22:38:13.212896: step 3381, loss 0.0128037, acc 1\n",
      "2017-01-10T22:38:15.332812: step 3382, loss 0.0525765, acc 0.984375\n",
      "2017-01-10T22:38:17.408575: step 3383, loss 0.181386, acc 0.945312\n",
      "2017-01-10T22:38:19.493281: step 3384, loss 0.0511893, acc 0.984375\n",
      "2017-01-10T22:38:21.571587: step 3385, loss 0.0750438, acc 0.984375\n",
      "2017-01-10T22:38:23.669289: step 3386, loss 0.102382, acc 0.960938\n",
      "2017-01-10T22:38:25.875286: step 3387, loss 0.0881243, acc 0.976562\n",
      "2017-01-10T22:38:27.976397: step 3388, loss 0.0712759, acc 0.984375\n",
      "2017-01-10T22:38:30.041885: step 3389, loss 0.192149, acc 0.984375\n",
      "2017-01-10T22:38:32.113686: step 3390, loss 0.105439, acc 0.953125\n",
      "2017-01-10T22:38:34.181795: step 3391, loss 0.0796846, acc 0.96875\n",
      "2017-01-10T22:38:36.251327: step 3392, loss 0.134144, acc 0.960938\n",
      "2017-01-10T22:38:38.368339: step 3393, loss 0.0858136, acc 0.976562\n",
      "2017-01-10T22:38:40.421901: step 3394, loss 0.0583157, acc 0.976562\n",
      "2017-01-10T22:38:42.599754: step 3395, loss 0.0494296, acc 0.976562\n",
      "2017-01-10T22:38:44.724387: step 3396, loss 0.0156167, acc 0.992188\n",
      "2017-01-10T22:38:46.816732: step 3397, loss 0.0744538, acc 0.976562\n",
      "2017-01-10T22:38:48.854859: step 3398, loss 0.144988, acc 0.953125\n",
      "2017-01-10T22:38:50.998837: step 3399, loss 0.0689274, acc 0.984375\n",
      "2017-01-10T22:38:53.108939: step 3400, loss 0.111543, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:39:24.171272: step 3400, loss 0.083766, acc 0.97532\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3400\n",
      "\n",
      "2017-01-10T22:39:28.924764: step 3401, loss 0.130558, acc 0.953125\n",
      "2017-01-10T22:39:31.704380: step 3402, loss 0.0922927, acc 0.984375\n",
      "2017-01-10T22:39:34.215577: step 3403, loss 0.120055, acc 0.960938\n",
      "2017-01-10T22:39:36.270130: step 3404, loss 0.180223, acc 0.9375\n",
      "2017-01-10T22:39:38.334914: step 3405, loss 0.089665, acc 0.953125\n",
      "2017-01-10T22:39:40.401790: step 3406, loss 0.19708, acc 0.9375\n",
      "2017-01-10T22:39:42.419090: step 3407, loss 0.0944574, acc 0.960938\n",
      "2017-01-10T22:39:44.537600: step 3408, loss 0.161651, acc 0.953125\n",
      "2017-01-10T22:39:46.811133: step 3409, loss 0.0420812, acc 0.984375\n",
      "2017-01-10T22:39:48.910129: step 3410, loss 0.107033, acc 0.960938\n",
      "2017-01-10T22:39:51.328176: step 3411, loss 0.0466441, acc 0.976562\n",
      "2017-01-10T22:39:53.573388: step 3412, loss 0.134588, acc 0.96875\n",
      "2017-01-10T22:39:55.614207: step 3413, loss 0.10932, acc 0.976562\n",
      "2017-01-10T22:39:57.736906: step 3414, loss 0.0656727, acc 0.976562\n",
      "2017-01-10T22:39:59.804113: step 3415, loss 0.0889667, acc 0.96875\n",
      "2017-01-10T22:40:02.491504: step 3416, loss 0.0946783, acc 0.96875\n",
      "2017-01-10T22:40:04.980751: step 3417, loss 0.154382, acc 0.953125\n",
      "2017-01-10T22:40:07.058094: step 3418, loss 0.204858, acc 0.945312\n",
      "2017-01-10T22:40:09.123659: step 3419, loss 0.091706, acc 0.976562\n",
      "2017-01-10T22:40:11.188808: step 3420, loss 0.108288, acc 0.96875\n",
      "2017-01-10T22:40:13.290937: step 3421, loss 0.074783, acc 0.96875\n",
      "2017-01-10T22:40:15.379606: step 3422, loss 0.0391679, acc 0.992188\n",
      "2017-01-10T22:40:17.476421: step 3423, loss 0.0918276, acc 0.96875\n",
      "2017-01-10T22:40:19.601485: step 3424, loss 0.137816, acc 0.960938\n",
      "2017-01-10T22:40:21.717396: step 3425, loss 0.0631115, acc 0.976562\n",
      "2017-01-10T22:40:24.010002: step 3426, loss 0.0464863, acc 0.992188\n",
      "2017-01-10T22:40:26.156677: step 3427, loss 0.0428482, acc 0.992188\n",
      "2017-01-10T22:40:28.257916: step 3428, loss 0.0421013, acc 0.976562\n",
      "2017-01-10T22:40:30.401754: step 3429, loss 0.0109398, acc 1\n",
      "2017-01-10T22:40:32.487731: step 3430, loss 0.119767, acc 0.976562\n",
      "2017-01-10T22:40:34.551285: step 3431, loss 0.108852, acc 0.96875\n",
      "2017-01-10T22:40:36.617728: step 3432, loss 0.122123, acc 0.960938\n",
      "2017-01-10T22:40:38.694485: step 3433, loss 0.137066, acc 0.960938\n",
      "2017-01-10T22:40:40.775918: step 3434, loss 0.0382188, acc 0.976562\n",
      "2017-01-10T22:40:42.858442: step 3435, loss 0.0788318, acc 0.960938\n",
      "2017-01-10T22:40:44.928679: step 3436, loss 0.118957, acc 0.976562\n",
      "2017-01-10T22:40:47.297171: step 3437, loss 0.310622, acc 0.953125\n",
      "2017-01-10T22:40:49.364131: step 3438, loss 0.0656359, acc 0.976562\n",
      "2017-01-10T22:40:51.479362: step 3439, loss 0.127927, acc 0.960938\n",
      "2017-01-10T22:40:53.567430: step 3440, loss 0.118748, acc 0.960938\n",
      "2017-01-10T22:40:55.788084: step 3441, loss 0.035393, acc 0.992188\n",
      "2017-01-10T22:40:57.854440: step 3442, loss 0.235699, acc 0.929688\n",
      "2017-01-10T22:40:59.945265: step 3443, loss 0.104889, acc 0.984375\n",
      "2017-01-10T22:41:02.362673: step 3444, loss 0.0919144, acc 0.984375\n",
      "2017-01-10T22:41:05.141245: step 3445, loss 0.145482, acc 0.960938\n",
      "2017-01-10T22:41:07.192947: step 3446, loss 0.151825, acc 0.960938\n",
      "2017-01-10T22:41:09.278254: step 3447, loss 0.113047, acc 0.960938\n",
      "2017-01-10T22:41:11.361839: step 3448, loss 0.0415222, acc 0.976562\n",
      "2017-01-10T22:41:13.426550: step 3449, loss 0.272444, acc 0.921875\n",
      "2017-01-10T22:41:15.525405: step 3450, loss 0.162468, acc 0.960938\n",
      "2017-01-10T22:41:17.624000: step 3451, loss 0.0856532, acc 0.984375\n",
      "2017-01-10T22:41:19.724337: step 3452, loss 0.0566608, acc 0.984375\n",
      "2017-01-10T22:41:21.865301: step 3453, loss 0.0902602, acc 0.976562\n",
      "2017-01-10T22:41:23.935849: step 3454, loss 0.093284, acc 0.976562\n",
      "2017-01-10T22:41:26.020395: step 3455, loss 0.026617, acc 0.992188\n",
      "2017-01-10T22:41:28.410613: step 3456, loss 0.0454845, acc 0.992188\n",
      "2017-01-10T22:41:30.502407: step 3457, loss 0.110679, acc 0.96875\n",
      "2017-01-10T22:41:32.555143: step 3458, loss 0.050352, acc 0.984375\n",
      "2017-01-10T22:41:34.652050: step 3459, loss 0.106841, acc 0.976562\n",
      "2017-01-10T22:41:36.735578: step 3460, loss 0.135621, acc 0.960938\n",
      "2017-01-10T22:41:38.781233: step 3461, loss 0.0693383, acc 0.984375\n",
      "2017-01-10T22:41:40.868289: step 3462, loss 0.0676356, acc 0.976562\n",
      "2017-01-10T22:41:42.955863: step 3463, loss 0.0904102, acc 0.976562\n",
      "2017-01-10T22:41:45.050226: step 3464, loss 0.0681635, acc 0.984375\n",
      "2017-01-10T22:41:47.332902: step 3465, loss 0.0219933, acc 0.992188\n",
      "2017-01-10T22:41:49.393986: step 3466, loss 0.0864977, acc 0.960938\n",
      "2017-01-10T22:41:51.520574: step 3467, loss 0.0579025, acc 0.984375\n",
      "2017-01-10T22:41:53.595421: step 3468, loss 0.151208, acc 0.960938\n",
      "2017-01-10T22:41:55.673028: step 3469, loss 0.125043, acc 0.976562\n",
      "2017-01-10T22:41:57.793157: step 3470, loss 0.0729501, acc 0.96875\n",
      "2017-01-10T22:42:00.039768: step 3471, loss 0.120302, acc 0.96875\n",
      "2017-01-10T22:42:02.705114: step 3472, loss 0.0569592, acc 0.984375\n",
      "2017-01-10T22:42:05.189525: step 3473, loss 0.0849115, acc 0.96875\n",
      "2017-01-10T22:42:07.281500: step 3474, loss 0.135349, acc 0.96875\n",
      "2017-01-10T22:42:09.364634: step 3475, loss 0.0539326, acc 0.976562\n",
      "2017-01-10T22:42:11.417283: step 3476, loss 0.131831, acc 0.976562\n",
      "2017-01-10T22:42:13.498940: step 3477, loss 0.110284, acc 0.960938\n",
      "2017-01-10T22:42:15.605101: step 3478, loss 0.161017, acc 0.953125\n",
      "2017-01-10T22:42:17.696067: step 3479, loss 0.10559, acc 0.984375\n",
      "2017-01-10T22:42:19.802730: step 3480, loss 0.0736589, acc 0.960938\n",
      "2017-01-10T22:42:21.901735: step 3481, loss 0.12865, acc 0.953125\n",
      "2017-01-10T22:42:23.996091: step 3482, loss 0.0466241, acc 0.992188\n",
      "2017-01-10T22:42:26.055978: step 3483, loss 0.118446, acc 0.960938\n",
      "2017-01-10T22:42:28.141724: step 3484, loss 0.108323, acc 0.976562\n",
      "2017-01-10T22:42:30.339191: step 3485, loss 0.0613829, acc 0.984375\n",
      "2017-01-10T22:42:32.696332: step 3486, loss 0.120169, acc 0.976562\n",
      "2017-01-10T22:42:34.809290: step 3487, loss 0.11268, acc 0.96875\n",
      "2017-01-10T22:42:36.844554: step 3488, loss 0.192265, acc 0.976562\n",
      "2017-01-10T22:42:38.916959: step 3489, loss 0.112664, acc 0.96875\n",
      "2017-01-10T22:42:41.003177: step 3490, loss 0.0821564, acc 0.976562\n",
      "2017-01-10T22:42:43.100014: step 3491, loss 0.0999416, acc 0.96875\n",
      "2017-01-10T22:42:45.148867: step 3492, loss 0.125077, acc 0.976562\n",
      "2017-01-10T22:42:47.249729: step 3493, loss 0.310106, acc 0.929688\n",
      "2017-01-10T22:42:49.333180: step 3494, loss 0.0383968, acc 0.992188\n",
      "2017-01-10T22:42:51.447588: step 3495, loss 0.104073, acc 0.96875\n",
      "2017-01-10T22:42:53.537231: step 3496, loss 0.0728397, acc 0.96875\n",
      "2017-01-10T22:42:55.613656: step 3497, loss 0.0903554, acc 0.984375\n",
      "2017-01-10T22:42:57.656023: step 3498, loss 0.052532, acc 0.976562\n",
      "2017-01-10T22:42:59.730387: step 3499, loss 0.11229, acc 0.976562\n",
      "2017-01-10T22:43:01.764052: step 3500, loss 0.0156868, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:43:26.275094: step 3500, loss 0.0812881, acc 0.97608\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3500\n",
      "\n",
      "2017-01-10T22:43:31.006538: step 3501, loss 0.0198702, acc 1\n",
      "2017-01-10T22:43:33.064192: step 3502, loss 0.0456139, acc 0.992188\n",
      "2017-01-10T22:43:35.187094: step 3503, loss 0.0885656, acc 0.984375\n",
      "2017-01-10T22:43:37.575081: step 3504, loss 0.0911028, acc 0.976562\n",
      "2017-01-10T22:43:39.660734: step 3505, loss 0.111414, acc 0.976562\n",
      "2017-01-10T22:43:41.785933: step 3506, loss 0.144963, acc 0.960938\n",
      "2017-01-10T22:43:43.875197: step 3507, loss 0.102704, acc 0.96875\n",
      "2017-01-10T22:43:45.972504: step 3508, loss 0.0934867, acc 0.96875\n",
      "2017-01-10T22:43:48.051657: step 3509, loss 0.0491955, acc 0.976562\n",
      "2017-01-10T22:43:50.193843: step 3510, loss 0.0418204, acc 0.984375\n",
      "2017-01-10T22:43:52.323541: step 3511, loss 0.0892171, acc 0.976562\n",
      "2017-01-10T22:43:54.414075: step 3512, loss 0.130923, acc 0.976562\n",
      "2017-01-10T22:43:56.499528: step 3513, loss 0.0641729, acc 0.976562\n",
      "2017-01-10T22:43:58.586110: step 3514, loss 0.117194, acc 0.960938\n",
      "2017-01-10T22:44:00.655966: step 3515, loss 0.0540608, acc 0.984375\n",
      "2017-01-10T22:44:02.690270: step 3516, loss 0.221716, acc 0.932692\n",
      "2017-01-10T22:44:04.946224: step 3517, loss 0.0470078, acc 0.992188\n",
      "2017-01-10T22:44:07.014141: step 3518, loss 0.0341595, acc 0.984375\n",
      "2017-01-10T22:44:09.216230: step 3519, loss 0.0192495, acc 1\n",
      "2017-01-10T22:44:11.360605: step 3520, loss 0.0472068, acc 0.984375\n",
      "2017-01-10T22:44:13.418767: step 3521, loss 0.16459, acc 0.953125\n",
      "2017-01-10T22:44:15.520881: step 3522, loss 0.138583, acc 0.96875\n",
      "2017-01-10T22:44:17.601486: step 3523, loss 0.0909936, acc 0.96875\n",
      "2017-01-10T22:44:19.668829: step 3524, loss 0.0652356, acc 0.992188\n",
      "2017-01-10T22:44:21.790253: step 3525, loss 0.0452161, acc 0.992188\n",
      "2017-01-10T22:44:23.880225: step 3526, loss 0.0338876, acc 0.992188\n",
      "2017-01-10T22:44:25.933534: step 3527, loss 0.0178655, acc 0.992188\n",
      "2017-01-10T22:44:28.140259: step 3528, loss 0.0368926, acc 0.992188\n",
      "2017-01-10T22:44:30.257104: step 3529, loss 0.0376907, acc 0.992188\n",
      "2017-01-10T22:44:32.330826: step 3530, loss 0.0565366, acc 0.976562\n",
      "2017-01-10T22:44:34.416755: step 3531, loss 0.0725434, acc 0.976562\n",
      "2017-01-10T22:44:36.457590: step 3532, loss 0.168005, acc 0.984375\n",
      "2017-01-10T22:44:38.510658: step 3533, loss 0.079736, acc 0.976562\n",
      "2017-01-10T22:44:40.580046: step 3534, loss 0.101868, acc 0.96875\n",
      "2017-01-10T22:44:42.825499: step 3535, loss 0.0306541, acc 0.984375\n",
      "2017-01-10T22:44:44.907629: step 3536, loss 0.0498175, acc 0.976562\n",
      "2017-01-10T22:44:46.979893: step 3537, loss 0.137485, acc 0.960938\n",
      "2017-01-10T22:44:49.062667: step 3538, loss 0.063561, acc 0.984375\n",
      "2017-01-10T22:44:51.177870: step 3539, loss 0.0877324, acc 0.945312\n",
      "2017-01-10T22:44:53.457691: step 3540, loss 0.0364081, acc 0.984375\n",
      "2017-01-10T22:44:55.525238: step 3541, loss 0.102942, acc 0.976562\n",
      "2017-01-10T22:44:57.606576: step 3542, loss 0.0660676, acc 0.984375\n",
      "2017-01-10T22:44:59.661698: step 3543, loss 0.061006, acc 0.984375\n",
      "2017-01-10T22:45:01.785375: step 3544, loss 0.0414061, acc 0.992188\n",
      "2017-01-10T22:45:03.925757: step 3545, loss 0.113203, acc 0.96875\n",
      "2017-01-10T22:45:06.066279: step 3546, loss 0.0537127, acc 0.984375\n",
      "2017-01-10T22:45:08.772130: step 3547, loss 0.0659084, acc 0.984375\n",
      "2017-01-10T22:45:11.189526: step 3548, loss 0.0291076, acc 0.992188\n",
      "2017-01-10T22:45:13.368750: step 3549, loss 0.0111086, acc 1\n",
      "2017-01-10T22:45:15.576234: step 3550, loss 0.0100595, acc 1\n",
      "2017-01-10T22:45:17.657661: step 3551, loss 0.020865, acc 0.992188\n",
      "2017-01-10T22:45:19.807120: step 3552, loss 0.0223988, acc 0.984375\n",
      "2017-01-10T22:45:21.919889: step 3553, loss 0.0429505, acc 0.992188\n",
      "2017-01-10T22:45:24.012252: step 3554, loss 0.110031, acc 0.96875\n",
      "2017-01-10T22:45:26.083774: step 3555, loss 0.00905776, acc 1\n",
      "2017-01-10T22:45:28.255231: step 3556, loss 0.0561797, acc 0.984375\n",
      "2017-01-10T22:45:30.339224: step 3557, loss 0.0506943, acc 0.992188\n",
      "2017-01-10T22:45:32.398997: step 3558, loss 0.0371713, acc 0.992188\n",
      "2017-01-10T22:45:34.465389: step 3559, loss 0.0496348, acc 0.984375\n",
      "2017-01-10T22:45:36.559251: step 3560, loss 0.0933697, acc 0.960938\n",
      "2017-01-10T22:45:39.232887: step 3561, loss 0.0406173, acc 0.984375\n",
      "2017-01-10T22:45:41.543429: step 3562, loss 0.0534183, acc 0.992188\n",
      "2017-01-10T22:45:44.049093: step 3563, loss 0.0262177, acc 0.992188\n",
      "2017-01-10T22:45:46.915488: step 3564, loss 0.0797647, acc 0.976562\n",
      "2017-01-10T22:45:49.360171: step 3565, loss 0.0389681, acc 0.984375\n",
      "2017-01-10T22:45:52.191969: step 3566, loss 0.0629934, acc 0.976562\n",
      "2017-01-10T22:45:56.065371: step 3567, loss 0.0606, acc 0.984375\n",
      "2017-01-10T22:45:58.789221: step 3568, loss 0.0568081, acc 0.992188\n",
      "2017-01-10T22:46:01.356020: step 3569, loss 0.0449383, acc 0.984375\n",
      "2017-01-10T22:46:03.879600: step 3570, loss 0.0452782, acc 0.992188\n",
      "2017-01-10T22:46:07.274273: step 3571, loss 0.0395163, acc 0.992188\n",
      "2017-01-10T22:46:10.361586: step 3572, loss 0.045668, acc 0.984375\n",
      "2017-01-10T22:46:12.507454: step 3573, loss 0.077348, acc 0.976562\n",
      "2017-01-10T22:46:14.604899: step 3574, loss 0.0824095, acc 0.976562\n",
      "2017-01-10T22:46:16.629928: step 3575, loss 0.0860147, acc 0.976562\n",
      "2017-01-10T22:46:18.877936: step 3576, loss 0.0497248, acc 0.984375\n",
      "2017-01-10T22:46:20.957311: step 3577, loss 0.00655207, acc 1\n",
      "2017-01-10T22:46:22.979263: step 3578, loss 0.117592, acc 0.945312\n",
      "2017-01-10T22:46:24.984718: step 3579, loss 0.0425008, acc 0.992188\n",
      "2017-01-10T22:46:27.037521: step 3580, loss 0.0710252, acc 0.96875\n",
      "2017-01-10T22:46:29.072794: step 3581, loss 0.0124583, acc 1\n",
      "2017-01-10T22:46:31.205289: step 3582, loss 0.0617037, acc 0.976562\n",
      "2017-01-10T22:46:33.227182: step 3583, loss 0.0168714, acc 0.992188\n",
      "2017-01-10T22:46:35.338787: step 3584, loss 0.0511954, acc 0.992188\n",
      "2017-01-10T22:46:37.402101: step 3585, loss 0.108863, acc 0.96875\n",
      "2017-01-10T22:46:39.401675: step 3586, loss 0.109996, acc 0.953125\n",
      "2017-01-10T22:46:41.470198: step 3587, loss 0.0953067, acc 0.96875\n",
      "2017-01-10T22:46:43.514745: step 3588, loss 0.0976388, acc 0.96875\n",
      "2017-01-10T22:46:45.536924: step 3589, loss 0.0418252, acc 0.992188\n",
      "2017-01-10T22:46:47.584680: step 3590, loss 0.0400973, acc 0.992188\n",
      "2017-01-10T22:46:49.694223: step 3591, loss 0.0803939, acc 0.96875\n",
      "2017-01-10T22:46:51.985009: step 3592, loss 0.0631683, acc 0.984375\n",
      "2017-01-10T22:46:54.004026: step 3593, loss 0.101104, acc 0.984375\n",
      "2017-01-10T22:46:56.059365: step 3594, loss 0.0900952, acc 0.976562\n",
      "2017-01-10T22:46:58.126928: step 3595, loss 0.0287172, acc 0.984375\n",
      "2017-01-10T22:47:00.171015: step 3596, loss 0.0838335, acc 0.976562\n",
      "2017-01-10T22:47:02.178241: step 3597, loss 0.0331471, acc 0.984375\n",
      "2017-01-10T22:47:04.210095: step 3598, loss 0.071169, acc 0.984375\n",
      "2017-01-10T22:47:06.265010: step 3599, loss 0.0316974, acc 0.992188\n",
      "2017-01-10T22:47:08.305783: step 3600, loss 0.00484275, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:47:36.459338: step 3600, loss 0.0804061, acc 0.9766\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3600\n",
      "\n",
      "2017-01-10T22:47:40.954571: step 3601, loss 0.0528586, acc 0.976562\n",
      "2017-01-10T22:47:42.975536: step 3602, loss 0.080413, acc 0.976562\n",
      "2017-01-10T22:47:45.013170: step 3603, loss 0.0597877, acc 0.992188\n",
      "2017-01-10T22:47:47.082034: step 3604, loss 0.0533247, acc 0.984375\n",
      "2017-01-10T22:47:49.122705: step 3605, loss 0.0927612, acc 0.960938\n",
      "2017-01-10T22:47:51.186741: step 3606, loss 0.110085, acc 0.953125\n",
      "2017-01-10T22:47:53.240064: step 3607, loss 0.0186384, acc 0.992188\n",
      "2017-01-10T22:47:55.305324: step 3608, loss 0.0859652, acc 0.984375\n",
      "2017-01-10T22:47:57.336898: step 3609, loss 0.0729464, acc 0.976562\n",
      "2017-01-10T22:47:59.612274: step 3610, loss 0.0721187, acc 0.976562\n",
      "2017-01-10T22:48:01.670456: step 3611, loss 0.0474956, acc 0.976562\n",
      "2017-01-10T22:48:03.750017: step 3612, loss 0.0908693, acc 0.960938\n",
      "2017-01-10T22:48:05.852066: step 3613, loss 0.0614308, acc 0.976562\n",
      "2017-01-10T22:48:07.879416: step 3614, loss 0.0835172, acc 0.96875\n",
      "2017-01-10T22:48:09.902338: step 3615, loss 0.136046, acc 0.984375\n",
      "2017-01-10T22:48:11.931163: step 3616, loss 0.0369978, acc 0.992188\n",
      "2017-01-10T22:48:13.996933: step 3617, loss 0.0770137, acc 0.976562\n",
      "2017-01-10T22:48:16.067874: step 3618, loss 0.0657448, acc 0.992188\n",
      "2017-01-10T22:48:18.080992: step 3619, loss 0.0160739, acc 1\n",
      "2017-01-10T22:48:20.178341: step 3620, loss 0.0261037, acc 0.992188\n",
      "2017-01-10T22:48:22.247714: step 3621, loss 0.0580439, acc 0.984375\n",
      "2017-01-10T22:48:24.287234: step 3622, loss 0.0909016, acc 0.976562\n",
      "2017-01-10T22:48:26.296265: step 3623, loss 0.00187209, acc 1\n",
      "2017-01-10T22:48:28.332559: step 3624, loss 0.0661147, acc 0.992188\n",
      "2017-01-10T22:48:30.658682: step 3625, loss 0.129894, acc 0.960938\n",
      "2017-01-10T22:48:32.713435: step 3626, loss 0.0198257, acc 0.992188\n",
      "2017-01-10T22:48:34.809710: step 3627, loss 0.0584291, acc 0.984375\n",
      "2017-01-10T22:48:36.865649: step 3628, loss 0.0141252, acc 1\n",
      "2017-01-10T22:48:38.913725: step 3629, loss 0.0340286, acc 0.992188\n",
      "2017-01-10T22:48:40.929825: step 3630, loss 0.0393676, acc 0.984375\n",
      "2017-01-10T22:48:42.969848: step 3631, loss 0.0376758, acc 0.976562\n",
      "2017-01-10T22:48:45.045987: step 3632, loss 0.0159974, acc 0.992188\n",
      "2017-01-10T22:48:47.069927: step 3633, loss 0.0602377, acc 0.984375\n",
      "2017-01-10T22:48:49.090526: step 3634, loss 0.0538036, acc 0.984375\n",
      "2017-01-10T22:48:51.122118: step 3635, loss 0.10547, acc 0.953125\n",
      "2017-01-10T22:48:53.157057: step 3636, loss 0.0383565, acc 0.984375\n",
      "2017-01-10T22:48:55.187885: step 3637, loss 0.0596092, acc 0.976562\n",
      "2017-01-10T22:48:57.213408: step 3638, loss 0.0663516, acc 0.976562\n",
      "2017-01-10T22:48:59.459537: step 3639, loss 0.158546, acc 0.953125\n",
      "2017-01-10T22:49:01.537181: step 3640, loss 0.0135847, acc 1\n",
      "2017-01-10T22:49:03.809767: step 3641, loss 0.0277913, acc 0.992188\n",
      "2017-01-10T22:49:05.890453: step 3642, loss 0.0206175, acc 1\n",
      "2017-01-10T22:49:07.950645: step 3643, loss 0.0844166, acc 0.960938\n",
      "2017-01-10T22:49:09.981721: step 3644, loss 0.059555, acc 0.984375\n",
      "2017-01-10T22:49:12.036958: step 3645, loss 0.0843039, acc 0.96875\n",
      "2017-01-10T22:49:14.095472: step 3646, loss 0.0720504, acc 0.984375\n",
      "2017-01-10T22:49:16.142311: step 3647, loss 0.0637553, acc 0.984375\n",
      "2017-01-10T22:49:18.200223: step 3648, loss 0.111988, acc 0.96875\n",
      "2017-01-10T22:49:20.266195: step 3649, loss 0.0726418, acc 0.96875\n",
      "2017-01-10T22:49:22.359709: step 3650, loss 0.0775161, acc 0.984375\n",
      "2017-01-10T22:49:24.406059: step 3651, loss 0.0604837, acc 0.984375\n",
      "2017-01-10T22:49:26.450730: step 3652, loss 0.0307282, acc 1\n",
      "2017-01-10T22:49:28.517502: step 3653, loss 0.0589665, acc 0.976562\n",
      "2017-01-10T22:49:30.615309: step 3654, loss 0.00659521, acc 1\n",
      "2017-01-10T22:49:32.662159: step 3655, loss 0.0863382, acc 0.976562\n",
      "2017-01-10T22:49:34.887647: step 3656, loss 0.0665072, acc 0.976562\n",
      "2017-01-10T22:49:36.875144: step 3657, loss 0.0441538, acc 0.976562\n",
      "2017-01-10T22:49:38.936124: step 3658, loss 0.0990496, acc 0.976562\n",
      "2017-01-10T22:49:40.962542: step 3659, loss 0.0912271, acc 0.984375\n",
      "2017-01-10T22:49:43.059663: step 3660, loss 0.0544683, acc 0.976562\n",
      "2017-01-10T22:49:45.077140: step 3661, loss 0.0789071, acc 0.96875\n",
      "2017-01-10T22:49:47.120724: step 3662, loss 0.101637, acc 0.976562\n",
      "2017-01-10T22:49:49.131308: step 3663, loss 0.0316104, acc 0.992188\n",
      "2017-01-10T22:49:51.202729: step 3664, loss 0.0417218, acc 1\n",
      "2017-01-10T22:49:53.254718: step 3665, loss 0.025744, acc 0.992188\n",
      "2017-01-10T22:49:55.261407: step 3666, loss 0.0137153, acc 1\n",
      "2017-01-10T22:49:57.300642: step 3667, loss 0.103681, acc 0.976562\n",
      "2017-01-10T22:49:59.555479: step 3668, loss 0.0410791, acc 0.984375\n",
      "2017-01-10T22:50:01.610449: step 3669, loss 0.0890977, acc 0.96875\n",
      "2017-01-10T22:50:03.698015: step 3670, loss 0.0543601, acc 0.984375\n",
      "2017-01-10T22:50:05.805786: step 3671, loss 0.00507995, acc 1\n",
      "2017-01-10T22:50:08.010168: step 3672, loss 0.223169, acc 0.945312\n",
      "2017-01-10T22:50:10.059380: step 3673, loss 0.0679217, acc 0.984375\n",
      "2017-01-10T22:50:12.102241: step 3674, loss 0.103194, acc 0.96875\n",
      "2017-01-10T22:50:14.549702: step 3675, loss 0.0692939, acc 0.976562\n",
      "2017-01-10T22:50:17.184210: step 3676, loss 0.136831, acc 0.976562\n",
      "2017-01-10T22:50:19.196881: step 3677, loss 0.0262391, acc 1\n",
      "2017-01-10T22:50:21.233994: step 3678, loss 0.0617395, acc 0.96875\n",
      "2017-01-10T22:50:23.276844: step 3679, loss 0.133257, acc 0.96875\n",
      "2017-01-10T22:50:25.308618: step 3680, loss 0.100228, acc 0.96875\n",
      "2017-01-10T22:50:27.385318: step 3681, loss 0.0421471, acc 0.984375\n",
      "2017-01-10T22:50:29.443396: step 3682, loss 0.0431537, acc 0.992188\n",
      "2017-01-10T22:50:31.521854: step 3683, loss 0.101851, acc 0.960938\n",
      "2017-01-10T22:50:33.576512: step 3684, loss 0.0648917, acc 0.96875\n",
      "2017-01-10T22:50:35.631663: step 3685, loss 0.0539836, acc 0.984375\n",
      "2017-01-10T22:50:37.689134: step 3686, loss 0.0874025, acc 0.976562\n",
      "2017-01-10T22:50:40.098157: step 3687, loss 0.0643763, acc 0.984375\n",
      "2017-01-10T22:50:42.167264: step 3688, loss 0.050883, acc 0.984375\n",
      "2017-01-10T22:50:44.162361: step 3689, loss 0.0418769, acc 0.992188\n",
      "2017-01-10T22:50:46.209531: step 3690, loss 0.114834, acc 0.960938\n",
      "2017-01-10T22:50:48.245496: step 3691, loss 0.0156917, acc 1\n",
      "2017-01-10T22:50:50.306453: step 3692, loss 0.126335, acc 0.945312\n",
      "2017-01-10T22:50:52.384753: step 3693, loss 0.0611546, acc 0.984375\n",
      "2017-01-10T22:50:54.447447: step 3694, loss 0.0653977, acc 0.976562\n",
      "2017-01-10T22:50:56.485543: step 3695, loss 0.0758611, acc 0.976562\n",
      "2017-01-10T22:50:58.553167: step 3696, loss 0.029696, acc 0.992188\n",
      "2017-01-10T22:51:00.770996: step 3697, loss 0.0429576, acc 0.984375\n",
      "2017-01-10T22:51:02.849922: step 3698, loss 0.0940915, acc 0.96875\n",
      "2017-01-10T22:51:05.455930: step 3699, loss 0.129891, acc 0.976562\n",
      "2017-01-10T22:51:07.558956: step 3700, loss 0.0515218, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:51:32.882281: step 3700, loss 0.0800741, acc 0.97628\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3700\n",
      "\n",
      "2017-01-10T22:51:37.503736: step 3701, loss 0.0526942, acc 0.976562\n",
      "2017-01-10T22:51:39.538120: step 3702, loss 0.0696497, acc 0.984375\n",
      "2017-01-10T22:51:41.599365: step 3703, loss 0.0263847, acc 0.992188\n",
      "2017-01-10T22:51:43.875964: step 3704, loss 0.0677879, acc 0.984375\n",
      "2017-01-10T22:51:46.107109: step 3705, loss 0.0908571, acc 0.96875\n",
      "2017-01-10T22:51:48.150321: step 3706, loss 0.034057, acc 0.992188\n",
      "2017-01-10T22:51:50.232597: step 3707, loss 0.0519307, acc 0.984375\n",
      "2017-01-10T22:51:52.296277: step 3708, loss 0.0808406, acc 0.976562\n",
      "2017-01-10T22:51:54.358222: step 3709, loss 0.145772, acc 0.953125\n",
      "2017-01-10T22:51:56.385134: step 3710, loss 0.0879047, acc 0.96875\n",
      "2017-01-10T22:51:58.418238: step 3711, loss 0.0472932, acc 0.984375\n",
      "2017-01-10T22:52:00.446926: step 3712, loss 0.117785, acc 0.960938\n",
      "2017-01-10T22:52:02.467892: step 3713, loss 0.0548836, acc 0.984375\n",
      "2017-01-10T22:52:04.506050: step 3714, loss 0.0229151, acc 1\n",
      "2017-01-10T22:52:06.529144: step 3715, loss 0.0546421, acc 0.976562\n",
      "2017-01-10T22:52:08.589440: step 3716, loss 0.0889334, acc 0.960938\n",
      "2017-01-10T22:52:10.624515: step 3717, loss 0.031983, acc 0.992188\n",
      "2017-01-10T22:52:12.663701: step 3718, loss 0.0766582, acc 0.976562\n",
      "2017-01-10T22:52:14.697057: step 3719, loss 0.0558686, acc 0.976562\n",
      "2017-01-10T22:52:16.884297: step 3720, loss 0.0526897, acc 0.984375\n",
      "2017-01-10T22:52:18.923601: step 3721, loss 0.0412217, acc 0.992188\n",
      "2017-01-10T22:52:21.025656: step 3722, loss 0.0736397, acc 0.976562\n",
      "2017-01-10T22:52:23.042655: step 3723, loss 0.100278, acc 0.960938\n",
      "2017-01-10T22:52:25.081752: step 3724, loss 0.266146, acc 0.960938\n",
      "2017-01-10T22:52:27.114220: step 3725, loss 0.0602418, acc 0.976562\n",
      "2017-01-10T22:52:29.156692: step 3726, loss 0.00721358, acc 1\n",
      "2017-01-10T22:52:31.424361: step 3727, loss 0.123485, acc 0.960938\n",
      "2017-01-10T22:52:33.444761: step 3728, loss 0.0405861, acc 0.992188\n",
      "2017-01-10T22:52:35.506927: step 3729, loss 0.0670885, acc 0.984375\n",
      "2017-01-10T22:52:37.533128: step 3730, loss 0.0562827, acc 0.976562\n",
      "2017-01-10T22:52:39.571299: step 3731, loss 0.0501467, acc 0.984375\n",
      "2017-01-10T22:52:41.638377: step 3732, loss 0.0736474, acc 0.984375\n",
      "2017-01-10T22:52:43.692890: step 3733, loss 0.0300565, acc 0.984375\n",
      "2017-01-10T22:52:45.761514: step 3734, loss 0.094739, acc 0.96875\n",
      "2017-01-10T22:52:47.803686: step 3735, loss 0.0806435, acc 0.96875\n",
      "2017-01-10T22:52:50.059005: step 3736, loss 0.083223, acc 0.96875\n",
      "2017-01-10T22:52:52.131061: step 3737, loss 0.0414345, acc 0.984375\n",
      "2017-01-10T22:52:54.191032: step 3738, loss 0.0178823, acc 1\n",
      "2017-01-10T22:52:56.218290: step 3739, loss 0.0363501, acc 0.992188\n",
      "2017-01-10T22:52:58.233968: step 3740, loss 0.07977, acc 0.984375\n",
      "2017-01-10T22:53:00.288014: step 3741, loss 0.0881984, acc 0.984375\n",
      "2017-01-10T22:53:02.327266: step 3742, loss 0.0526883, acc 0.984375\n",
      "2017-01-10T22:53:04.391625: step 3743, loss 0.0551417, acc 0.976562\n",
      "2017-01-10T22:53:06.427576: step 3744, loss 0.00640605, acc 1\n",
      "2017-01-10T22:53:08.498244: step 3745, loss 0.0760701, acc 0.976562\n",
      "2017-01-10T22:53:10.560847: step 3746, loss 0.0101666, acc 1\n",
      "2017-01-10T22:53:12.641816: step 3747, loss 0.0276708, acc 0.992188\n",
      "2017-01-10T22:53:14.665913: step 3748, loss 0.0815586, acc 0.976562\n",
      "2017-01-10T22:53:16.700145: step 3749, loss 0.0502129, acc 0.984375\n",
      "2017-01-10T22:53:18.795692: step 3750, loss 0.0734437, acc 0.96875\n",
      "2017-01-10T22:53:21.115319: step 3751, loss 0.0276311, acc 1\n",
      "2017-01-10T22:53:23.142686: step 3752, loss 0.048762, acc 0.984375\n",
      "2017-01-10T22:53:25.187782: step 3753, loss 0.0733335, acc 0.976562\n",
      "2017-01-10T22:53:27.246223: step 3754, loss 0.0722406, acc 0.976562\n",
      "2017-01-10T22:53:29.308351: step 3755, loss 0.0272007, acc 0.992188\n",
      "2017-01-10T22:53:31.417088: step 3756, loss 0.0850524, acc 0.976562\n",
      "2017-01-10T22:53:33.453132: step 3757, loss 0.0461111, acc 0.984375\n",
      "2017-01-10T22:53:35.502742: step 3758, loss 0.0474981, acc 0.992188\n",
      "2017-01-10T22:53:37.536385: step 3759, loss 0.0641449, acc 0.984375\n",
      "2017-01-10T22:53:39.581014: step 3760, loss 0.146072, acc 0.9375\n",
      "2017-01-10T22:53:41.602603: step 3761, loss 0.0603798, acc 0.984375\n",
      "2017-01-10T22:53:43.655573: step 3762, loss 0.100692, acc 0.960938\n",
      "2017-01-10T22:53:45.708271: step 3763, loss 0.0687025, acc 0.984375\n",
      "2017-01-10T22:53:47.758920: step 3764, loss 0.0887716, acc 0.976562\n",
      "2017-01-10T22:53:49.827195: step 3765, loss 0.0202379, acc 1\n",
      "2017-01-10T22:53:51.944545: step 3766, loss 0.123622, acc 0.96875\n",
      "2017-01-10T22:53:54.167479: step 3767, loss 0.116791, acc 0.96875\n",
      "2017-01-10T22:53:56.198698: step 3768, loss 0.0979151, acc 0.96875\n",
      "2017-01-10T22:53:58.240365: step 3769, loss 0.0629451, acc 0.96875\n",
      "2017-01-10T22:54:00.279762: step 3770, loss 0.0107813, acc 1\n",
      "2017-01-10T22:54:02.307746: step 3771, loss 0.101159, acc 0.976562\n",
      "2017-01-10T22:54:04.352558: step 3772, loss 0.192801, acc 0.953125\n",
      "2017-01-10T22:54:06.361473: step 3773, loss 0.0523772, acc 0.992188\n",
      "2017-01-10T22:54:08.409854: step 3774, loss 0.0482891, acc 0.992188\n",
      "2017-01-10T22:54:10.452835: step 3775, loss 0.0875072, acc 0.976562\n",
      "2017-01-10T22:54:12.498343: step 3776, loss 0.105557, acc 0.96875\n",
      "2017-01-10T22:54:14.512763: step 3777, loss 0.0951661, acc 0.96875\n",
      "2017-01-10T22:54:16.574806: step 3778, loss 0.0748242, acc 0.976562\n",
      "2017-01-10T22:54:18.591630: step 3779, loss 0.0530067, acc 0.976562\n",
      "2017-01-10T22:54:20.715475: step 3780, loss 0.0610259, acc 0.976562\n",
      "2017-01-10T22:54:22.833582: step 3781, loss 0.0722081, acc 0.976562\n",
      "2017-01-10T22:54:25.094150: step 3782, loss 0.110349, acc 0.96875\n",
      "2017-01-10T22:54:27.125764: step 3783, loss 0.030623, acc 0.992188\n",
      "2017-01-10T22:54:29.166892: step 3784, loss 0.0780557, acc 0.96875\n",
      "2017-01-10T22:54:31.460731: step 3785, loss 0.0467814, acc 0.992188\n",
      "2017-01-10T22:54:33.510498: step 3786, loss 0.10229, acc 0.96875\n",
      "2017-01-10T22:54:35.566612: step 3787, loss 0.0997434, acc 0.96875\n",
      "2017-01-10T22:54:37.607182: step 3788, loss 0.0935315, acc 0.960938\n",
      "2017-01-10T22:54:39.642976: step 3789, loss 0.0705012, acc 0.976562\n",
      "2017-01-10T22:54:41.705287: step 3790, loss 0.0656335, acc 0.96875\n",
      "2017-01-10T22:54:43.750800: step 3791, loss 0.091839, acc 0.96875\n",
      "2017-01-10T22:54:45.784644: step 3792, loss 0.120809, acc 0.960938\n",
      "2017-01-10T22:54:48.078559: step 3793, loss 0.0337959, acc 0.992188\n",
      "2017-01-10T22:54:50.439269: step 3794, loss 0.105467, acc 0.953125\n",
      "2017-01-10T22:54:52.664553: step 3795, loss 0.04727, acc 0.976562\n",
      "2017-01-10T22:54:54.880988: step 3796, loss 0.0653022, acc 0.96875\n",
      "2017-01-10T22:54:57.230283: step 3797, loss 0.0820083, acc 0.984375\n",
      "2017-01-10T22:54:59.281892: step 3798, loss 0.0473057, acc 0.984375\n",
      "2017-01-10T22:55:01.310779: step 3799, loss 0.0586806, acc 0.984375\n",
      "2017-01-10T22:55:03.353718: step 3800, loss 0.0766136, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:55:26.840823: step 3800, loss 0.0790618, acc 0.97756\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3800\n",
      "\n",
      "2017-01-10T22:55:31.916223: step 3801, loss 0.0945267, acc 0.960938\n",
      "2017-01-10T22:55:33.963466: step 3802, loss 0.0750211, acc 0.984375\n",
      "2017-01-10T22:55:36.009183: step 3803, loss 0.0442708, acc 0.992188\n",
      "2017-01-10T22:55:38.086493: step 3804, loss 0.0794493, acc 0.984375\n",
      "2017-01-10T22:55:40.156266: step 3805, loss 0.0546864, acc 0.984375\n",
      "2017-01-10T22:55:42.184407: step 3806, loss 0.0545873, acc 0.984375\n",
      "2017-01-10T22:55:44.209986: step 3807, loss 0.0753619, acc 0.976562\n",
      "2017-01-10T22:55:46.268298: step 3808, loss 0.0350793, acc 0.992188\n",
      "2017-01-10T22:55:48.313527: step 3809, loss 0.244053, acc 0.960938\n",
      "2017-01-10T22:55:50.377586: step 3810, loss 0.0470883, acc 0.992188\n",
      "2017-01-10T22:55:52.461872: step 3811, loss 0.0443499, acc 0.984375\n",
      "2017-01-10T22:55:54.485462: step 3812, loss 0.0638768, acc 0.976562\n",
      "2017-01-10T22:55:56.533461: step 3813, loss 0.00777389, acc 1\n",
      "2017-01-10T22:55:58.588434: step 3814, loss 0.136161, acc 0.953125\n",
      "2017-01-10T22:56:00.641564: step 3815, loss 0.0567906, acc 0.976562\n",
      "2017-01-10T22:56:02.791681: step 3816, loss 0.10445, acc 0.976562\n",
      "2017-01-10T22:56:05.124010: step 3817, loss 0.0859536, acc 0.984375\n",
      "2017-01-10T22:56:07.189956: step 3818, loss 0.0107454, acc 1\n",
      "2017-01-10T22:56:09.208566: step 3819, loss 0.0230107, acc 0.992188\n",
      "2017-01-10T22:56:11.250671: step 3820, loss 0.0204047, acc 0.992188\n",
      "2017-01-10T22:56:13.298952: step 3821, loss 0.0996371, acc 0.960938\n",
      "2017-01-10T22:56:15.343537: step 3822, loss 0.00904487, acc 1\n",
      "2017-01-10T22:56:17.407898: step 3823, loss 0.031562, acc 0.992188\n",
      "2017-01-10T22:56:19.461078: step 3824, loss 0.0280507, acc 1\n",
      "2017-01-10T22:56:22.429317: step 3825, loss 0.145286, acc 0.960938\n",
      "2017-01-10T22:56:24.646595: step 3826, loss 0.0154199, acc 1\n",
      "2017-01-10T22:56:26.671487: step 3827, loss 0.0612899, acc 0.984375\n",
      "2017-01-10T22:56:28.727263: step 3828, loss 0.115159, acc 0.960938\n",
      "2017-01-10T22:56:30.906482: step 3829, loss 0.0765834, acc 0.976562\n",
      "2017-01-10T22:56:33.057901: step 3830, loss 0.0817313, acc 0.984375\n",
      "2017-01-10T22:56:35.427990: step 3831, loss 0.0186021, acc 1\n",
      "2017-01-10T22:56:37.462641: step 3832, loss 0.119777, acc 0.945312\n",
      "2017-01-10T22:56:39.513406: step 3833, loss 0.0194752, acc 1\n",
      "2017-01-10T22:56:41.564284: step 3834, loss 0.116592, acc 0.960938\n",
      "2017-01-10T22:56:43.611439: step 3835, loss 0.0747142, acc 0.992188\n",
      "2017-01-10T22:56:45.647835: step 3836, loss 0.0879813, acc 0.976562\n",
      "2017-01-10T22:56:47.734105: step 3837, loss 0.0236034, acc 0.992188\n",
      "2017-01-10T22:56:49.816884: step 3838, loss 0.0697827, acc 0.976562\n",
      "2017-01-10T22:56:51.900992: step 3839, loss 0.0701803, acc 0.976562\n",
      "2017-01-10T22:56:53.977660: step 3840, loss 0.0346149, acc 0.992188\n",
      "2017-01-10T22:56:56.037942: step 3841, loss 0.12863, acc 0.960938\n",
      "2017-01-10T22:56:58.078476: step 3842, loss 0.0345167, acc 0.984375\n",
      "2017-01-10T22:57:00.083926: step 3843, loss 0.037534, acc 0.984375\n",
      "2017-01-10T22:57:02.124971: step 3844, loss 0.0465583, acc 0.984375\n",
      "2017-01-10T22:57:04.223906: step 3845, loss 0.121713, acc 0.960938\n",
      "2017-01-10T22:57:06.372561: step 3846, loss 0.0720083, acc 0.976562\n",
      "2017-01-10T22:57:08.499623: step 3847, loss 0.105618, acc 0.953125\n",
      "2017-01-10T22:57:10.513404: step 3848, loss 0.0825838, acc 0.96875\n",
      "2017-01-10T22:57:12.535441: step 3849, loss 0.148034, acc 0.945312\n",
      "2017-01-10T22:57:14.580606: step 3850, loss 0.0326738, acc 0.992188\n",
      "2017-01-10T22:57:16.633908: step 3851, loss 0.0167489, acc 0.992188\n",
      "2017-01-10T22:57:18.635937: step 3852, loss 0.0798314, acc 0.96875\n",
      "2017-01-10T22:57:21.638190: step 3853, loss 0.0850741, acc 0.976562\n",
      "2017-01-10T22:57:23.824763: step 3854, loss 0.0607805, acc 0.96875\n",
      "2017-01-10T22:57:25.820999: step 3855, loss 0.181088, acc 0.953125\n",
      "2017-01-10T22:57:27.871375: step 3856, loss 0.0892572, acc 0.960938\n",
      "2017-01-10T22:57:29.941427: step 3857, loss 0.0935098, acc 0.976562\n",
      "2017-01-10T22:57:31.986677: step 3858, loss 0.0270333, acc 0.992188\n",
      "2017-01-10T22:57:34.041449: step 3859, loss 0.0463, acc 0.992188\n",
      "2017-01-10T22:57:36.128337: step 3860, loss 0.0708157, acc 0.976562\n",
      "2017-01-10T22:57:38.303455: step 3861, loss 0.0467557, acc 0.976562\n",
      "2017-01-10T22:57:40.353388: step 3862, loss 0.0745675, acc 0.984375\n",
      "2017-01-10T22:57:42.418610: step 3863, loss 0.0145835, acc 0.992188\n",
      "2017-01-10T22:57:44.435637: step 3864, loss 0.0930357, acc 0.96875\n",
      "2017-01-10T22:57:46.442514: step 3865, loss 0.0493193, acc 0.992188\n",
      "2017-01-10T22:57:48.491905: step 3866, loss 0.0371785, acc 0.992188\n",
      "2017-01-10T22:57:50.549081: step 3867, loss 0.112481, acc 0.976562\n",
      "2017-01-10T22:57:52.609049: step 3868, loss 0.0760756, acc 0.984375\n",
      "2017-01-10T22:57:54.651901: step 3869, loss 0.0273473, acc 1\n",
      "2017-01-10T22:57:56.694571: step 3870, loss 0.079758, acc 0.96875\n",
      "2017-01-10T22:57:58.726862: step 3871, loss 0.0919691, acc 0.976562\n",
      "2017-01-10T22:58:00.741201: step 3872, loss 0.0977604, acc 0.976562\n",
      "2017-01-10T22:58:02.792619: step 3873, loss 0.108555, acc 0.960938\n",
      "2017-01-10T22:58:04.880050: step 3874, loss 0.0581358, acc 0.984375\n",
      "2017-01-10T22:58:06.936958: step 3875, loss 0.0609658, acc 0.984375\n",
      "2017-01-10T22:58:08.987482: step 3876, loss 0.053341, acc 0.976562\n",
      "2017-01-10T22:58:11.254644: step 3877, loss 0.0916475, acc 0.976562\n",
      "2017-01-10T22:58:13.272876: step 3878, loss 0.0488169, acc 0.992188\n",
      "2017-01-10T22:58:15.270531: step 3879, loss 0.0308687, acc 0.992188\n",
      "2017-01-10T22:58:17.296635: step 3880, loss 0.0350405, acc 0.992188\n",
      "2017-01-10T22:58:19.352780: step 3881, loss 0.0601997, acc 0.984375\n",
      "2017-01-10T22:58:21.432938: step 3882, loss 0.0878488, acc 0.96875\n",
      "2017-01-10T22:58:23.449116: step 3883, loss 0.00398801, acc 1\n",
      "2017-01-10T22:58:25.490945: step 3884, loss 0.0530448, acc 0.984375\n",
      "2017-01-10T22:58:27.545898: step 3885, loss 0.0684349, acc 0.984375\n",
      "2017-01-10T22:58:29.588975: step 3886, loss 0.0696797, acc 0.976562\n",
      "2017-01-10T22:58:31.640896: step 3887, loss 0.0144642, acc 1\n",
      "2017-01-10T22:58:33.765113: step 3888, loss 0.0318497, acc 0.992188\n",
      "2017-01-10T22:58:35.901356: step 3889, loss 0.271914, acc 0.96875\n",
      "2017-01-10T22:58:37.947794: step 3890, loss 0.108334, acc 0.96875\n",
      "2017-01-10T22:58:39.978805: step 3891, loss 0.0817362, acc 0.976562\n",
      "2017-01-10T22:58:42.152419: step 3892, loss 0.0194741, acc 1\n",
      "2017-01-10T22:58:44.192610: step 3893, loss 0.0336991, acc 0.984375\n",
      "2017-01-10T22:58:46.223277: step 3894, loss 0.0932918, acc 0.976562\n",
      "2017-01-10T22:58:48.265352: step 3895, loss 0.0673584, acc 0.992188\n",
      "2017-01-10T22:58:50.348498: step 3896, loss 0.086291, acc 0.976562\n",
      "2017-01-10T22:58:52.420658: step 3897, loss 0.118442, acc 0.96875\n",
      "2017-01-10T22:58:54.469905: step 3898, loss 0.0255093, acc 1\n",
      "2017-01-10T22:58:56.489993: step 3899, loss 0.0652911, acc 0.984375\n",
      "2017-01-10T22:58:58.541981: step 3900, loss 0.0166578, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T22:59:31.269495: step 3900, loss 0.0793528, acc 0.97668\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-3900\n",
      "\n",
      "2017-01-10T22:59:36.344786: step 3901, loss 0.0344969, acc 0.992188\n",
      "2017-01-10T22:59:38.423785: step 3902, loss 0.220825, acc 0.976562\n",
      "2017-01-10T22:59:40.447929: step 3903, loss 0.092466, acc 0.976562\n",
      "2017-01-10T22:59:42.458102: step 3904, loss 0.117353, acc 0.953125\n",
      "2017-01-10T22:59:44.518851: step 3905, loss 0.055347, acc 0.984375\n",
      "2017-01-10T22:59:46.588687: step 3906, loss 0.09885, acc 0.96875\n",
      "2017-01-10T22:59:48.641405: step 3907, loss 0.0665184, acc 0.976562\n",
      "2017-01-10T22:59:51.064767: step 3908, loss 0.0932984, acc 0.984375\n",
      "2017-01-10T22:59:53.094849: step 3909, loss 0.129673, acc 0.960938\n",
      "2017-01-10T22:59:55.157367: step 3910, loss 0.0272002, acc 0.992188\n",
      "2017-01-10T22:59:57.177664: step 3911, loss 0.0164087, acc 0.992188\n",
      "2017-01-10T22:59:59.247898: step 3912, loss 0.000930802, acc 1\n",
      "2017-01-10T23:00:01.275319: step 3913, loss 0.0146518, acc 0.992188\n",
      "2017-01-10T23:00:03.304250: step 3914, loss 0.118897, acc 0.96875\n",
      "2017-01-10T23:00:05.390655: step 3915, loss 0.0582551, acc 0.984375\n",
      "2017-01-10T23:00:07.452866: step 3916, loss 0.127576, acc 0.960938\n",
      "2017-01-10T23:00:09.493675: step 3917, loss 0.0510481, acc 0.992188\n",
      "2017-01-10T23:00:11.737152: step 3918, loss 0.0316635, acc 0.992188\n",
      "2017-01-10T23:00:13.752992: step 3919, loss 0.0154974, acc 0.992188\n",
      "2017-01-10T23:00:15.803248: step 3920, loss 0.0929056, acc 0.96875\n",
      "2017-01-10T23:00:17.855271: step 3921, loss 0.0344437, acc 0.992188\n",
      "2017-01-10T23:00:19.882264: step 3922, loss 0.0897685, acc 0.960938\n",
      "2017-01-10T23:00:22.066235: step 3923, loss 0.0552527, acc 0.976562\n",
      "2017-01-10T23:00:24.216785: step 3924, loss 0.0343772, acc 1\n",
      "2017-01-10T23:00:26.571033: step 3925, loss 0.0589277, acc 0.976562\n",
      "2017-01-10T23:00:29.331832: step 3926, loss 0.171833, acc 0.960938\n",
      "2017-01-10T23:00:31.461952: step 3927, loss 0.0502052, acc 0.992188\n",
      "2017-01-10T23:00:33.570210: step 3928, loss 0.102685, acc 0.960938\n",
      "2017-01-10T23:00:35.623074: step 3929, loss 0.0420932, acc 0.992188\n",
      "2017-01-10T23:00:37.673829: step 3930, loss 0.0835963, acc 0.976562\n",
      "2017-01-10T23:00:39.726178: step 3931, loss 0.0771495, acc 0.96875\n",
      "2017-01-10T23:00:41.746179: step 3932, loss 0.113558, acc 0.96875\n",
      "2017-01-10T23:00:43.803191: step 3933, loss 0.027866, acc 0.992188\n",
      "2017-01-10T23:00:45.778410: step 3934, loss 0.057801, acc 0.976562\n",
      "2017-01-10T23:00:47.838708: step 3935, loss 0.0648814, acc 0.96875\n",
      "2017-01-10T23:00:49.887623: step 3936, loss 0.141872, acc 0.96875\n",
      "2017-01-10T23:00:52.007877: step 3937, loss 0.0980683, acc 0.976562\n",
      "2017-01-10T23:00:54.192713: step 3938, loss 0.0546407, acc 0.976562\n",
      "2017-01-10T23:00:56.350256: step 3939, loss 0.0646548, acc 0.976562\n",
      "2017-01-10T23:00:58.382419: step 3940, loss 0.0837501, acc 0.96875\n",
      "2017-01-10T23:01:00.426786: step 3941, loss 0.0404785, acc 0.984375\n",
      "2017-01-10T23:01:02.467624: step 3942, loss 0.0175533, acc 1\n",
      "2017-01-10T23:01:04.542006: step 3943, loss 0.046125, acc 0.984375\n",
      "2017-01-10T23:01:06.564720: step 3944, loss 0.0864856, acc 0.96875\n",
      "2017-01-10T23:01:08.607661: step 3945, loss 0.00647195, acc 1\n",
      "2017-01-10T23:01:10.845552: step 3946, loss 0.0772274, acc 0.976562\n",
      "2017-01-10T23:01:12.981211: step 3947, loss 0.0281951, acc 0.992188\n",
      "2017-01-10T23:01:15.015932: step 3948, loss 0.0465571, acc 1\n",
      "2017-01-10T23:01:17.058867: step 3949, loss 0.0491502, acc 0.984375\n",
      "2017-01-10T23:01:19.156996: step 3950, loss 0.0356903, acc 0.984375\n",
      "2017-01-10T23:01:21.242022: step 3951, loss 0.0269979, acc 1\n",
      "2017-01-10T23:01:23.337967: step 3952, loss 0.0416107, acc 0.992188\n",
      "2017-01-10T23:01:25.461259: step 3953, loss 0.0857429, acc 0.960938\n",
      "2017-01-10T23:01:28.782479: step 3954, loss 0.0389888, acc 0.992188\n",
      "2017-01-10T23:01:31.016549: step 3955, loss 0.0501716, acc 0.992188\n",
      "2017-01-10T23:01:33.077618: step 3956, loss 0.0644018, acc 0.984375\n",
      "2017-01-10T23:01:35.215293: step 3957, loss 0.222456, acc 0.992188\n",
      "2017-01-10T23:01:37.242738: step 3958, loss 0.0163322, acc 0.992188\n",
      "2017-01-10T23:01:39.279464: step 3959, loss 0.0632404, acc 0.976562\n",
      "2017-01-10T23:01:41.313379: step 3960, loss 0.0172342, acc 1\n",
      "2017-01-10T23:01:43.361508: step 3961, loss 0.0235339, acc 0.992188\n",
      "2017-01-10T23:01:45.375887: step 3962, loss 0.0306711, acc 0.992188\n",
      "2017-01-10T23:01:47.427627: step 3963, loss 0.0967519, acc 0.96875\n",
      "2017-01-10T23:01:49.458828: step 3964, loss 0.0267583, acc 1\n",
      "2017-01-10T23:01:51.528322: step 3965, loss 0.0690114, acc 0.984375\n",
      "2017-01-10T23:01:53.570988: step 3966, loss 0.04051, acc 0.992188\n",
      "2017-01-10T23:01:55.593452: step 3967, loss 0.0291364, acc 0.992188\n",
      "2017-01-10T23:01:57.647200: step 3968, loss 0.0710686, acc 0.984375\n",
      "2017-01-10T23:01:59.860787: step 3969, loss 0.042685, acc 0.992188\n",
      "2017-01-10T23:02:01.896938: step 3970, loss 0.104058, acc 0.960938\n",
      "2017-01-10T23:02:03.941786: step 3971, loss 0.0941199, acc 0.976562\n",
      "2017-01-10T23:02:06.023624: step 3972, loss 0.0414063, acc 0.992188\n",
      "2017-01-10T23:02:08.060983: step 3973, loss 0.112291, acc 0.96875\n",
      "2017-01-10T23:02:10.096845: step 3974, loss 0.0307449, acc 0.992188\n",
      "2017-01-10T23:02:12.224631: step 3975, loss 0.0343006, acc 0.992188\n",
      "2017-01-10T23:02:14.246810: step 3976, loss 0.0593201, acc 0.984375\n",
      "2017-01-10T23:02:16.308982: step 3977, loss 0.0733091, acc 0.976562\n",
      "2017-01-10T23:02:18.354388: step 3978, loss 0.0662329, acc 0.984375\n",
      "2017-01-10T23:02:20.418420: step 3979, loss 0.082115, acc 0.976562\n",
      "2017-01-10T23:02:22.532809: step 3980, loss 0.048713, acc 0.984375\n",
      "2017-01-10T23:02:24.577192: step 3981, loss 0.0546946, acc 0.976562\n",
      "2017-01-10T23:02:27.405249: step 3982, loss 0.11769, acc 0.960938\n",
      "2017-01-10T23:02:29.647960: step 3983, loss 0.0618672, acc 0.976562\n",
      "2017-01-10T23:02:31.948425: step 3984, loss 0.0387646, acc 0.992188\n",
      "2017-01-10T23:02:34.076888: step 3985, loss 0.0144287, acc 1\n",
      "2017-01-10T23:02:36.086280: step 3986, loss 0.063614, acc 0.984375\n",
      "2017-01-10T23:02:38.145414: step 3987, loss 0.0334074, acc 0.992188\n",
      "2017-01-10T23:02:40.181466: step 3988, loss 0.0541449, acc 0.984375\n",
      "2017-01-10T23:02:42.207851: step 3989, loss 0.125622, acc 0.953125\n",
      "2017-01-10T23:02:44.231340: step 3990, loss 0.0683321, acc 0.976562\n",
      "2017-01-10T23:02:46.285090: step 3991, loss 0.0295346, acc 0.984375\n",
      "2017-01-10T23:02:48.322701: step 3992, loss 0.0574731, acc 0.984375\n",
      "2017-01-10T23:02:50.343550: step 3993, loss 0.0520925, acc 0.992188\n",
      "2017-01-10T23:02:52.462154: step 3994, loss 0.0378802, acc 0.984375\n",
      "2017-01-10T23:02:54.515319: step 3995, loss 0.00556175, acc 1\n",
      "2017-01-10T23:02:56.546937: step 3996, loss 0.0712999, acc 0.984375\n",
      "2017-01-10T23:02:58.571422: step 3997, loss 0.0643495, acc 0.976562\n",
      "2017-01-10T23:03:00.607553: step 3998, loss 0.0259058, acc 0.992188\n",
      "2017-01-10T23:03:02.683245: step 3999, loss 0.060572, acc 0.984375\n",
      "2017-01-10T23:03:04.964186: step 4000, loss 0.0542367, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:03:33.426492: step 4000, loss 0.0769985, acc 0.97748\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4000\n",
      "\n",
      "2017-01-10T23:03:38.521146: step 4001, loss 0.0489538, acc 0.984375\n",
      "2017-01-10T23:03:40.548468: step 4002, loss 0.0445556, acc 0.992188\n",
      "2017-01-10T23:03:42.631560: step 4003, loss 0.067897, acc 0.976562\n",
      "2017-01-10T23:03:44.653292: step 4004, loss 0.0425846, acc 0.992188\n",
      "2017-01-10T23:03:46.729679: step 4005, loss 0.05133, acc 0.984375\n",
      "2017-01-10T23:03:48.815987: step 4006, loss 0.0515118, acc 0.992188\n",
      "2017-01-10T23:03:50.952134: step 4007, loss 0.0696863, acc 0.976562\n",
      "2017-01-10T23:03:53.048738: step 4008, loss 0.0393887, acc 0.992188\n",
      "2017-01-10T23:03:55.117882: step 4009, loss 0.116277, acc 0.984375\n",
      "2017-01-10T23:03:57.141597: step 4010, loss 0.118502, acc 0.953125\n",
      "2017-01-10T23:03:59.163790: step 4011, loss 0.0597182, acc 0.984375\n",
      "2017-01-10T23:04:01.223330: step 4012, loss 0.0625816, acc 0.976562\n",
      "2017-01-10T23:04:03.254805: step 4013, loss 0.0214124, acc 0.992188\n",
      "2017-01-10T23:04:05.356270: step 4014, loss 0.0662767, acc 0.984375\n",
      "2017-01-10T23:04:07.371181: step 4015, loss 0.0858578, acc 0.976562\n",
      "2017-01-10T23:04:09.523934: step 4016, loss 0.0347941, acc 0.992188\n",
      "2017-01-10T23:04:11.661904: step 4017, loss 0.0500481, acc 0.984375\n",
      "2017-01-10T23:04:13.685809: step 4018, loss 0.0940405, acc 0.96875\n",
      "2017-01-10T23:04:15.720730: step 4019, loss 0.0521283, acc 0.976562\n",
      "2017-01-10T23:04:17.745513: step 4020, loss 0.044322, acc 0.976562\n",
      "2017-01-10T23:04:19.837787: step 4021, loss 0.0351246, acc 0.984375\n",
      "2017-01-10T23:04:21.922585: step 4022, loss 0.108979, acc 0.960938\n",
      "2017-01-10T23:04:23.900139: step 4023, loss 0.0821388, acc 0.96875\n",
      "2017-01-10T23:04:25.925811: step 4024, loss 0.105081, acc 0.984375\n",
      "2017-01-10T23:04:28.007848: step 4025, loss 0.0974268, acc 0.976562\n",
      "2017-01-10T23:04:30.121978: step 4026, loss 0.0410943, acc 0.984375\n",
      "2017-01-10T23:04:32.161720: step 4027, loss 0.100778, acc 0.976562\n",
      "2017-01-10T23:04:34.203417: step 4028, loss 0.109905, acc 0.984375\n",
      "2017-01-10T23:04:36.241966: step 4029, loss 0.0853174, acc 0.953125\n",
      "2017-01-10T23:04:38.276882: step 4030, loss 0.0542175, acc 0.984375\n",
      "2017-01-10T23:04:40.307030: step 4031, loss 0.0443577, acc 0.984375\n",
      "2017-01-10T23:04:42.514823: step 4032, loss 0.11911, acc 0.96875\n",
      "2017-01-10T23:04:44.511519: step 4033, loss 0.036125, acc 0.992188\n",
      "2017-01-10T23:04:46.553032: step 4034, loss 0.0437752, acc 0.992188\n",
      "2017-01-10T23:04:48.593998: step 4035, loss 0.114283, acc 0.96875\n",
      "2017-01-10T23:04:50.655725: step 4036, loss 0.106319, acc 0.953125\n",
      "2017-01-10T23:04:52.708592: step 4037, loss 0.124474, acc 0.96875\n",
      "2017-01-10T23:04:54.750297: step 4038, loss 0.110052, acc 0.96875\n",
      "2017-01-10T23:04:56.786905: step 4039, loss 0.0578479, acc 0.976562\n",
      "2017-01-10T23:04:58.834809: step 4040, loss 0.0254732, acc 0.992188\n",
      "2017-01-10T23:05:00.826935: step 4041, loss 0.150349, acc 0.953125\n",
      "2017-01-10T23:05:02.886307: step 4042, loss 0.0190116, acc 1\n",
      "2017-01-10T23:05:04.966341: step 4043, loss 0.0221012, acc 0.992188\n",
      "2017-01-10T23:05:06.981010: step 4044, loss 0.055132, acc 0.976562\n",
      "2017-01-10T23:05:08.981673: step 4045, loss 0.0946827, acc 0.96875\n",
      "2017-01-10T23:05:11.046929: step 4046, loss 0.0527072, acc 0.992188\n",
      "2017-01-10T23:05:13.096844: step 4047, loss 0.0603515, acc 0.984375\n",
      "2017-01-10T23:05:15.407568: step 4048, loss 0.0461955, acc 0.984375\n",
      "2017-01-10T23:05:17.640828: step 4049, loss 0.0463036, acc 0.976562\n",
      "2017-01-10T23:05:19.726556: step 4050, loss 0.125207, acc 0.96875\n",
      "2017-01-10T23:05:21.814587: step 4051, loss 0.00762677, acc 1\n",
      "2017-01-10T23:05:23.873098: step 4052, loss 0.0396388, acc 0.984375\n",
      "2017-01-10T23:05:25.928027: step 4053, loss 0.00761073, acc 0.992188\n",
      "2017-01-10T23:05:27.977286: step 4054, loss 0.17531, acc 0.953125\n",
      "2017-01-10T23:05:30.038722: step 4055, loss 0.0547047, acc 0.976562\n",
      "2017-01-10T23:05:32.240242: step 4056, loss 0.156203, acc 0.953125\n",
      "2017-01-10T23:05:35.283873: step 4057, loss 0.00884414, acc 1\n",
      "2017-01-10T23:05:37.312761: step 4058, loss 0.0456266, acc 0.992188\n",
      "2017-01-10T23:05:39.308917: step 4059, loss 0.155642, acc 0.945312\n",
      "2017-01-10T23:05:41.329873: step 4060, loss 0.0426114, acc 0.992188\n",
      "2017-01-10T23:05:43.417390: step 4061, loss 0.0351788, acc 0.984375\n",
      "2017-01-10T23:05:45.468346: step 4062, loss 0.0759996, acc 0.976562\n",
      "2017-01-10T23:05:47.734673: step 4063, loss 0.0623921, acc 0.976562\n",
      "2017-01-10T23:05:49.819777: step 4064, loss 0.0824308, acc 0.960938\n",
      "2017-01-10T23:05:51.897947: step 4065, loss 0.0753607, acc 0.976562\n",
      "2017-01-10T23:05:53.921348: step 4066, loss 0.0224534, acc 0.992188\n",
      "2017-01-10T23:05:55.954981: step 4067, loss 0.0949167, acc 0.96875\n",
      "2017-01-10T23:05:58.035192: step 4068, loss 0.0749822, acc 0.976562\n",
      "2017-01-10T23:06:00.063481: step 4069, loss 0.118599, acc 0.976562\n",
      "2017-01-10T23:06:02.092898: step 4070, loss 0.0638117, acc 0.976562\n",
      "2017-01-10T23:06:04.136152: step 4071, loss 0.0632862, acc 0.976562\n",
      "2017-01-10T23:06:06.176897: step 4072, loss 0.0118937, acc 0.992188\n",
      "2017-01-10T23:06:08.217570: step 4073, loss 0.0448334, acc 0.984375\n",
      "2017-01-10T23:06:10.235037: step 4074, loss 0.090331, acc 0.96875\n",
      "2017-01-10T23:06:12.248334: step 4075, loss 0.0343007, acc 0.992188\n",
      "2017-01-10T23:06:14.296598: step 4076, loss 0.091464, acc 0.976562\n",
      "2017-01-10T23:06:16.348977: step 4077, loss 0.0453644, acc 0.984375\n",
      "2017-01-10T23:06:18.709728: step 4078, loss 0.0597231, acc 0.96875\n",
      "2017-01-10T23:06:20.833429: step 4079, loss 0.0170413, acc 1\n",
      "2017-01-10T23:06:22.885348: step 4080, loss 0.0838319, acc 0.976562\n",
      "2017-01-10T23:06:24.941939: step 4081, loss 0.0764999, acc 0.976562\n",
      "2017-01-10T23:06:26.949723: step 4082, loss 0.0774243, acc 0.976562\n",
      "2017-01-10T23:06:29.000670: step 4083, loss 0.0899239, acc 0.976562\n",
      "2017-01-10T23:06:31.146291: step 4084, loss 0.0485517, acc 0.984375\n",
      "2017-01-10T23:06:34.003861: step 4085, loss 0.051197, acc 0.984375\n",
      "2017-01-10T23:06:36.343257: step 4086, loss 0.0271102, acc 0.992188\n",
      "2017-01-10T23:06:38.395107: step 4087, loss 0.0724866, acc 0.976562\n",
      "2017-01-10T23:06:40.410652: step 4088, loss 0.0205134, acc 1\n",
      "2017-01-10T23:06:42.443575: step 4089, loss 0.0487482, acc 0.984375\n",
      "2017-01-10T23:06:44.476829: step 4090, loss 0.0514462, acc 0.984375\n",
      "2017-01-10T23:06:46.504106: step 4091, loss 0.0342598, acc 0.992188\n",
      "2017-01-10T23:06:48.554170: step 4092, loss 0.049099, acc 0.976562\n",
      "2017-01-10T23:06:50.865932: step 4093, loss 0.0353817, acc 0.992188\n",
      "2017-01-10T23:06:52.920904: step 4094, loss 0.0315556, acc 0.992188\n",
      "2017-01-10T23:06:54.935852: step 4095, loss 0.0253773, acc 1\n",
      "2017-01-10T23:06:56.972806: step 4096, loss 0.106838, acc 0.96875\n",
      "2017-01-10T23:06:58.972586: step 4097, loss 0.0847694, acc 0.953125\n",
      "2017-01-10T23:07:00.996490: step 4098, loss 0.0514654, acc 0.992188\n",
      "2017-01-10T23:07:03.031866: step 4099, loss 0.0314883, acc 0.992188\n",
      "2017-01-10T23:07:05.130317: step 4100, loss 0.0537496, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:07:26.675364: step 4100, loss 0.0772373, acc 0.97772\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4100\n",
      "\n",
      "2017-01-10T23:07:31.237944: step 4101, loss 0.0632906, acc 0.992188\n",
      "2017-01-10T23:07:34.134034: step 4102, loss 0.119199, acc 0.984375\n",
      "2017-01-10T23:07:36.429005: step 4103, loss 0.0561587, acc 0.984375\n",
      "2017-01-10T23:07:38.460576: step 4104, loss 0.00950504, acc 1\n",
      "2017-01-10T23:07:40.504532: step 4105, loss 0.145973, acc 0.960938\n",
      "2017-01-10T23:07:42.494369: step 4106, loss 0.00259948, acc 1\n",
      "2017-01-10T23:07:44.511646: step 4107, loss 0.0955931, acc 0.960938\n",
      "2017-01-10T23:07:46.543371: step 4108, loss 0.0457296, acc 0.984375\n",
      "2017-01-10T23:07:48.573559: step 4109, loss 0.0536321, acc 0.984375\n",
      "2017-01-10T23:07:50.680178: step 4110, loss 0.0522589, acc 0.984375\n",
      "2017-01-10T23:07:52.738126: step 4111, loss 0.0330335, acc 0.992188\n",
      "2017-01-10T23:07:54.922718: step 4112, loss 0.0626818, acc 0.984375\n",
      "2017-01-10T23:07:56.980156: step 4113, loss 0.0630217, acc 0.976562\n",
      "2017-01-10T23:07:59.029725: step 4114, loss 0.0355536, acc 0.992188\n",
      "2017-01-10T23:08:01.057546: step 4115, loss 0.0622061, acc 0.984375\n",
      "2017-01-10T23:08:03.099162: step 4116, loss 0.0538362, acc 0.984375\n",
      "2017-01-10T23:08:05.152022: step 4117, loss 0.0681387, acc 0.976562\n",
      "2017-01-10T23:08:07.167093: step 4118, loss 0.0805214, acc 0.984375\n",
      "2017-01-10T23:08:09.184030: step 4119, loss 0.0230247, acc 0.984375\n",
      "2017-01-10T23:08:11.214779: step 4120, loss 0.15245, acc 0.960938\n",
      "2017-01-10T23:08:13.274524: step 4121, loss 0.0826256, acc 0.976562\n",
      "2017-01-10T23:08:15.304286: step 4122, loss 0.061179, acc 0.96875\n",
      "2017-01-10T23:08:17.355727: step 4123, loss 0.0473879, acc 0.984375\n",
      "2017-01-10T23:08:19.388443: step 4124, loss 0.0529529, acc 0.984375\n",
      "2017-01-10T23:08:21.491805: step 4125, loss 0.0728797, acc 0.976562\n",
      "2017-01-10T23:08:23.559916: step 4126, loss 0.0927323, acc 0.96875\n",
      "2017-01-10T23:08:25.626093: step 4127, loss 0.0605666, acc 0.976562\n",
      "2017-01-10T23:08:27.928757: step 4128, loss 0.0502317, acc 0.976562\n",
      "2017-01-10T23:08:30.005448: step 4129, loss 0.029031, acc 0.976562\n",
      "2017-01-10T23:08:32.028413: step 4130, loss 0.0577825, acc 0.992188\n",
      "2017-01-10T23:08:34.085133: step 4131, loss 0.0453409, acc 0.992188\n",
      "2017-01-10T23:08:36.226356: step 4132, loss 0.107519, acc 0.976562\n",
      "2017-01-10T23:08:38.278227: step 4133, loss 0.0300946, acc 0.984375\n",
      "2017-01-10T23:08:40.340211: step 4134, loss 0.127397, acc 0.953125\n",
      "2017-01-10T23:08:42.400484: step 4135, loss 0.0222663, acc 1\n",
      "2017-01-10T23:08:44.419842: step 4136, loss 0.0796893, acc 0.984375\n",
      "2017-01-10T23:08:46.429600: step 4137, loss 0.0648006, acc 0.976562\n",
      "2017-01-10T23:08:48.447025: step 4138, loss 0.029854, acc 0.992188\n",
      "2017-01-10T23:08:50.568161: step 4139, loss 0.0529534, acc 0.984375\n",
      "2017-01-10T23:08:52.656954: step 4140, loss 0.0570937, acc 0.976562\n",
      "2017-01-10T23:08:54.687326: step 4141, loss 0.0397283, acc 0.976562\n",
      "2017-01-10T23:08:56.734435: step 4142, loss 0.101275, acc 0.976562\n",
      "2017-01-10T23:08:58.938437: step 4143, loss 0.0545048, acc 0.984375\n",
      "2017-01-10T23:09:00.982874: step 4144, loss 0.0471516, acc 0.984375\n",
      "2017-01-10T23:09:02.997276: step 4145, loss 0.0801407, acc 0.96875\n",
      "2017-01-10T23:09:05.050185: step 4146, loss 0.00224102, acc 1\n",
      "2017-01-10T23:09:07.041665: step 4147, loss 0.097329, acc 0.96875\n",
      "2017-01-10T23:09:09.124593: step 4148, loss 0.041917, acc 0.992188\n",
      "2017-01-10T23:09:11.170817: step 4149, loss 0.0639717, acc 0.976562\n",
      "2017-01-10T23:09:13.212450: step 4150, loss 0.0248248, acc 1\n",
      "2017-01-10T23:09:15.279153: step 4151, loss 0.0214492, acc 0.992188\n",
      "2017-01-10T23:09:17.283675: step 4152, loss 0.0512604, acc 0.984375\n",
      "2017-01-10T23:09:19.322587: step 4153, loss 0.0401924, acc 0.984375\n",
      "2017-01-10T23:09:21.385817: step 4154, loss 0.0326329, acc 0.984375\n",
      "2017-01-10T23:09:23.456435: step 4155, loss 0.0957317, acc 0.976562\n",
      "2017-01-10T23:09:25.507670: step 4156, loss 0.0664209, acc 0.976562\n",
      "2017-01-10T23:09:27.561421: step 4157, loss 0.0676751, acc 0.976562\n",
      "2017-01-10T23:09:29.614472: step 4158, loss 0.0820386, acc 0.96875\n",
      "2017-01-10T23:09:31.860584: step 4159, loss 0.0900295, acc 0.992188\n",
      "2017-01-10T23:09:33.896986: step 4160, loss 0.0680368, acc 0.984375\n",
      "2017-01-10T23:09:35.966987: step 4161, loss 0.0213355, acc 0.992188\n",
      "2017-01-10T23:09:37.993622: step 4162, loss 0.0373268, acc 0.992188\n",
      "2017-01-10T23:09:39.987470: step 4163, loss 0.102156, acc 0.976562\n",
      "2017-01-10T23:09:42.046999: step 4164, loss 0.0790094, acc 0.96875\n",
      "2017-01-10T23:09:44.056412: step 4165, loss 0.149078, acc 0.953125\n",
      "2017-01-10T23:09:46.079531: step 4166, loss 0.136377, acc 0.96875\n",
      "2017-01-10T23:09:48.092174: step 4167, loss 0.0467619, acc 0.992188\n",
      "2017-01-10T23:09:50.188707: step 4168, loss 0.109553, acc 0.96875\n",
      "2017-01-10T23:09:52.305246: step 4169, loss 0.026411, acc 0.984375\n",
      "2017-01-10T23:09:54.358948: step 4170, loss 0.0401983, acc 0.984375\n",
      "2017-01-10T23:09:56.397775: step 4171, loss 0.0579231, acc 0.984375\n",
      "2017-01-10T23:09:58.418276: step 4172, loss 0.0944303, acc 0.976562\n",
      "2017-01-10T23:10:00.465791: step 4173, loss 0.0562892, acc 0.976562\n",
      "2017-01-10T23:10:02.648076: step 4174, loss 0.131124, acc 0.953125\n",
      "2017-01-10T23:10:04.761090: step 4175, loss 0.0888901, acc 0.976562\n",
      "2017-01-10T23:10:06.811587: step 4176, loss 0.0851396, acc 0.960938\n",
      "2017-01-10T23:10:08.863782: step 4177, loss 0.0322355, acc 0.992188\n",
      "2017-01-10T23:10:10.898560: step 4178, loss 0.0893525, acc 0.976562\n",
      "2017-01-10T23:10:12.923698: step 4179, loss 0.0619429, acc 0.976562\n",
      "2017-01-10T23:10:14.946773: step 4180, loss 0.00363575, acc 1\n",
      "2017-01-10T23:10:16.975289: step 4181, loss 0.157226, acc 0.9375\n",
      "2017-01-10T23:10:19.008098: step 4182, loss 0.0608438, acc 0.976562\n",
      "2017-01-10T23:10:21.070359: step 4183, loss 0.0316105, acc 0.992188\n",
      "2017-01-10T23:10:23.331549: step 4184, loss 0.343564, acc 0.945312\n",
      "2017-01-10T23:10:25.394644: step 4185, loss 0.0540116, acc 0.984375\n",
      "2017-01-10T23:10:27.453781: step 4186, loss 0.0366405, acc 0.992188\n",
      "2017-01-10T23:10:29.481990: step 4187, loss 0.0490761, acc 0.992188\n",
      "2017-01-10T23:10:31.624924: step 4188, loss 0.0241125, acc 0.992188\n",
      "2017-01-10T23:10:33.742982: step 4189, loss 0.0310047, acc 0.992188\n",
      "2017-01-10T23:10:35.996041: step 4190, loss 0.070803, acc 0.976562\n",
      "2017-01-10T23:10:38.451008: step 4191, loss 0.0793093, acc 0.96875\n",
      "2017-01-10T23:10:41.039946: step 4192, loss 0.0773648, acc 0.976562\n",
      "2017-01-10T23:10:43.053229: step 4193, loss 0.0394985, acc 0.992188\n",
      "2017-01-10T23:10:45.074941: step 4194, loss 0.0411853, acc 0.984375\n",
      "2017-01-10T23:10:47.076635: step 4195, loss 0.0839758, acc 0.976562\n",
      "2017-01-10T23:10:49.145967: step 4196, loss 0.140509, acc 0.953125\n",
      "2017-01-10T23:10:51.249196: step 4197, loss 0.051402, acc 0.976562\n",
      "2017-01-10T23:10:53.309199: step 4198, loss 0.0741231, acc 0.984375\n",
      "2017-01-10T23:10:55.353014: step 4199, loss 0.0307753, acc 0.984375\n",
      "2017-01-10T23:10:57.385035: step 4200, loss 0.0683406, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:11:20.271087: step 4200, loss 0.0771478, acc 0.97792\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4200\n",
      "\n",
      "2017-01-10T23:11:25.195849: step 4201, loss 0.120235, acc 0.960938\n",
      "2017-01-10T23:11:27.234743: step 4202, loss 0.0596591, acc 0.984375\n",
      "2017-01-10T23:11:29.256547: step 4203, loss 0.0790059, acc 0.976562\n",
      "2017-01-10T23:11:31.307472: step 4204, loss 0.160996, acc 0.953125\n",
      "2017-01-10T23:11:33.363479: step 4205, loss 0.00867824, acc 1\n",
      "2017-01-10T23:11:35.424135: step 4206, loss 0.0884021, acc 0.96875\n",
      "2017-01-10T23:11:37.488229: step 4207, loss 0.0729713, acc 0.976562\n",
      "2017-01-10T23:11:41.030742: step 4208, loss 0.152622, acc 0.96875\n",
      "2017-01-10T23:11:43.094350: step 4209, loss 0.0320667, acc 0.992188\n",
      "2017-01-10T23:11:45.129056: step 4210, loss 0.0358624, acc 1\n",
      "2017-01-10T23:11:47.177246: step 4211, loss 0.0629156, acc 0.984375\n",
      "2017-01-10T23:11:49.212045: step 4212, loss 0.138219, acc 0.953125\n",
      "2017-01-10T23:11:51.290254: step 4213, loss 0.104863, acc 0.976562\n",
      "2017-01-10T23:11:53.290717: step 4214, loss 0.120778, acc 0.953125\n",
      "2017-01-10T23:11:55.299967: step 4215, loss 0.0590128, acc 0.992188\n",
      "2017-01-10T23:11:57.338328: step 4216, loss 0.102929, acc 0.96875\n",
      "2017-01-10T23:11:59.363812: step 4217, loss 0.0418601, acc 0.992188\n",
      "2017-01-10T23:12:01.410615: step 4218, loss 0.0471584, acc 0.984375\n",
      "2017-01-10T23:12:03.447003: step 4219, loss 0.0461873, acc 0.984375\n",
      "2017-01-10T23:12:05.522696: step 4220, loss 0.0447052, acc 0.984375\n",
      "2017-01-10T23:12:07.631645: step 4221, loss 0.0164946, acc 1\n",
      "2017-01-10T23:12:09.665972: step 4222, loss 0.0631259, acc 0.984375\n",
      "2017-01-10T23:12:11.842605: step 4223, loss 0.0513593, acc 0.984375\n",
      "2017-01-10T23:12:13.913437: step 4224, loss 0.000942123, acc 1\n",
      "2017-01-10T23:12:15.936435: step 4225, loss 0.0117842, acc 1\n",
      "2017-01-10T23:12:17.975806: step 4226, loss 0.093945, acc 0.984375\n",
      "2017-01-10T23:12:20.015520: step 4227, loss 0.0410361, acc 0.976562\n",
      "2017-01-10T23:12:22.089754: step 4228, loss 0.171825, acc 0.96875\n",
      "2017-01-10T23:12:24.279638: step 4229, loss 0.064479, acc 0.976562\n",
      "2017-01-10T23:12:26.299954: step 4230, loss 0.131255, acc 0.960938\n",
      "2017-01-10T23:12:28.331194: step 4231, loss 0.0592092, acc 0.984375\n",
      "2017-01-10T23:12:30.484744: step 4232, loss 0.0279786, acc 0.992188\n",
      "2017-01-10T23:12:32.517066: step 4233, loss 0.0669108, acc 0.984375\n",
      "2017-01-10T23:12:34.602849: step 4234, loss 0.0907798, acc 0.976562\n",
      "2017-01-10T23:12:36.661066: step 4235, loss 0.0917152, acc 0.960938\n",
      "2017-01-10T23:12:39.409993: step 4236, loss 0.0630435, acc 0.984375\n",
      "2017-01-10T23:12:41.757607: step 4237, loss 0.0249063, acc 1\n",
      "2017-01-10T23:12:44.053095: step 4238, loss 0.0615229, acc 0.984375\n",
      "2017-01-10T23:12:46.077576: step 4239, loss 0.0132796, acc 0.992188\n",
      "2017-01-10T23:12:48.104981: step 4240, loss 0.00236667, acc 1\n",
      "2017-01-10T23:12:50.214019: step 4241, loss 0.0916624, acc 0.953125\n",
      "2017-01-10T23:12:52.316214: step 4242, loss 0.0946642, acc 0.96875\n",
      "2017-01-10T23:12:54.367242: step 4243, loss 0.0385476, acc 0.984375\n",
      "2017-01-10T23:12:56.410213: step 4244, loss 0.107445, acc 0.96875\n",
      "2017-01-10T23:12:58.454206: step 4245, loss 0.00127364, acc 1\n",
      "2017-01-10T23:13:00.491904: step 4246, loss 0.0857265, acc 0.96875\n",
      "2017-01-10T23:13:02.542804: step 4247, loss 0.047015, acc 0.976562\n",
      "2017-01-10T23:13:04.621428: step 4248, loss 0.0727461, acc 0.96875\n",
      "2017-01-10T23:13:06.647878: step 4249, loss 0.102036, acc 0.976562\n",
      "2017-01-10T23:13:08.705898: step 4250, loss 0.0372533, acc 0.984375\n",
      "2017-01-10T23:13:10.750324: step 4251, loss 0.0770846, acc 0.960938\n",
      "2017-01-10T23:13:12.786078: step 4252, loss 0.101688, acc 0.960938\n",
      "2017-01-10T23:13:14.924401: step 4253, loss 0.0620042, acc 0.96875\n",
      "2017-01-10T23:13:17.087205: step 4254, loss 0.0451395, acc 0.992188\n",
      "2017-01-10T23:13:19.120377: step 4255, loss 0.0417423, acc 0.992188\n",
      "2017-01-10T23:13:21.222670: step 4256, loss 0.0773493, acc 0.984375\n",
      "2017-01-10T23:13:23.261409: step 4257, loss 0.0281942, acc 0.992188\n",
      "2017-01-10T23:13:25.299613: step 4258, loss 0.0520123, acc 0.984375\n",
      "2017-01-10T23:13:27.324710: step 4259, loss 0.103683, acc 0.960938\n",
      "2017-01-10T23:13:29.371006: step 4260, loss 0.0416745, acc 0.984375\n",
      "2017-01-10T23:13:31.376894: step 4261, loss 0.071731, acc 0.984375\n",
      "2017-01-10T23:13:33.405576: step 4262, loss 0.0319291, acc 0.984375\n",
      "2017-01-10T23:13:35.485128: step 4263, loss 0.044158, acc 0.984375\n",
      "2017-01-10T23:13:37.517862: step 4264, loss 0.0321186, acc 0.992188\n",
      "2017-01-10T23:13:39.544439: step 4265, loss 0.0488792, acc 0.984375\n",
      "2017-01-10T23:13:41.596734: step 4266, loss 0.0605069, acc 0.984375\n",
      "2017-01-10T23:13:43.657510: step 4267, loss 0.0648567, acc 0.976562\n",
      "2017-01-10T23:13:45.692818: step 4268, loss 0.0225004, acc 1\n",
      "2017-01-10T23:13:47.986005: step 4269, loss 0.063069, acc 0.976562\n",
      "2017-01-10T23:13:50.030450: step 4270, loss 0.0225866, acc 0.992188\n",
      "2017-01-10T23:13:52.104516: step 4271, loss 0.0608894, acc 0.976562\n",
      "2017-01-10T23:13:54.151320: step 4272, loss 0.0997738, acc 0.960938\n",
      "2017-01-10T23:13:56.192371: step 4273, loss 0.0315524, acc 1\n",
      "2017-01-10T23:13:58.247287: step 4274, loss 0.169911, acc 0.960938\n",
      "2017-01-10T23:14:00.294694: step 4275, loss 0.0319952, acc 0.992188\n",
      "2017-01-10T23:14:02.351873: step 4276, loss 0.0831399, acc 0.976562\n",
      "2017-01-10T23:14:04.403436: step 4277, loss 0.10705, acc 0.960938\n",
      "2017-01-10T23:14:06.453627: step 4278, loss 0.00366368, acc 1\n",
      "2017-01-10T23:14:08.499184: step 4279, loss 0.0762978, acc 0.976562\n",
      "2017-01-10T23:14:10.550337: step 4280, loss 0.144069, acc 0.953125\n",
      "2017-01-10T23:14:12.617527: step 4281, loss 0.108745, acc 0.96875\n",
      "2017-01-10T23:14:14.644125: step 4282, loss 0.0263316, acc 0.984375\n",
      "2017-01-10T23:14:16.705601: step 4283, loss 0.0746186, acc 0.976562\n",
      "2017-01-10T23:14:18.878298: step 4284, loss 0.049625, acc 0.976562\n",
      "2017-01-10T23:14:21.077793: step 4285, loss 0.0240665, acc 0.992188\n",
      "2017-01-10T23:14:23.109043: step 4286, loss 0.0398619, acc 0.992188\n",
      "2017-01-10T23:14:25.115435: step 4287, loss 0.0462718, acc 0.976562\n",
      "2017-01-10T23:14:27.172407: step 4288, loss 0.0610858, acc 0.976562\n",
      "2017-01-10T23:14:29.227007: step 4289, loss 0.0820857, acc 0.96875\n",
      "2017-01-10T23:14:31.337138: step 4290, loss 0.0435154, acc 0.992188\n",
      "2017-01-10T23:14:33.391043: step 4291, loss 0.113593, acc 0.960938\n",
      "2017-01-10T23:14:35.428877: step 4292, loss 0.0582732, acc 0.976562\n",
      "2017-01-10T23:14:37.474654: step 4293, loss 0.0480816, acc 0.984375\n",
      "2017-01-10T23:14:39.566076: step 4294, loss 0.078929, acc 0.976562\n",
      "2017-01-10T23:14:41.643728: step 4295, loss 0.0600958, acc 0.992188\n",
      "2017-01-10T23:14:43.653594: step 4296, loss 0.00263127, acc 1\n",
      "2017-01-10T23:14:45.694004: step 4297, loss 0.0115121, acc 1\n",
      "2017-01-10T23:14:47.725095: step 4298, loss 0.0795741, acc 0.976562\n",
      "2017-01-10T23:14:49.797944: step 4299, loss 0.0998602, acc 0.96875\n",
      "2017-01-10T23:14:52.206500: step 4300, loss 0.0634088, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:15:13.189380: step 4300, loss 0.0761294, acc 0.97824\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4300\n",
      "\n",
      "2017-01-10T23:15:17.814564: step 4301, loss 0.0242502, acc 0.984375\n",
      "2017-01-10T23:15:19.904815: step 4302, loss 0.0496574, acc 0.984375\n",
      "2017-01-10T23:15:21.947506: step 4303, loss 0.0221816, acc 0.992188\n",
      "2017-01-10T23:15:24.322120: step 4304, loss 0.0182818, acc 0.992188\n",
      "2017-01-10T23:15:26.383147: step 4305, loss 0.0217913, acc 0.992188\n",
      "2017-01-10T23:15:28.587634: step 4306, loss 0.0573819, acc 0.984375\n",
      "2017-01-10T23:15:30.559052: step 4307, loss 0.0180925, acc 0.984375\n",
      "2017-01-10T23:15:32.625631: step 4308, loss 0.0613689, acc 0.96875\n",
      "2017-01-10T23:15:34.688729: step 4309, loss 0.0183263, acc 0.992188\n",
      "2017-01-10T23:15:36.697383: step 4310, loss 0.0661994, acc 0.976562\n",
      "2017-01-10T23:15:38.764053: step 4311, loss 0.0895976, acc 0.96875\n",
      "2017-01-10T23:15:40.801703: step 4312, loss 0.138319, acc 0.953125\n",
      "2017-01-10T23:15:42.855274: step 4313, loss 0.0513603, acc 0.976562\n",
      "2017-01-10T23:15:45.624094: step 4314, loss 0.063613, acc 0.96875\n",
      "2017-01-10T23:15:47.879032: step 4315, loss 0.0604874, acc 0.984375\n",
      "2017-01-10T23:15:49.927801: step 4316, loss 0.0485564, acc 0.984375\n",
      "2017-01-10T23:15:51.985367: step 4317, loss 0.0570168, acc 0.984375\n",
      "2017-01-10T23:15:54.044633: step 4318, loss 0.0453945, acc 0.992188\n",
      "2017-01-10T23:15:56.428308: step 4319, loss 0.062056, acc 0.992188\n",
      "2017-01-10T23:15:58.443232: step 4320, loss 0.0455275, acc 0.984375\n",
      "2017-01-10T23:16:00.526487: step 4321, loss 0.062814, acc 0.976562\n",
      "2017-01-10T23:16:02.563064: step 4322, loss 0.104314, acc 0.96875\n",
      "2017-01-10T23:16:04.629122: step 4323, loss 0.0389885, acc 0.984375\n",
      "2017-01-10T23:16:06.673049: step 4324, loss 0.052024, acc 0.984375\n",
      "2017-01-10T23:16:08.696071: step 4325, loss 0.0863082, acc 0.96875\n",
      "2017-01-10T23:16:10.727321: step 4326, loss 0.0716927, acc 0.976562\n",
      "2017-01-10T23:16:12.756544: step 4327, loss 0.0385645, acc 0.992188\n",
      "2017-01-10T23:16:14.776341: step 4328, loss 0.0825666, acc 0.976562\n",
      "2017-01-10T23:16:16.813865: step 4329, loss 0.0963456, acc 0.96875\n",
      "2017-01-10T23:16:18.844299: step 4330, loss 0.0594994, acc 0.992188\n",
      "2017-01-10T23:16:20.901218: step 4331, loss 0.0835464, acc 0.96875\n",
      "2017-01-10T23:16:22.915076: step 4332, loss 0.0645235, acc 0.984375\n",
      "2017-01-10T23:16:24.973254: step 4333, loss 0.0746601, acc 0.976562\n",
      "2017-01-10T23:16:27.073993: step 4334, loss 0.0368749, acc 0.984375\n",
      "2017-01-10T23:16:29.652269: step 4335, loss 0.0888455, acc 0.96875\n",
      "2017-01-10T23:16:31.764049: step 4336, loss 0.0924975, acc 0.96875\n",
      "2017-01-10T23:16:33.810509: step 4337, loss 0.0372919, acc 0.984375\n",
      "2017-01-10T23:16:35.884539: step 4338, loss 0.0543256, acc 0.976562\n",
      "2017-01-10T23:16:37.894827: step 4339, loss 0.0670848, acc 0.984375\n",
      "2017-01-10T23:16:39.997370: step 4340, loss 0.0898197, acc 0.976562\n",
      "2017-01-10T23:16:42.045656: step 4341, loss 0.0437153, acc 0.984375\n",
      "2017-01-10T23:16:44.084926: step 4342, loss 0.0634278, acc 0.976562\n",
      "2017-01-10T23:16:47.013879: step 4343, loss 0.120399, acc 0.953125\n",
      "2017-01-10T23:16:49.262927: step 4344, loss 0.0152991, acc 0.992188\n",
      "2017-01-10T23:16:51.339661: step 4345, loss 0.0560797, acc 0.984375\n",
      "2017-01-10T23:16:53.382522: step 4346, loss 0.0946335, acc 0.96875\n",
      "2017-01-10T23:16:55.422097: step 4347, loss 0.0469061, acc 0.984375\n",
      "2017-01-10T23:16:57.450186: step 4348, loss 0.0151015, acc 1\n",
      "2017-01-10T23:16:59.563947: step 4349, loss 0.0826726, acc 0.976562\n",
      "2017-01-10T23:17:01.763874: step 4350, loss 0.00807903, acc 1\n",
      "2017-01-10T23:17:03.828384: step 4351, loss 0.0279161, acc 0.992188\n",
      "2017-01-10T23:17:05.862281: step 4352, loss 0.056099, acc 0.984375\n",
      "2017-01-10T23:17:07.883923: step 4353, loss 0.089413, acc 0.96875\n",
      "2017-01-10T23:17:09.913452: step 4354, loss 0.149375, acc 0.96875\n",
      "2017-01-10T23:17:11.943176: step 4355, loss 0.106637, acc 0.984375\n",
      "2017-01-10T23:17:13.986250: step 4356, loss 0.0883366, acc 0.96875\n",
      "2017-01-10T23:17:16.021294: step 4357, loss 0.0978598, acc 0.96875\n",
      "2017-01-10T23:17:18.028039: step 4358, loss 0.0548348, acc 0.984375\n",
      "2017-01-10T23:17:20.086605: step 4359, loss 0.0418899, acc 0.992188\n",
      "2017-01-10T23:17:22.185555: step 4360, loss 0.0684935, acc 0.976562\n",
      "2017-01-10T23:17:24.222440: step 4361, loss 0.164849, acc 0.960938\n",
      "2017-01-10T23:17:26.262187: step 4362, loss 0.0355978, acc 0.984375\n",
      "2017-01-10T23:17:28.311456: step 4363, loss 0.0584284, acc 0.976562\n",
      "2017-01-10T23:17:30.430268: step 4364, loss 0.0423743, acc 0.992188\n",
      "2017-01-10T23:17:32.644715: step 4365, loss 0.0281156, acc 0.992188\n",
      "2017-01-10T23:17:34.699509: step 4366, loss 0.139726, acc 0.953125\n",
      "2017-01-10T23:17:36.750832: step 4367, loss 0.0207548, acc 0.992188\n",
      "2017-01-10T23:17:38.798147: step 4368, loss 0.0833901, acc 0.960938\n",
      "2017-01-10T23:17:40.979555: step 4369, loss 0.134447, acc 0.953125\n",
      "2017-01-10T23:17:43.005871: step 4370, loss 0.0136274, acc 0.992188\n",
      "2017-01-10T23:17:45.907562: step 4371, loss 0.0598902, acc 0.976562\n",
      "2017-01-10T23:17:48.086716: step 4372, loss 0.0480364, acc 0.984375\n",
      "2017-01-10T23:17:50.208075: step 4373, loss 0.00654542, acc 1\n",
      "2017-01-10T23:17:52.295382: step 4374, loss 0.0717541, acc 0.976562\n",
      "2017-01-10T23:17:54.342998: step 4375, loss 0.0392498, acc 0.992188\n",
      "2017-01-10T23:17:56.393073: step 4376, loss 0.0303797, acc 0.992188\n",
      "2017-01-10T23:17:58.381712: step 4377, loss 0.0295269, acc 0.984375\n",
      "2017-01-10T23:18:00.425154: step 4378, loss 0.126791, acc 0.960938\n",
      "2017-01-10T23:18:02.464816: step 4379, loss 0.114334, acc 0.960938\n",
      "2017-01-10T23:18:04.754715: step 4380, loss 0.0366094, acc 0.992188\n",
      "2017-01-10T23:18:06.784743: step 4381, loss 0.00473926, acc 1\n",
      "2017-01-10T23:18:08.857639: step 4382, loss 0.0274137, acc 0.984375\n",
      "2017-01-10T23:18:10.936168: step 4383, loss 0.0290247, acc 0.992188\n",
      "2017-01-10T23:18:12.978219: step 4384, loss 0.0563897, acc 0.96875\n",
      "2017-01-10T23:18:15.031029: step 4385, loss 0.0279748, acc 0.992188\n",
      "2017-01-10T23:18:17.081333: step 4386, loss 0.0429548, acc 0.992188\n",
      "2017-01-10T23:18:19.106663: step 4387, loss 0.0489567, acc 0.984375\n",
      "2017-01-10T23:18:21.175282: step 4388, loss 0.0722516, acc 0.976562\n",
      "2017-01-10T23:18:23.209328: step 4389, loss 0.123632, acc 0.960938\n",
      "2017-01-10T23:18:25.245422: step 4390, loss 0.0358959, acc 0.984375\n",
      "2017-01-10T23:18:27.289618: step 4391, loss 0.0483224, acc 0.992188\n",
      "2017-01-10T23:18:29.322937: step 4392, loss 0.142358, acc 0.960938\n",
      "2017-01-10T23:18:31.441745: step 4393, loss 0.046063, acc 0.984375\n",
      "2017-01-10T23:18:33.479517: step 4394, loss 0.0439655, acc 0.984375\n",
      "2017-01-10T23:18:35.595534: step 4395, loss 0.0544185, acc 0.984375\n",
      "2017-01-10T23:18:37.837039: step 4396, loss 0.0491277, acc 1\n",
      "2017-01-10T23:18:39.918037: step 4397, loss 0.0610467, acc 0.984375\n",
      "2017-01-10T23:18:41.948176: step 4398, loss 0.0734057, acc 0.960938\n",
      "2017-01-10T23:18:43.994100: step 4399, loss 0.0881585, acc 0.976562\n",
      "2017-01-10T23:18:46.008241: step 4400, loss 0.0477703, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:19:13.039404: step 4400, loss 0.0753559, acc 0.97848\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4400\n",
      "\n",
      "2017-01-10T23:19:17.668620: step 4401, loss 0.0511466, acc 0.992188\n",
      "2017-01-10T23:19:19.686267: step 4402, loss 0.0390438, acc 0.984375\n",
      "2017-01-10T23:19:21.752181: step 4403, loss 0.0668812, acc 0.976562\n",
      "2017-01-10T23:19:23.755650: step 4404, loss 0.0489462, acc 0.984375\n",
      "2017-01-10T23:19:25.822700: step 4405, loss 0.0280268, acc 0.984375\n",
      "2017-01-10T23:19:27.861233: step 4406, loss 0.0753451, acc 0.976562\n",
      "2017-01-10T23:19:29.865269: step 4407, loss 0.0280719, acc 0.992188\n",
      "2017-01-10T23:19:31.904548: step 4408, loss 0.0155207, acc 1\n",
      "2017-01-10T23:19:33.944652: step 4409, loss 0.247854, acc 0.960938\n",
      "2017-01-10T23:19:35.947424: step 4410, loss 0.0472442, acc 0.984375\n",
      "2017-01-10T23:19:38.010560: step 4411, loss 0.0163622, acc 1\n",
      "2017-01-10T23:19:40.029397: step 4412, loss 0.0558708, acc 0.984375\n",
      "2017-01-10T23:19:42.058409: step 4413, loss 0.0452319, acc 0.984375\n",
      "2017-01-10T23:19:44.099736: step 4414, loss 0.0460672, acc 0.992188\n",
      "2017-01-10T23:19:46.473985: step 4415, loss 0.00616344, acc 1\n",
      "2017-01-10T23:19:48.520511: step 4416, loss 0.09113, acc 0.976562\n",
      "2017-01-10T23:19:50.590499: step 4417, loss 0.0272029, acc 0.984375\n",
      "2017-01-10T23:19:52.645372: step 4418, loss 0.0530874, acc 0.976562\n",
      "2017-01-10T23:19:54.687467: step 4419, loss 0.0674811, acc 0.976562\n",
      "2017-01-10T23:19:56.711490: step 4420, loss 0.0142792, acc 0.992188\n",
      "2017-01-10T23:19:58.769513: step 4421, loss 0.0408353, acc 0.992188\n",
      "2017-01-10T23:20:00.799466: step 4422, loss 0.0246851, acc 0.992188\n",
      "2017-01-10T23:20:02.843355: step 4423, loss 0.0198302, acc 0.992188\n",
      "2017-01-10T23:20:04.975724: step 4424, loss 0.0242242, acc 0.992188\n",
      "2017-01-10T23:20:06.988381: step 4425, loss 0.0259653, acc 1\n",
      "2017-01-10T23:20:09.009992: step 4426, loss 0.0061154, acc 1\n",
      "2017-01-10T23:20:11.025267: step 4427, loss 0.0553919, acc 0.984375\n",
      "2017-01-10T23:20:13.070010: step 4428, loss 0.0557718, acc 0.976562\n",
      "2017-01-10T23:20:15.086600: step 4429, loss 0.150541, acc 0.960938\n",
      "2017-01-10T23:20:17.165798: step 4430, loss 0.0621069, acc 0.976562\n",
      "2017-01-10T23:20:19.421995: step 4431, loss 0.0404502, acc 0.992188\n",
      "2017-01-10T23:20:21.474980: step 4432, loss 0.0693157, acc 0.976562\n",
      "2017-01-10T23:20:23.505341: step 4433, loss 0.0695465, acc 0.96875\n",
      "2017-01-10T23:20:25.526144: step 4434, loss 0.090602, acc 0.984375\n",
      "2017-01-10T23:20:27.566742: step 4435, loss 0.10571, acc 0.96875\n",
      "2017-01-10T23:20:29.813474: step 4436, loss 0.0417893, acc 0.984375\n",
      "2017-01-10T23:20:31.834954: step 4437, loss 0.0101874, acc 0.992188\n",
      "2017-01-10T23:20:33.875867: step 4438, loss 0.0459451, acc 0.984375\n",
      "2017-01-10T23:20:35.960505: step 4439, loss 0.0852216, acc 0.976562\n",
      "2017-01-10T23:20:38.023867: step 4440, loss 0.0287775, acc 0.992188\n",
      "2017-01-10T23:20:40.056773: step 4441, loss 0.0394743, acc 0.984375\n",
      "2017-01-10T23:20:42.089120: step 4442, loss 0.0728868, acc 0.96875\n",
      "2017-01-10T23:20:44.618912: step 4443, loss 0.0261159, acc 1\n",
      "2017-01-10T23:20:47.252979: step 4444, loss 0.147804, acc 0.953125\n",
      "2017-01-10T23:20:49.348714: step 4445, loss 0.121677, acc 0.960938\n",
      "2017-01-10T23:20:51.635255: step 4446, loss 0.0573461, acc 0.976562\n",
      "2017-01-10T23:20:53.678539: step 4447, loss 0.0756513, acc 0.984375\n",
      "2017-01-10T23:20:55.700579: step 4448, loss 0.0868811, acc 0.984375\n",
      "2017-01-10T23:20:57.720641: step 4449, loss 0.0488777, acc 0.984375\n",
      "2017-01-10T23:20:59.753155: step 4450, loss 0.0740735, acc 0.976562\n",
      "2017-01-10T23:21:01.815069: step 4451, loss 0.0403065, acc 0.984375\n",
      "2017-01-10T23:21:03.847773: step 4452, loss 0.0386049, acc 0.992188\n",
      "2017-01-10T23:21:05.917220: step 4453, loss 0.0168222, acc 0.992188\n",
      "2017-01-10T23:21:07.938236: step 4454, loss 0.137552, acc 0.953125\n",
      "2017-01-10T23:21:09.956504: step 4455, loss 0.0867499, acc 0.96875\n",
      "2017-01-10T23:21:11.995658: step 4456, loss 0.083252, acc 0.984375\n",
      "2017-01-10T23:21:14.029452: step 4457, loss 0.0848699, acc 0.976562\n",
      "2017-01-10T23:21:16.072814: step 4458, loss 0.0636985, acc 0.976562\n",
      "2017-01-10T23:21:18.103383: step 4459, loss 0.205708, acc 0.960938\n",
      "2017-01-10T23:21:20.159167: step 4460, loss 0.0837114, acc 0.984375\n",
      "2017-01-10T23:21:22.438775: step 4461, loss 0.0772532, acc 0.96875\n",
      "2017-01-10T23:21:24.520592: step 4462, loss 0.0339006, acc 0.992188\n",
      "2017-01-10T23:21:26.529020: step 4463, loss 0.0830059, acc 0.976562\n",
      "2017-01-10T23:21:28.722024: step 4464, loss 0.0263103, acc 1\n",
      "2017-01-10T23:21:30.764333: step 4465, loss 0.0542791, acc 0.984375\n",
      "2017-01-10T23:21:32.789193: step 4466, loss 0.0668628, acc 0.984375\n",
      "2017-01-10T23:21:34.872022: step 4467, loss 0.102004, acc 0.96875\n",
      "2017-01-10T23:21:36.918156: step 4468, loss 0.0258478, acc 0.992188\n",
      "2017-01-10T23:21:38.953610: step 4469, loss 0.0656103, acc 0.96875\n",
      "2017-01-10T23:21:41.083739: step 4470, loss 0.0410819, acc 0.984375\n",
      "2017-01-10T23:21:43.052570: step 4471, loss 0.120741, acc 0.96875\n",
      "2017-01-10T23:21:45.968645: step 4472, loss 0.0187969, acc 0.992188\n",
      "2017-01-10T23:21:48.174929: step 4473, loss 0.0717246, acc 0.96875\n",
      "2017-01-10T23:21:50.245754: step 4474, loss 0.0112861, acc 1\n",
      "2017-01-10T23:21:52.317969: step 4475, loss 0.00635144, acc 1\n",
      "2017-01-10T23:21:54.736129: step 4476, loss 0.00544473, acc 1\n",
      "2017-01-10T23:21:56.760418: step 4477, loss 0.063683, acc 0.976562\n",
      "2017-01-10T23:21:58.798887: step 4478, loss 0.0666064, acc 0.984375\n",
      "2017-01-10T23:22:00.812029: step 4479, loss 0.0414746, acc 0.984375\n",
      "2017-01-10T23:22:02.888776: step 4480, loss 0.064237, acc 0.976562\n",
      "2017-01-10T23:22:04.984105: step 4481, loss 0.0477585, acc 0.984375\n",
      "2017-01-10T23:22:06.981285: step 4482, loss 0.049926, acc 0.984375\n",
      "2017-01-10T23:22:09.036518: step 4483, loss 0.037364, acc 0.992188\n",
      "2017-01-10T23:22:11.032787: step 4484, loss 0.0402023, acc 0.984375\n",
      "2017-01-10T23:22:13.083316: step 4485, loss 0.0765529, acc 0.96875\n",
      "2017-01-10T23:22:15.103093: step 4486, loss 0.0181682, acc 0.992188\n",
      "2017-01-10T23:22:17.127328: step 4487, loss 0.0612524, acc 0.984375\n",
      "2017-01-10T23:22:19.172087: step 4488, loss 0.0307716, acc 0.992188\n",
      "2017-01-10T23:22:21.229942: step 4489, loss 0.0890642, acc 0.976562\n",
      "2017-01-10T23:22:23.237704: step 4490, loss 0.112379, acc 0.953125\n",
      "2017-01-10T23:22:25.297612: step 4491, loss 0.0534242, acc 0.984375\n",
      "2017-01-10T23:22:27.519876: step 4492, loss 0.00933584, acc 1\n",
      "2017-01-10T23:22:29.562762: step 4493, loss 0.0421653, acc 0.984375\n",
      "2017-01-10T23:22:31.668549: step 4494, loss 0.111339, acc 0.96875\n",
      "2017-01-10T23:22:33.700796: step 4495, loss 0.0895441, acc 0.960938\n",
      "2017-01-10T23:22:36.003211: step 4496, loss 0.0649227, acc 0.984375\n",
      "2017-01-10T23:22:38.010773: step 4497, loss 0.0472019, acc 0.992188\n",
      "2017-01-10T23:22:40.030297: step 4498, loss 0.086456, acc 0.96875\n",
      "2017-01-10T23:22:42.172093: step 4499, loss 0.104079, acc 0.96875\n",
      "2017-01-10T23:22:44.232674: step 4500, loss 0.0421942, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:23:14.986724: step 4500, loss 0.0749577, acc 0.97848\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4500\n",
      "\n",
      "2017-01-10T23:23:20.000892: step 4501, loss 0.038902, acc 0.992188\n",
      "2017-01-10T23:23:22.026147: step 4502, loss 0.0441852, acc 0.984375\n",
      "2017-01-10T23:23:24.087759: step 4503, loss 0.0353803, acc 0.984375\n",
      "2017-01-10T23:23:26.174367: step 4504, loss 0.0892834, acc 0.96875\n",
      "2017-01-10T23:23:28.211979: step 4505, loss 0.0317714, acc 0.984375\n",
      "2017-01-10T23:23:30.214914: step 4506, loss 0.0490147, acc 0.984375\n",
      "2017-01-10T23:23:32.611188: step 4507, loss 0.106652, acc 0.96875\n",
      "2017-01-10T23:23:34.664050: step 4508, loss 0.0694796, acc 0.984375\n",
      "2017-01-10T23:23:36.703476: step 4509, loss 0.115883, acc 0.953125\n",
      "2017-01-10T23:23:38.729275: step 4510, loss 0.0595798, acc 0.976562\n",
      "2017-01-10T23:23:40.732211: step 4511, loss 0.13796, acc 0.945312\n",
      "2017-01-10T23:23:42.827272: step 4512, loss 0.041346, acc 0.976562\n",
      "2017-01-10T23:23:44.824609: step 4513, loss 0.00165932, acc 1\n",
      "2017-01-10T23:23:46.862474: step 4514, loss 0.0879316, acc 0.976562\n",
      "2017-01-10T23:23:48.914768: step 4515, loss 0.264147, acc 0.96875\n",
      "2017-01-10T23:23:51.033057: step 4516, loss 0.0469639, acc 0.984375\n",
      "2017-01-10T23:23:53.077056: step 4517, loss 0.0419133, acc 0.992188\n",
      "2017-01-10T23:23:55.144868: step 4518, loss 0.0468919, acc 0.984375\n",
      "2017-01-10T23:23:57.165629: step 4519, loss 0.0362581, acc 0.984375\n",
      "2017-01-10T23:23:59.197968: step 4520, loss 0.0342917, acc 0.976562\n",
      "2017-01-10T23:24:01.248567: step 4521, loss 0.0627714, acc 0.984375\n",
      "2017-01-10T23:24:03.446257: step 4522, loss 0.0722971, acc 0.984375\n",
      "2017-01-10T23:24:05.585099: step 4523, loss 0.0132104, acc 1\n",
      "2017-01-10T23:24:07.632361: step 4524, loss 0.0645629, acc 0.976562\n",
      "2017-01-10T23:24:09.689107: step 4525, loss 0.0744824, acc 0.976562\n",
      "2017-01-10T23:24:11.777303: step 4526, loss 0.112322, acc 0.960938\n",
      "2017-01-10T23:24:13.816023: step 4527, loss 0.0692271, acc 0.976562\n",
      "2017-01-10T23:24:15.859928: step 4528, loss 0.0707712, acc 0.984375\n",
      "2017-01-10T23:24:17.899358: step 4529, loss 0.0905415, acc 0.96875\n",
      "2017-01-10T23:24:19.949121: step 4530, loss 0.0477019, acc 0.96875\n",
      "2017-01-10T23:24:22.048347: step 4531, loss 0.0399588, acc 0.984375\n",
      "2017-01-10T23:24:24.065121: step 4532, loss 0.0562831, acc 0.96875\n",
      "2017-01-10T23:24:26.112253: step 4533, loss 0.0105185, acc 1\n",
      "2017-01-10T23:24:28.173129: step 4534, loss 0.0305174, acc 0.984375\n",
      "2017-01-10T23:24:30.273654: step 4535, loss 0.0466252, acc 0.976562\n",
      "2017-01-10T23:24:32.300432: step 4536, loss 0.0747216, acc 0.960938\n",
      "2017-01-10T23:24:34.338585: step 4537, loss 0.00886941, acc 1\n",
      "2017-01-10T23:24:36.732398: step 4538, loss 0.0541875, acc 0.976562\n",
      "2017-01-10T23:24:38.756648: step 4539, loss 0.140864, acc 0.960938\n",
      "2017-01-10T23:24:40.775918: step 4540, loss 0.0155195, acc 0.992188\n",
      "2017-01-10T23:24:42.907489: step 4541, loss 0.00537464, acc 1\n",
      "2017-01-10T23:24:44.924182: step 4542, loss 0.0219899, acc 0.992188\n",
      "2017-01-10T23:24:46.946086: step 4543, loss 0.0963601, acc 0.96875\n",
      "2017-01-10T23:24:49.000436: step 4544, loss 0.0275861, acc 0.992188\n",
      "2017-01-10T23:24:51.078675: step 4545, loss 0.0705768, acc 0.976562\n",
      "2017-01-10T23:24:53.096367: step 4546, loss 0.0404144, acc 0.992188\n",
      "2017-01-10T23:24:55.113865: step 4547, loss 0.0336271, acc 0.984375\n",
      "2017-01-10T23:24:57.172737: step 4548, loss 0.0566565, acc 0.984375\n",
      "2017-01-10T23:24:59.189654: step 4549, loss 0.0565378, acc 0.96875\n",
      "2017-01-10T23:25:01.229840: step 4550, loss 0.0347525, acc 0.984375\n",
      "2017-01-10T23:25:03.260106: step 4551, loss 0.0198747, acc 0.992188\n",
      "2017-01-10T23:25:05.355012: step 4552, loss 0.0913841, acc 0.976562\n",
      "2017-01-10T23:25:07.534533: step 4553, loss 0.0349642, acc 0.976562\n",
      "2017-01-10T23:25:09.630111: step 4554, loss 0.127189, acc 0.945312\n",
      "2017-01-10T23:25:11.709683: step 4555, loss 0.0344224, acc 0.984375\n",
      "2017-01-10T23:25:13.721611: step 4556, loss 0.150909, acc 0.960938\n",
      "2017-01-10T23:25:15.710909: step 4557, loss 0.108667, acc 0.96875\n",
      "2017-01-10T23:25:17.744835: step 4558, loss 0.00309455, acc 1\n",
      "2017-01-10T23:25:19.784449: step 4559, loss 0.106567, acc 0.960938\n",
      "2017-01-10T23:25:21.856442: step 4560, loss 0.0127665, acc 0.992188\n",
      "2017-01-10T23:25:23.894222: step 4561, loss 0.0406971, acc 0.984375\n",
      "2017-01-10T23:25:25.913556: step 4562, loss 0.114229, acc 0.96875\n",
      "2017-01-10T23:25:27.975907: step 4563, loss 0.0624214, acc 0.976562\n",
      "2017-01-10T23:25:30.273517: step 4564, loss 0.0194261, acc 0.992188\n",
      "2017-01-10T23:25:32.308220: step 4565, loss 0.0047088, acc 1\n",
      "2017-01-10T23:25:34.344829: step 4566, loss 0.140169, acc 0.953125\n",
      "2017-01-10T23:25:36.410830: step 4567, loss 0.0524879, acc 0.976562\n",
      "2017-01-10T23:25:38.463349: step 4568, loss 0.0163232, acc 0.992188\n",
      "2017-01-10T23:25:40.853074: step 4569, loss 0.0549833, acc 0.992188\n",
      "2017-01-10T23:25:42.965444: step 4570, loss 0.0274193, acc 0.992188\n",
      "2017-01-10T23:25:45.688930: step 4571, loss 0.0817539, acc 0.96875\n",
      "2017-01-10T23:25:48.027126: step 4572, loss 0.0298114, acc 0.984375\n",
      "2017-01-10T23:25:50.053628: step 4573, loss 0.0230251, acc 1\n",
      "2017-01-10T23:25:52.122048: step 4574, loss 0.0384433, acc 0.992188\n",
      "2017-01-10T23:25:54.154493: step 4575, loss 0.0676206, acc 0.984375\n",
      "2017-01-10T23:25:56.175209: step 4576, loss 0.0612657, acc 0.984375\n",
      "2017-01-10T23:25:58.198219: step 4577, loss 0.024384, acc 0.992188\n",
      "2017-01-10T23:26:00.234418: step 4578, loss 0.0898449, acc 0.976562\n",
      "2017-01-10T23:26:02.297670: step 4579, loss 0.0391297, acc 0.984375\n",
      "2017-01-10T23:26:04.376208: step 4580, loss 0.0681061, acc 0.96875\n",
      "2017-01-10T23:26:06.425705: step 4581, loss 0.0876508, acc 0.96875\n",
      "2017-01-10T23:26:08.447985: step 4582, loss 0.0258966, acc 0.992188\n",
      "2017-01-10T23:26:10.489937: step 4583, loss 0.0921203, acc 0.984375\n",
      "2017-01-10T23:26:12.881837: step 4584, loss 0.0651453, acc 0.976562\n",
      "2017-01-10T23:26:14.950370: step 4585, loss 0.10753, acc 0.976562\n",
      "2017-01-10T23:26:17.011177: step 4586, loss 0.0366157, acc 0.984375\n",
      "2017-01-10T23:26:19.038257: step 4587, loss 0.0587068, acc 0.984375\n",
      "2017-01-10T23:26:21.157059: step 4588, loss 0.0723434, acc 0.976562\n",
      "2017-01-10T23:26:23.187539: step 4589, loss 0.0235523, acc 0.992188\n",
      "2017-01-10T23:26:25.193505: step 4590, loss 0.0577908, acc 0.984375\n",
      "2017-01-10T23:26:27.227580: step 4591, loss 0.023615, acc 0.992188\n",
      "2017-01-10T23:26:29.257623: step 4592, loss 0.0571405, acc 0.984375\n",
      "2017-01-10T23:26:31.366628: step 4593, loss 0.0370926, acc 0.992188\n",
      "2017-01-10T23:26:33.718946: step 4594, loss 0.0616184, acc 0.984375\n",
      "2017-01-10T23:26:36.254340: step 4595, loss 0.0513721, acc 0.984375\n",
      "2017-01-10T23:26:38.369934: step 4596, loss 0.0996468, acc 0.953125\n",
      "2017-01-10T23:26:40.631586: step 4597, loss 0.0162731, acc 0.992188\n",
      "2017-01-10T23:26:42.730850: step 4598, loss 0.0546687, acc 0.984375\n",
      "2017-01-10T23:26:45.110677: step 4599, loss 0.0277093, acc 0.992188\n",
      "2017-01-10T23:26:47.184822: step 4600, loss 0.0382505, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:27:13.536624: step 4600, loss 0.0742585, acc 0.97868\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4600\n",
      "\n",
      "2017-01-10T23:27:18.387852: step 4601, loss 0.0462639, acc 0.984375\n",
      "2017-01-10T23:27:20.440389: step 4602, loss 0.0539903, acc 0.992188\n",
      "2017-01-10T23:27:22.493930: step 4603, loss 0.0642556, acc 0.992188\n",
      "2017-01-10T23:27:24.514953: step 4604, loss 0.0662762, acc 0.976562\n",
      "2017-01-10T23:27:26.516751: step 4605, loss 0.14742, acc 0.945312\n",
      "2017-01-10T23:27:28.576974: step 4606, loss 0.0767628, acc 0.96875\n",
      "2017-01-10T23:27:30.609065: step 4607, loss 0.0298752, acc 0.992188\n",
      "2017-01-10T23:27:32.659428: step 4608, loss 0.0775415, acc 0.976562\n",
      "2017-01-10T23:27:34.717064: step 4609, loss 0.105745, acc 0.976562\n",
      "2017-01-10T23:27:36.747905: step 4610, loss 0.0438336, acc 0.984375\n",
      "2017-01-10T23:27:38.736266: step 4611, loss 0.0105544, acc 0.992188\n",
      "2017-01-10T23:27:40.935452: step 4612, loss 0.0383302, acc 0.992188\n",
      "2017-01-10T23:27:42.999390: step 4613, loss 0.0454553, acc 0.984375\n",
      "2017-01-10T23:27:45.015246: step 4614, loss 0.0531659, acc 0.976562\n",
      "2017-01-10T23:27:47.049006: step 4615, loss 0.0741038, acc 0.976562\n",
      "2017-01-10T23:27:49.261127: step 4616, loss 0.237152, acc 0.976562\n",
      "2017-01-10T23:27:51.480802: step 4617, loss 0.0553689, acc 0.984375\n",
      "2017-01-10T23:27:53.542454: step 4618, loss 0.079193, acc 0.984375\n",
      "2017-01-10T23:27:55.613929: step 4619, loss 0.0740214, acc 0.976562\n",
      "2017-01-10T23:27:58.542663: step 4620, loss 0.0360635, acc 0.992188\n",
      "2017-01-10T23:28:00.750017: step 4621, loss 0.121038, acc 0.976562\n",
      "2017-01-10T23:28:02.789316: step 4622, loss 0.0752599, acc 0.96875\n",
      "2017-01-10T23:28:04.863858: step 4623, loss 0.0111436, acc 1\n",
      "2017-01-10T23:28:06.920605: step 4624, loss 0.0989562, acc 0.960938\n",
      "2017-01-10T23:28:08.973519: step 4625, loss 0.0556058, acc 0.976562\n",
      "2017-01-10T23:28:10.985421: step 4626, loss 0.0943067, acc 0.96875\n",
      "2017-01-10T23:28:12.995903: step 4627, loss 0.0795616, acc 0.96875\n",
      "2017-01-10T23:28:15.138508: step 4628, loss 0.00081164, acc 1\n",
      "2017-01-10T23:28:17.171797: step 4629, loss 0.135363, acc 0.96875\n",
      "2017-01-10T23:28:19.200665: step 4630, loss 0.0333576, acc 0.992188\n",
      "2017-01-10T23:28:21.212998: step 4631, loss 0.0471236, acc 0.976562\n",
      "2017-01-10T23:28:23.465310: step 4632, loss 0.0599251, acc 0.984375\n",
      "2017-01-10T23:28:25.465675: step 4633, loss 0.0394256, acc 0.992188\n",
      "2017-01-10T23:28:27.502572: step 4634, loss 0.0734925, acc 0.976562\n",
      "2017-01-10T23:28:29.538130: step 4635, loss 0.0743803, acc 0.992188\n",
      "2017-01-10T23:28:31.631075: step 4636, loss 0.0584733, acc 0.984375\n",
      "2017-01-10T23:28:33.670784: step 4637, loss 0.0547881, acc 0.976562\n",
      "2017-01-10T23:28:35.782835: step 4638, loss 0.10779, acc 0.960938\n",
      "2017-01-10T23:28:37.812126: step 4639, loss 0.090444, acc 0.992188\n",
      "2017-01-10T23:28:39.853731: step 4640, loss 0.0637455, acc 0.984375\n",
      "2017-01-10T23:28:41.854477: step 4641, loss 0.0712884, acc 0.976562\n",
      "2017-01-10T23:28:43.983162: step 4642, loss 0.0148779, acc 1\n",
      "2017-01-10T23:28:46.054470: step 4643, loss 0.0535196, acc 0.984375\n",
      "2017-01-10T23:28:48.081132: step 4644, loss 0.0440542, acc 0.984375\n",
      "2017-01-10T23:28:50.144807: step 4645, loss 0.0724416, acc 0.96875\n",
      "2017-01-10T23:28:52.238878: step 4646, loss 0.0354015, acc 0.992188\n",
      "2017-01-10T23:28:54.510582: step 4647, loss 0.0052701, acc 1\n",
      "2017-01-10T23:28:56.579306: step 4648, loss 0.0524759, acc 0.976562\n",
      "2017-01-10T23:28:58.618489: step 4649, loss 0.048119, acc 0.984375\n",
      "2017-01-10T23:29:00.596395: step 4650, loss 0.0725913, acc 0.976562\n",
      "2017-01-10T23:29:02.611511: step 4651, loss 0.0278016, acc 0.992188\n",
      "2017-01-10T23:29:04.682520: step 4652, loss 0.059186, acc 0.992188\n",
      "2017-01-10T23:29:06.680196: step 4653, loss 0.0349673, acc 0.992188\n",
      "2017-01-10T23:29:08.727740: step 4654, loss 0.0461249, acc 0.976562\n",
      "2017-01-10T23:29:10.758302: step 4655, loss 0.0313349, acc 0.992188\n",
      "2017-01-10T23:29:12.800284: step 4656, loss 0.0401389, acc 0.992188\n",
      "2017-01-10T23:29:14.811089: step 4657, loss 0.026308, acc 1\n",
      "2017-01-10T23:29:16.867085: step 4658, loss 0.0843728, acc 0.960938\n",
      "2017-01-10T23:29:18.895626: step 4659, loss 0.100195, acc 0.96875\n",
      "2017-01-10T23:29:20.937520: step 4660, loss 0.0584052, acc 0.984375\n",
      "2017-01-10T23:29:22.964234: step 4661, loss 0.091109, acc 0.96875\n",
      "2017-01-10T23:29:24.962304: step 4662, loss 0.055997, acc 0.984375\n",
      "2017-01-10T23:29:27.370691: step 4663, loss 0.0131386, acc 1\n",
      "2017-01-10T23:29:29.369356: step 4664, loss 0.0665557, acc 0.96875\n",
      "2017-01-10T23:29:31.387477: step 4665, loss 0.0242014, acc 0.992188\n",
      "2017-01-10T23:29:33.426729: step 4666, loss 0.0444391, acc 0.984375\n",
      "2017-01-10T23:29:35.474110: step 4667, loss 0.0246322, acc 0.992188\n",
      "2017-01-10T23:29:37.517976: step 4668, loss 0.0578478, acc 0.992188\n",
      "2017-01-10T23:29:39.548244: step 4669, loss 0.0979455, acc 0.96875\n",
      "2017-01-10T23:29:41.540738: step 4670, loss 0.0734541, acc 0.96875\n",
      "2017-01-10T23:29:43.615835: step 4671, loss 0.0274342, acc 0.992188\n",
      "2017-01-10T23:29:45.623809: step 4672, loss 0.0679275, acc 0.976562\n",
      "2017-01-10T23:29:47.655184: step 4673, loss 0.0332217, acc 0.992188\n",
      "2017-01-10T23:29:49.701472: step 4674, loss 0.0175811, acc 0.992188\n",
      "2017-01-10T23:29:51.752899: step 4675, loss 0.0638174, acc 0.976562\n",
      "2017-01-10T23:29:53.827950: step 4676, loss 0.0673156, acc 0.984375\n",
      "2017-01-10T23:29:55.879835: step 4677, loss 0.0421073, acc 0.992188\n",
      "2017-01-10T23:29:58.046218: step 4678, loss 0.0651382, acc 0.96875\n",
      "2017-01-10T23:30:00.244750: step 4679, loss 0.121334, acc 0.976562\n",
      "2017-01-10T23:30:02.274958: step 4680, loss 0.061324, acc 0.976562\n",
      "2017-01-10T23:30:04.295203: step 4681, loss 0.0413596, acc 0.992188\n",
      "2017-01-10T23:30:06.344943: step 4682, loss 0.0833832, acc 0.976562\n",
      "2017-01-10T23:30:08.379387: step 4683, loss 0.0440258, acc 0.984375\n",
      "2017-01-10T23:30:10.438598: step 4684, loss 0.0796646, acc 0.976562\n",
      "2017-01-10T23:30:12.480055: step 4685, loss 0.0479691, acc 0.992188\n",
      "2017-01-10T23:30:14.498027: step 4686, loss 0.0405801, acc 0.984375\n",
      "2017-01-10T23:30:16.515398: step 4687, loss 0.0385167, acc 0.984375\n",
      "2017-01-10T23:30:18.540397: step 4688, loss 0.10144, acc 0.96875\n",
      "2017-01-10T23:30:20.606237: step 4689, loss 0.0557704, acc 0.984375\n",
      "2017-01-10T23:30:22.679297: step 4690, loss 0.0541099, acc 0.976562\n",
      "2017-01-10T23:30:24.711128: step 4691, loss 0.0217571, acc 0.992188\n",
      "2017-01-10T23:30:26.751793: step 4692, loss 0.0831749, acc 0.96875\n",
      "2017-01-10T23:30:29.002028: step 4693, loss 0.0890193, acc 0.976562\n",
      "2017-01-10T23:30:31.400925: step 4694, loss 0.0484835, acc 0.992188\n",
      "2017-01-10T23:30:33.457628: step 4695, loss 0.0955452, acc 0.960938\n",
      "2017-01-10T23:30:35.520406: step 4696, loss 0.112066, acc 0.953125\n",
      "2017-01-10T23:30:37.550557: step 4697, loss 0.0924104, acc 0.960938\n",
      "2017-01-10T23:30:39.557590: step 4698, loss 0.0248596, acc 0.992188\n",
      "2017-01-10T23:30:41.587826: step 4699, loss 0.0837077, acc 0.976562\n",
      "2017-01-10T23:30:43.604198: step 4700, loss 0.0625033, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:31:06.397547: step 4700, loss 0.0733671, acc 0.9788\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4700\n",
      "\n",
      "2017-01-10T23:31:11.019531: step 4701, loss 0.0795662, acc 0.976562\n",
      "2017-01-10T23:31:13.083263: step 4702, loss 0.0104773, acc 0.992188\n",
      "2017-01-10T23:31:15.111683: step 4703, loss 0.155717, acc 0.945312\n",
      "2017-01-10T23:31:17.147198: step 4704, loss 0.065219, acc 0.984375\n",
      "2017-01-10T23:31:19.173370: step 4705, loss 0.0142296, acc 1\n",
      "2017-01-10T23:31:21.222977: step 4706, loss 0.0827713, acc 0.96875\n",
      "2017-01-10T23:31:23.271611: step 4707, loss 0.0561154, acc 0.984375\n",
      "2017-01-10T23:31:25.295238: step 4708, loss 0.107666, acc 0.96875\n",
      "2017-01-10T23:31:27.312363: step 4709, loss 0.0398025, acc 0.992188\n",
      "2017-01-10T23:31:29.347487: step 4710, loss 0.064572, acc 0.984375\n",
      "2017-01-10T23:31:31.394621: step 4711, loss 0.00534963, acc 1\n",
      "2017-01-10T23:31:33.441471: step 4712, loss 0.0451209, acc 0.984375\n",
      "2017-01-10T23:31:35.899639: step 4713, loss 0.060879, acc 0.984375\n",
      "2017-01-10T23:31:37.932066: step 4714, loss 0.0607032, acc 0.984375\n",
      "2017-01-10T23:31:40.062761: step 4715, loss 0.0564759, acc 0.984375\n",
      "2017-01-10T23:31:42.022558: step 4716, loss 0.188589, acc 0.945312\n",
      "2017-01-10T23:31:44.079676: step 4717, loss 0.0924124, acc 0.953125\n",
      "2017-01-10T23:31:46.119954: step 4718, loss 0.102881, acc 0.976562\n",
      "2017-01-10T23:31:48.150413: step 4719, loss 0.0107657, acc 1\n",
      "2017-01-10T23:31:50.212239: step 4720, loss 0.170011, acc 0.953125\n",
      "2017-01-10T23:31:52.277552: step 4721, loss 0.0460131, acc 0.992188\n",
      "2017-01-10T23:31:54.323187: step 4722, loss 0.0200708, acc 0.992188\n",
      "2017-01-10T23:31:57.286843: step 4723, loss 0.0353601, acc 0.984375\n",
      "2017-01-10T23:31:59.488176: step 4724, loss 0.137804, acc 0.976562\n",
      "2017-01-10T23:32:01.507827: step 4725, loss 0.0359039, acc 0.984375\n",
      "2017-01-10T23:32:03.544432: step 4726, loss 0.00122546, acc 1\n",
      "2017-01-10T23:32:05.616783: step 4727, loss 0.121218, acc 0.953125\n",
      "2017-01-10T23:32:08.028571: step 4728, loss 0.019932, acc 0.992188\n",
      "2017-01-10T23:32:10.068758: step 4729, loss 0.0119116, acc 1\n",
      "2017-01-10T23:32:12.121972: step 4730, loss 0.0244427, acc 0.992188\n",
      "2017-01-10T23:32:14.139497: step 4731, loss 0.268158, acc 0.960938\n",
      "2017-01-10T23:32:16.144328: step 4732, loss 0.0315783, acc 0.984375\n",
      "2017-01-10T23:32:18.169781: step 4733, loss 0.105676, acc 0.960938\n",
      "2017-01-10T23:32:20.252836: step 4734, loss 0.0923008, acc 0.976562\n",
      "2017-01-10T23:32:22.330424: step 4735, loss 0.100957, acc 0.960938\n",
      "2017-01-10T23:32:24.380437: step 4736, loss 0.0234514, acc 0.992188\n",
      "2017-01-10T23:32:26.403869: step 4737, loss 0.0295555, acc 0.992188\n",
      "2017-01-10T23:32:28.418679: step 4738, loss 0.110525, acc 0.96875\n",
      "2017-01-10T23:32:30.528713: step 4739, loss 0.066076, acc 0.96875\n",
      "2017-01-10T23:32:32.551277: step 4740, loss 0.10443, acc 0.960938\n",
      "2017-01-10T23:32:34.623491: step 4741, loss 0.0872477, acc 0.976562\n",
      "2017-01-10T23:32:36.635243: step 4742, loss 0.107637, acc 0.96875\n",
      "2017-01-10T23:32:38.905898: step 4743, loss 0.0498875, acc 0.984375\n",
      "2017-01-10T23:32:40.990965: step 4744, loss 0.0600294, acc 0.96875\n",
      "2017-01-10T23:32:43.031808: step 4745, loss 0.0423881, acc 0.984375\n",
      "2017-01-10T23:32:45.162510: step 4746, loss 0.0919724, acc 0.96875\n",
      "2017-01-10T23:32:47.391597: step 4747, loss 0.0986127, acc 0.976562\n",
      "2017-01-10T23:32:49.428550: step 4748, loss 0.0197047, acc 0.992188\n",
      "2017-01-10T23:32:51.493893: step 4749, loss 0.0703893, acc 0.984375\n",
      "2017-01-10T23:32:53.539359: step 4750, loss 0.0694391, acc 0.984375\n",
      "2017-01-10T23:32:55.556960: step 4751, loss 0.100478, acc 0.96875\n",
      "2017-01-10T23:32:57.604698: step 4752, loss 0.00991409, acc 0.992188\n",
      "2017-01-10T23:32:59.657009: step 4753, loss 0.0819075, acc 0.976562\n",
      "2017-01-10T23:33:01.698549: step 4754, loss 0.0522392, acc 0.984375\n",
      "2017-01-10T23:33:04.902340: step 4755, loss 0.0314754, acc 0.992188\n",
      "2017-01-10T23:33:06.918900: step 4756, loss 0.0915823, acc 0.976562\n",
      "2017-01-10T23:33:08.952589: step 4757, loss 0.124334, acc 0.96875\n",
      "2017-01-10T23:33:11.332918: step 4758, loss 0.04643, acc 0.992188\n",
      "2017-01-10T23:33:13.418974: step 4759, loss 0.0250896, acc 0.992188\n",
      "2017-01-10T23:33:15.424465: step 4760, loss 0.0267632, acc 1\n",
      "2017-01-10T23:33:17.469604: step 4761, loss 0.04049, acc 0.992188\n",
      "2017-01-10T23:33:19.530095: step 4762, loss 0.0511135, acc 0.992188\n",
      "2017-01-10T23:33:21.574156: step 4763, loss 0.113066, acc 0.953125\n",
      "2017-01-10T23:33:23.579415: step 4764, loss 0.0230802, acc 0.992188\n",
      "2017-01-10T23:33:25.594907: step 4765, loss 0.0223544, acc 0.992188\n",
      "2017-01-10T23:33:27.586321: step 4766, loss 0.0759003, acc 0.976562\n",
      "2017-01-10T23:33:29.621624: step 4767, loss 0.0966244, acc 0.976562\n",
      "2017-01-10T23:33:31.669826: step 4768, loss 0.0724039, acc 0.976562\n",
      "2017-01-10T23:33:33.688183: step 4769, loss 0.0693886, acc 0.96875\n",
      "2017-01-10T23:33:35.757662: step 4770, loss 0.0544508, acc 0.976562\n",
      "2017-01-10T23:33:37.764640: step 4771, loss 0.02377, acc 0.992188\n",
      "2017-01-10T23:33:39.780766: step 4772, loss 0.0776453, acc 0.96875\n",
      "2017-01-10T23:33:41.814383: step 4773, loss 0.00106799, acc 1\n",
      "2017-01-10T23:33:44.076324: step 4774, loss 0.0677793, acc 0.976562\n",
      "2017-01-10T23:33:46.176612: step 4775, loss 0.00511984, acc 1\n",
      "2017-01-10T23:33:48.163259: step 4776, loss 0.0283322, acc 0.992188\n",
      "2017-01-10T23:33:50.232359: step 4777, loss 0.0269582, acc 0.984375\n",
      "2017-01-10T23:33:52.308602: step 4778, loss 0.137552, acc 0.960938\n",
      "2017-01-10T23:33:54.355943: step 4779, loss 0.0487698, acc 0.984375\n",
      "2017-01-10T23:33:56.356279: step 4780, loss 0.0954035, acc 0.976562\n",
      "2017-01-10T23:33:58.395540: step 4781, loss 0.0621217, acc 0.984375\n",
      "2017-01-10T23:34:00.418087: step 4782, loss 0.0993761, acc 0.96875\n",
      "2017-01-10T23:34:02.468303: step 4783, loss 0.0982715, acc 0.976562\n",
      "2017-01-10T23:34:04.533860: step 4784, loss 0.0156654, acc 0.992188\n",
      "2017-01-10T23:34:06.574713: step 4785, loss 0.0621821, acc 0.96875\n",
      "2017-01-10T23:34:08.585255: step 4786, loss 0.0448463, acc 0.992188\n",
      "2017-01-10T23:34:10.624005: step 4787, loss 0.0863006, acc 0.984375\n",
      "2017-01-10T23:34:12.676421: step 4788, loss 0.0017672, acc 1\n",
      "2017-01-10T23:34:14.930369: step 4789, loss 0.0197998, acc 0.984375\n",
      "2017-01-10T23:34:16.946023: step 4790, loss 0.061714, acc 0.976562\n",
      "2017-01-10T23:34:18.996287: step 4791, loss 0.0248952, acc 0.992188\n",
      "2017-01-10T23:34:21.069515: step 4792, loss 0.00826253, acc 1\n",
      "2017-01-10T23:34:23.125428: step 4793, loss 0.124406, acc 0.953125\n",
      "2017-01-10T23:34:25.148747: step 4794, loss 0.0714653, acc 0.984375\n",
      "2017-01-10T23:34:27.199938: step 4795, loss 0.105969, acc 0.976562\n",
      "2017-01-10T23:34:29.226820: step 4796, loss 0.0335568, acc 1\n",
      "2017-01-10T23:34:31.484418: step 4797, loss 0.0794532, acc 0.96875\n",
      "2017-01-10T23:34:33.527374: step 4798, loss 0.0287522, acc 0.984375\n",
      "2017-01-10T23:34:35.573260: step 4799, loss 0.0783218, acc 0.960938\n",
      "2017-01-10T23:34:37.605685: step 4800, loss 0.00766604, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:35:02.991378: step 4800, loss 0.0724727, acc 0.97916\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4800\n",
      "\n",
      "2017-01-10T23:35:07.655807: step 4801, loss 0.0296126, acc 0.984375\n",
      "2017-01-10T23:35:09.700617: step 4802, loss 0.189778, acc 0.960938\n",
      "2017-01-10T23:35:11.732663: step 4803, loss 0.0861857, acc 0.976562\n",
      "2017-01-10T23:35:13.796924: step 4804, loss 0.0957047, acc 0.96875\n",
      "2017-01-10T23:35:15.836276: step 4805, loss 0.0539247, acc 0.96875\n",
      "2017-01-10T23:35:17.933667: step 4806, loss 0.0423636, acc 0.984375\n",
      "2017-01-10T23:35:20.230045: step 4807, loss 0.0823538, acc 0.976562\n",
      "2017-01-10T23:35:22.332312: step 4808, loss 0.111819, acc 0.96875\n",
      "2017-01-10T23:35:24.360631: step 4809, loss 0.125775, acc 0.953125\n",
      "2017-01-10T23:35:26.384505: step 4810, loss 0.0432402, acc 0.992188\n",
      "2017-01-10T23:35:28.526816: step 4811, loss 0.0400391, acc 0.992188\n",
      "2017-01-10T23:35:30.753520: step 4812, loss 0.0125244, acc 0.992188\n",
      "2017-01-10T23:35:32.773450: step 4813, loss 0.11891, acc 0.96875\n",
      "2017-01-10T23:35:34.863686: step 4814, loss 0.0551921, acc 0.984375\n",
      "2017-01-10T23:35:36.923336: step 4815, loss 0.0575139, acc 0.984375\n",
      "2017-01-10T23:35:38.953574: step 4816, loss 0.0829833, acc 0.96875\n",
      "2017-01-10T23:35:40.973681: step 4817, loss 0.0418091, acc 0.984375\n",
      "2017-01-10T23:35:42.978748: step 4818, loss 0.0715671, acc 0.984375\n",
      "2017-01-10T23:35:45.755053: step 4819, loss 0.0808788, acc 0.984375\n",
      "2017-01-10T23:35:48.200774: step 4820, loss 0.026272, acc 0.992188\n",
      "2017-01-10T23:35:50.366740: step 4821, loss 0.0216316, acc 0.992188\n",
      "2017-01-10T23:35:52.669827: step 4822, loss 0.102162, acc 0.960938\n",
      "2017-01-10T23:35:54.697868: step 4823, loss 0.0418619, acc 0.992188\n",
      "2017-01-10T23:35:56.720129: step 4824, loss 0.0984859, acc 0.976562\n",
      "2017-01-10T23:35:58.742287: step 4825, loss 0.012189, acc 1\n",
      "2017-01-10T23:36:00.829856: step 4826, loss 0.0314739, acc 0.992188\n",
      "2017-01-10T23:36:02.808588: step 4827, loss 0.0699019, acc 0.976562\n",
      "2017-01-10T23:36:04.878403: step 4828, loss 0.0368046, acc 0.984375\n",
      "2017-01-10T23:36:06.888777: step 4829, loss 0.0671648, acc 0.984375\n",
      "2017-01-10T23:36:08.955192: step 4830, loss 0.130323, acc 0.953125\n",
      "2017-01-10T23:36:10.995124: step 4831, loss 0.0221986, acc 0.992188\n",
      "2017-01-10T23:36:13.020762: step 4832, loss 0.0239735, acc 0.992188\n",
      "2017-01-10T23:36:15.062929: step 4833, loss 0.0299758, acc 1\n",
      "2017-01-10T23:36:17.109860: step 4834, loss 0.0759949, acc 0.984375\n",
      "2017-01-10T23:36:19.151517: step 4835, loss 0.0399573, acc 0.992188\n",
      "2017-01-10T23:36:21.238523: step 4836, loss 0.0486806, acc 0.984375\n",
      "2017-01-10T23:36:23.510762: step 4837, loss 0.122871, acc 0.960938\n",
      "2017-01-10T23:36:25.547693: step 4838, loss 0.0188662, acc 0.992188\n",
      "2017-01-10T23:36:27.557148: step 4839, loss 0.0839, acc 0.960938\n",
      "2017-01-10T23:36:29.588528: step 4840, loss 0.0325028, acc 0.992188\n",
      "2017-01-10T23:36:31.697440: step 4841, loss 0.115725, acc 0.96875\n",
      "2017-01-10T23:36:33.727228: step 4842, loss 0.127571, acc 0.992188\n",
      "2017-01-10T23:36:35.794573: step 4843, loss 0.0361189, acc 0.984375\n",
      "2017-01-10T23:36:37.824327: step 4844, loss 0.0264081, acc 0.992188\n",
      "2017-01-10T23:36:39.871436: step 4845, loss 0.00140659, acc 1\n",
      "2017-01-10T23:36:41.906679: step 4846, loss 0.0533965, acc 0.984375\n",
      "2017-01-10T23:36:43.925023: step 4847, loss 0.071865, acc 0.976562\n",
      "2017-01-10T23:36:46.281243: step 4848, loss 0.078708, acc 0.96875\n",
      "2017-01-10T23:36:48.318222: step 4849, loss 0.0715385, acc 0.96875\n",
      "2017-01-10T23:36:50.352106: step 4850, loss 0.0786466, acc 0.96875\n",
      "2017-01-10T23:36:52.416841: step 4851, loss 0.0120946, acc 1\n",
      "2017-01-10T23:36:54.561126: step 4852, loss 0.0912881, acc 0.96875\n",
      "2017-01-10T23:36:56.828144: step 4853, loss 0.0451771, acc 0.984375\n",
      "2017-01-10T23:36:58.868653: step 4854, loss 0.0676665, acc 0.992188\n",
      "2017-01-10T23:37:01.099195: step 4855, loss 0.105855, acc 0.96875\n",
      "2017-01-10T23:37:04.019991: step 4856, loss 0.110854, acc 0.960938\n",
      "2017-01-10T23:37:06.082651: step 4857, loss 0.014006, acc 0.992188\n",
      "2017-01-10T23:37:08.110885: step 4858, loss 0.00917575, acc 1\n",
      "2017-01-10T23:37:10.163866: step 4859, loss 0.0321198, acc 0.992188\n",
      "2017-01-10T23:37:12.207266: step 4860, loss 0.0941886, acc 0.976562\n",
      "2017-01-10T23:37:14.161990: step 4861, loss 0.00654987, acc 1\n",
      "2017-01-10T23:37:16.183406: step 4862, loss 0.0229752, acc 1\n",
      "2017-01-10T23:37:18.245062: step 4863, loss 0.0769773, acc 0.976562\n",
      "2017-01-10T23:37:20.267456: step 4864, loss 0.0412651, acc 0.984375\n",
      "2017-01-10T23:37:22.306117: step 4865, loss 0.0763621, acc 0.976562\n",
      "2017-01-10T23:37:24.334268: step 4866, loss 0.15806, acc 0.976562\n",
      "2017-01-10T23:37:26.356005: step 4867, loss 0.0825474, acc 0.976562\n",
      "2017-01-10T23:37:28.696402: step 4868, loss 0.059806, acc 0.96875\n",
      "2017-01-10T23:37:30.703957: step 4869, loss 0.0470596, acc 0.992188\n",
      "2017-01-10T23:37:32.716576: step 4870, loss 0.0193477, acc 0.992188\n",
      "2017-01-10T23:37:34.764917: step 4871, loss 0.075369, acc 0.976562\n",
      "2017-01-10T23:37:36.769024: step 4872, loss 0.0133728, acc 0.992188\n",
      "2017-01-10T23:37:38.810664: step 4873, loss 0.00926135, acc 1\n",
      "2017-01-10T23:37:40.833264: step 4874, loss 0.0869954, acc 0.96875\n",
      "2017-01-10T23:37:42.863705: step 4875, loss 0.0938646, acc 0.960938\n",
      "2017-01-10T23:37:44.848311: step 4876, loss 0.00907974, acc 1\n",
      "2017-01-10T23:37:46.971271: step 4877, loss 0.14535, acc 0.960938\n",
      "2017-01-10T23:37:49.013916: step 4878, loss 0.0812157, acc 0.976562\n",
      "2017-01-10T23:37:51.085007: step 4879, loss 0.0404137, acc 0.992188\n",
      "2017-01-10T23:37:53.279058: step 4880, loss 0.105281, acc 0.96875\n",
      "2017-01-10T23:37:55.302906: step 4881, loss 0.0373622, acc 0.992188\n",
      "2017-01-10T23:37:57.339139: step 4882, loss 0.0572295, acc 0.984375\n",
      "2017-01-10T23:37:59.572636: step 4883, loss 0.0152328, acc 0.992188\n",
      "2017-01-10T23:38:01.650552: step 4884, loss 0.109365, acc 0.984375\n",
      "2017-01-10T23:38:03.626535: step 4885, loss 0.0376805, acc 0.984375\n",
      "2017-01-10T23:38:05.690162: step 4886, loss 0.0507971, acc 0.976562\n",
      "2017-01-10T23:38:07.937531: step 4887, loss 0.038815, acc 0.984375\n",
      "2017-01-10T23:38:10.911814: step 4888, loss 0.0167964, acc 1\n",
      "2017-01-10T23:38:12.929052: step 4889, loss 0.0832148, acc 0.96875\n",
      "2017-01-10T23:38:14.964646: step 4890, loss 0.0693172, acc 0.976562\n",
      "2017-01-10T23:38:17.004899: step 4891, loss 0.0612579, acc 0.984375\n",
      "2017-01-10T23:38:19.046746: step 4892, loss 0.0309994, acc 0.992188\n",
      "2017-01-10T23:38:21.090924: step 4893, loss 0.0623847, acc 0.984375\n",
      "2017-01-10T23:38:23.159817: step 4894, loss 0.0712489, acc 0.976562\n",
      "2017-01-10T23:38:25.174199: step 4895, loss 0.0201352, acc 0.992188\n",
      "2017-01-10T23:38:27.181516: step 4896, loss 0.0465062, acc 0.984375\n",
      "2017-01-10T23:38:29.229362: step 4897, loss 0.0765108, acc 0.976562\n",
      "2017-01-10T23:38:31.610408: step 4898, loss 0.0470492, acc 0.984375\n",
      "2017-01-10T23:38:33.640444: step 4899, loss 0.0535478, acc 0.984375\n",
      "2017-01-10T23:38:35.748751: step 4900, loss 0.0702582, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:39:09.337357: step 4900, loss 0.074814, acc 0.97892\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-4900\n",
      "\n",
      "2017-01-10T23:39:14.643839: step 4901, loss 0.0227197, acc 1\n",
      "2017-01-10T23:39:16.667146: step 4902, loss 0.127255, acc 0.96875\n",
      "2017-01-10T23:39:18.648718: step 4903, loss 0.0537335, acc 0.992188\n",
      "2017-01-10T23:39:20.761227: step 4904, loss 0.0340182, acc 0.992188\n",
      "2017-01-10T23:39:22.893414: step 4905, loss 0.0386501, acc 0.984375\n",
      "2017-01-10T23:39:24.970813: step 4906, loss 0.0310051, acc 0.984375\n",
      "2017-01-10T23:39:27.110394: step 4907, loss 0.0377049, acc 0.992188\n",
      "2017-01-10T23:39:29.197370: step 4908, loss 0.0432644, acc 0.984375\n",
      "2017-01-10T23:39:31.252559: step 4909, loss 0.0510929, acc 0.976562\n",
      "2017-01-10T23:39:33.291674: step 4910, loss 0.035904, acc 0.992188\n",
      "2017-01-10T23:39:35.353824: step 4911, loss 0.0461496, acc 0.984375\n",
      "2017-01-10T23:39:37.365021: step 4912, loss 0.0624456, acc 0.976562\n",
      "2017-01-10T23:39:39.697214: step 4913, loss 0.0322226, acc 0.984375\n",
      "2017-01-10T23:39:41.952301: step 4914, loss 0.00449981, acc 1\n",
      "2017-01-10T23:39:43.945261: step 4915, loss 0.02823, acc 0.992188\n",
      "2017-01-10T23:39:45.989017: step 4916, loss 0.0297101, acc 0.992188\n",
      "2017-01-10T23:39:48.103476: step 4917, loss 0.0589047, acc 0.96875\n",
      "2017-01-10T23:39:50.138200: step 4918, loss 0.0893196, acc 0.984375\n",
      "2017-01-10T23:39:52.277236: step 4919, loss 0.0221356, acc 0.992188\n",
      "2017-01-10T23:39:54.894264: step 4920, loss 0.107901, acc 0.96875\n",
      "2017-01-10T23:39:57.559273: step 4921, loss 0.0844888, acc 0.96875\n",
      "2017-01-10T23:39:59.573689: step 4922, loss 0.0132574, acc 0.992188\n",
      "2017-01-10T23:40:01.592664: step 4923, loss 0.085285, acc 0.976562\n",
      "2017-01-10T23:40:03.690630: step 4924, loss 0.0131868, acc 1\n",
      "2017-01-10T23:40:05.810861: step 4925, loss 0.0760049, acc 0.96875\n",
      "2017-01-10T23:40:07.939814: step 4926, loss 0.0768963, acc 0.96875\n",
      "2017-01-10T23:40:10.042606: step 4927, loss 0.0109358, acc 1\n",
      "2017-01-10T23:40:12.433042: step 4928, loss 0.138718, acc 0.984375\n",
      "2017-01-10T23:40:14.443661: step 4929, loss 0.0265743, acc 0.984375\n",
      "2017-01-10T23:40:16.421070: step 4930, loss 0.029027, acc 0.992188\n",
      "2017-01-10T23:40:18.460463: step 4931, loss 0.0912412, acc 0.96875\n",
      "2017-01-10T23:40:20.522508: step 4932, loss 0.0745656, acc 0.976562\n",
      "2017-01-10T23:40:22.601551: step 4933, loss 0.106802, acc 0.96875\n",
      "2017-01-10T23:40:24.637260: step 4934, loss 0.0526547, acc 0.984375\n",
      "2017-01-10T23:40:26.665838: step 4935, loss 0.0948339, acc 0.976562\n",
      "2017-01-10T23:40:28.873711: step 4936, loss 0.0400327, acc 0.984375\n",
      "2017-01-10T23:40:31.019619: step 4937, loss 0.089576, acc 0.96875\n",
      "2017-01-10T23:40:33.046508: step 4938, loss 0.0631309, acc 0.984375\n",
      "2017-01-10T23:40:35.107514: step 4939, loss 0.016841, acc 1\n",
      "2017-01-10T23:40:37.134654: step 4940, loss 0.0707953, acc 0.976562\n",
      "2017-01-10T23:40:39.155890: step 4941, loss 0.0755032, acc 0.984375\n",
      "2017-01-10T23:40:41.209028: step 4942, loss 0.0325825, acc 0.992188\n",
      "2017-01-10T23:40:43.431780: step 4943, loss 0.0568917, acc 0.984375\n",
      "2017-01-10T23:40:46.505686: step 4944, loss 0.0801894, acc 0.976562\n",
      "2017-01-10T23:40:48.759506: step 4945, loss 0.0471792, acc 0.992188\n",
      "2017-01-10T23:40:50.790198: step 4946, loss 0.0532192, acc 0.976562\n",
      "2017-01-10T23:40:52.872930: step 4947, loss 0.0424957, acc 0.984375\n",
      "2017-01-10T23:40:54.905572: step 4948, loss 0.105606, acc 0.96875\n",
      "2017-01-10T23:40:56.990499: step 4949, loss 0.0198817, acc 0.992188\n",
      "2017-01-10T23:40:59.038789: step 4950, loss 0.0703095, acc 0.976562\n",
      "2017-01-10T23:41:01.104774: step 4951, loss 0.0874414, acc 0.960938\n",
      "2017-01-10T23:41:03.142464: step 4952, loss 0.00101665, acc 1\n",
      "2017-01-10T23:41:05.198285: step 4953, loss 0.100233, acc 0.96875\n",
      "2017-01-10T23:41:07.223494: step 4954, loss 0.104817, acc 0.96875\n",
      "2017-01-10T23:41:09.256990: step 4955, loss 0.00145738, acc 1\n",
      "2017-01-10T23:41:11.298588: step 4956, loss 0.0739586, acc 0.960938\n",
      "2017-01-10T23:41:13.317200: step 4957, loss 0.146357, acc 0.976562\n",
      "2017-01-10T23:41:15.497934: step 4958, loss 0.0247566, acc 0.992188\n",
      "2017-01-10T23:41:17.622376: step 4959, loss 0.0702932, acc 0.984375\n",
      "2017-01-10T23:41:19.683807: step 4960, loss 0.0359598, acc 0.984375\n",
      "2017-01-10T23:41:21.786936: step 4961, loss 0.0598052, acc 0.984375\n",
      "2017-01-10T23:41:23.793929: step 4962, loss 0.0213607, acc 0.992188\n",
      "2017-01-10T23:41:25.830558: step 4963, loss 0.0541938, acc 0.976562\n",
      "2017-01-10T23:41:27.870324: step 4964, loss 0.049037, acc 0.992188\n",
      "2017-01-10T23:41:29.910657: step 4965, loss 0.0646929, acc 0.992188\n",
      "2017-01-10T23:41:31.926500: step 4966, loss 0.046886, acc 0.992188\n",
      "2017-01-10T23:41:33.958222: step 4967, loss 0.0684606, acc 0.992188\n",
      "2017-01-10T23:41:36.017195: step 4968, loss 0.0739879, acc 0.976562\n",
      "2017-01-10T23:41:38.064267: step 4969, loss 0.0636299, acc 0.976562\n",
      "2017-01-10T23:41:40.076397: step 4970, loss 0.0441485, acc 0.976562\n",
      "2017-01-10T23:41:42.127875: step 4971, loss 0.0198861, acc 0.992188\n",
      "2017-01-10T23:41:44.153502: step 4972, loss 0.0503558, acc 0.992188\n",
      "2017-01-10T23:41:46.180419: step 4973, loss 0.00235669, acc 1\n",
      "2017-01-10T23:41:48.614269: step 4974, loss 0.0498907, acc 0.984375\n",
      "2017-01-10T23:41:50.678294: step 4975, loss 0.136389, acc 0.960938\n",
      "2017-01-10T23:41:52.931806: step 4976, loss 0.0663843, acc 0.976562\n",
      "2017-01-10T23:41:54.945274: step 4977, loss 0.0529887, acc 0.992188\n",
      "2017-01-10T23:41:56.999340: step 4978, loss 0.0266316, acc 0.984375\n",
      "2017-01-10T23:41:59.022448: step 4979, loss 0.0512136, acc 0.984375\n",
      "2017-01-10T23:42:01.075379: step 4980, loss 0.00378395, acc 1\n",
      "2017-01-10T23:42:03.124162: step 4981, loss 0.0845313, acc 0.960938\n",
      "2017-01-10T23:42:05.209215: step 4982, loss 0.0370458, acc 0.992188\n",
      "2017-01-10T23:42:07.513134: step 4983, loss 0.014718, acc 1\n",
      "2017-01-10T23:42:10.301826: step 4984, loss 0.0361504, acc 0.992188\n",
      "2017-01-10T23:42:12.372990: step 4985, loss 0.0463609, acc 0.984375\n",
      "2017-01-10T23:42:14.370266: step 4986, loss 0.0682945, acc 0.976562\n",
      "2017-01-10T23:42:16.370244: step 4987, loss 0.0516317, acc 0.96875\n",
      "2017-01-10T23:42:18.429575: step 4988, loss 0.0370401, acc 0.992188\n",
      "2017-01-10T23:42:20.859296: step 4989, loss 0.0304585, acc 0.992188\n",
      "2017-01-10T23:42:22.961746: step 4990, loss 0.027745, acc 0.992188\n",
      "2017-01-10T23:42:24.976291: step 4991, loss 0.0186982, acc 1\n",
      "2017-01-10T23:42:26.979461: step 4992, loss 0.0833137, acc 0.976562\n",
      "2017-01-10T23:42:28.997046: step 4993, loss 0.0359515, acc 0.992188\n",
      "2017-01-10T23:42:31.161282: step 4994, loss 0.0389595, acc 0.992188\n",
      "2017-01-10T23:42:33.198232: step 4995, loss 0.0730156, acc 0.976562\n",
      "2017-01-10T23:42:35.248494: step 4996, loss 0.0724113, acc 0.984375\n",
      "2017-01-10T23:42:37.275158: step 4997, loss 0.0594087, acc 0.992188\n",
      "2017-01-10T23:42:39.291867: step 4998, loss 0.0600359, acc 0.984375\n",
      "2017-01-10T23:42:41.307911: step 4999, loss 0.0232433, acc 0.992188\n",
      "2017-01-10T23:42:43.321750: step 5000, loss 0.0420687, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:43:07.283888: step 5000, loss 0.0735254, acc 0.97996\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5000\n",
      "\n",
      "2017-01-10T23:43:11.806932: step 5001, loss 0.0642935, acc 0.976562\n",
      "2017-01-10T23:43:14.006870: step 5002, loss 0.0726651, acc 0.984375\n",
      "2017-01-10T23:43:16.959430: step 5003, loss 0.0351187, acc 0.992188\n",
      "2017-01-10T23:43:18.992374: step 5004, loss 0.0715416, acc 0.976562\n",
      "2017-01-10T23:43:21.091388: step 5005, loss 0.0619781, acc 0.96875\n",
      "2017-01-10T23:43:23.159723: step 5006, loss 0.0449188, acc 0.976562\n",
      "2017-01-10T23:43:25.566881: step 5007, loss 0.0328066, acc 0.992188\n",
      "2017-01-10T23:43:27.655032: step 5008, loss 0.0423405, acc 0.992188\n",
      "2017-01-10T23:43:29.685237: step 5009, loss 0.0905801, acc 0.976562\n",
      "2017-01-10T23:43:31.706574: step 5010, loss 0.0982866, acc 0.96875\n",
      "2017-01-10T23:43:33.752004: step 5011, loss 0.0561763, acc 0.984375\n",
      "2017-01-10T23:43:35.785447: step 5012, loss 0.0217028, acc 1\n",
      "2017-01-10T23:43:37.829825: step 5013, loss 0.117911, acc 0.960938\n",
      "2017-01-10T23:43:39.857332: step 5014, loss 0.0481993, acc 0.984375\n",
      "2017-01-10T23:43:41.913935: step 5015, loss 0.0295263, acc 0.992188\n",
      "2017-01-10T23:43:43.953370: step 5016, loss 0.0915235, acc 0.96875\n",
      "2017-01-10T23:43:45.978693: step 5017, loss 0.0592264, acc 0.976562\n",
      "2017-01-10T23:43:48.042397: step 5018, loss 0.0936586, acc 0.976562\n",
      "2017-01-10T23:43:50.177586: step 5019, loss 0.0459952, acc 0.984375\n",
      "2017-01-10T23:43:52.273889: step 5020, loss 0.116248, acc 0.945312\n",
      "2017-01-10T23:43:54.281725: step 5021, loss 0.0374977, acc 0.992188\n",
      "2017-01-10T23:43:56.318452: step 5022, loss 0.0890643, acc 0.976562\n",
      "2017-01-10T23:43:58.699817: step 5023, loss 0.0411855, acc 0.984375\n",
      "2017-01-10T23:44:00.738313: step 5024, loss 0.00101177, acc 1\n",
      "2017-01-10T23:44:02.813223: step 5025, loss 0.0312201, acc 0.984375\n",
      "2017-01-10T23:44:04.849662: step 5026, loss 0.0609972, acc 0.984375\n",
      "2017-01-10T23:44:06.876387: step 5027, loss 0.0305051, acc 0.984375\n",
      "2017-01-10T23:44:08.924248: step 5028, loss 0.0728312, acc 0.976562\n",
      "2017-01-10T23:44:10.928170: step 5029, loss 0.0366727, acc 0.984375\n",
      "2017-01-10T23:44:12.943910: step 5030, loss 0.051291, acc 0.976562\n",
      "2017-01-10T23:44:14.962432: step 5031, loss 0.0627921, acc 0.992188\n",
      "2017-01-10T23:44:17.011519: step 5032, loss 0.0381242, acc 0.984375\n",
      "2017-01-10T23:44:19.049185: step 5033, loss 0.11322, acc 0.960938\n",
      "2017-01-10T23:44:21.145004: step 5034, loss 0.0383874, acc 0.992188\n",
      "2017-01-10T23:44:23.150505: step 5035, loss 0.0823634, acc 0.976562\n",
      "2017-01-10T23:44:25.202425: step 5036, loss 0.090708, acc 0.96875\n",
      "2017-01-10T23:44:27.269240: step 5037, loss 0.00504179, acc 1\n",
      "2017-01-10T23:44:29.410584: step 5038, loss 0.0481167, acc 0.976562\n",
      "2017-01-10T23:44:31.747791: step 5039, loss 0.0236049, acc 0.992188\n",
      "2017-01-10T23:44:33.767793: step 5040, loss 0.0101209, acc 1\n",
      "2017-01-10T23:44:35.834556: step 5041, loss 0.0704732, acc 0.976562\n",
      "2017-01-10T23:44:37.852688: step 5042, loss 0.0613015, acc 0.984375\n",
      "2017-01-10T23:44:39.872008: step 5043, loss 0.0546996, acc 0.976562\n",
      "2017-01-10T23:44:41.899797: step 5044, loss 0.115209, acc 0.960938\n",
      "2017-01-10T23:44:43.916143: step 5045, loss 0.0149536, acc 1\n",
      "2017-01-10T23:44:45.928869: step 5046, loss 0.0495093, acc 0.984375\n",
      "2017-01-10T23:44:47.989829: step 5047, loss 0.0303147, acc 0.984375\n",
      "2017-01-10T23:44:50.156441: step 5048, loss 0.0917523, acc 0.976562\n",
      "2017-01-10T23:44:52.215049: step 5049, loss 0.0545861, acc 0.984375\n",
      "2017-01-10T23:44:54.229531: step 5050, loss 0.152456, acc 0.992188\n",
      "2017-01-10T23:44:56.254145: step 5051, loss 0.0181534, acc 0.992188\n",
      "2017-01-10T23:44:58.267139: step 5052, loss 0.0331002, acc 0.984375\n",
      "2017-01-10T23:45:00.310246: step 5053, loss 0.015681, acc 1\n",
      "2017-01-10T23:45:02.696012: step 5054, loss 0.0483835, acc 0.976562\n",
      "2017-01-10T23:45:04.798205: step 5055, loss 0.0437738, acc 0.984375\n",
      "2017-01-10T23:45:06.795422: step 5056, loss 0.037347, acc 0.984375\n",
      "2017-01-10T23:45:08.820172: step 5057, loss 0.0160018, acc 0.992188\n",
      "2017-01-10T23:45:10.822760: step 5058, loss 0.0808921, acc 0.976562\n",
      "2017-01-10T23:45:12.862979: step 5059, loss 0.00671444, acc 1\n",
      "2017-01-10T23:45:14.909569: step 5060, loss 0.0376849, acc 0.984375\n",
      "2017-01-10T23:45:16.948515: step 5061, loss 0.130737, acc 0.976562\n",
      "2017-01-10T23:45:18.985215: step 5062, loss 0.060704, acc 0.984375\n",
      "2017-01-10T23:45:21.057222: step 5063, loss 0.0979447, acc 0.96875\n",
      "2017-01-10T23:45:23.083895: step 5064, loss 0.00226917, acc 1\n",
      "2017-01-10T23:45:25.105101: step 5065, loss 0.0505062, acc 0.976562\n",
      "2017-01-10T23:45:27.103070: step 5066, loss 0.0195961, acc 0.992188\n",
      "2017-01-10T23:45:29.414928: step 5067, loss 0.063028, acc 0.984375\n",
      "2017-01-10T23:45:31.453641: step 5068, loss 0.0995538, acc 0.96875\n",
      "2017-01-10T23:45:33.567615: step 5069, loss 0.0346464, acc 1\n",
      "2017-01-10T23:45:35.831866: step 5070, loss 0.0317644, acc 0.992188\n",
      "2017-01-10T23:45:37.848568: step 5071, loss 0.0285606, acc 0.992188\n",
      "2017-01-10T23:45:39.876248: step 5072, loss 0.0792495, acc 0.976562\n",
      "2017-01-10T23:45:41.899700: step 5073, loss 0.0630156, acc 0.984375\n",
      "2017-01-10T23:45:43.930559: step 5074, loss 0.0995952, acc 0.960938\n",
      "2017-01-10T23:45:46.880159: step 5075, loss 0.055311, acc 0.984375\n",
      "2017-01-10T23:45:49.033904: step 5076, loss 0.00229012, acc 1\n",
      "2017-01-10T23:45:51.303454: step 5077, loss 0.0303468, acc 0.992188\n",
      "2017-01-10T23:45:53.325268: step 5078, loss 0.0489544, acc 0.984375\n",
      "2017-01-10T23:45:55.345677: step 5079, loss 0.0476709, acc 0.984375\n",
      "2017-01-10T23:45:57.362590: step 5080, loss 0.0772565, acc 0.984375\n",
      "2017-01-10T23:45:59.406714: step 5081, loss 0.117096, acc 0.953125\n",
      "2017-01-10T23:46:01.450906: step 5082, loss 0.124062, acc 0.960938\n",
      "2017-01-10T23:46:03.507486: step 5083, loss 0.0394684, acc 0.992188\n",
      "2017-01-10T23:46:05.577627: step 5084, loss 0.0980175, acc 0.960938\n",
      "2017-01-10T23:46:07.955024: step 5085, loss 0.0180822, acc 0.992188\n",
      "2017-01-10T23:46:09.945946: step 5086, loss 0.0712697, acc 0.992188\n",
      "2017-01-10T23:46:12.063427: step 5087, loss 0.157674, acc 0.960938\n",
      "2017-01-10T23:46:14.309265: step 5088, loss 0.0181975, acc 0.992188\n",
      "2017-01-10T23:46:16.634972: step 5089, loss 0.0200206, acc 0.992188\n",
      "2017-01-10T23:46:18.710313: step 5090, loss 0.0434931, acc 0.984375\n",
      "2017-01-10T23:46:21.121808: step 5091, loss 0.0192987, acc 1\n",
      "2017-01-10T23:46:23.177511: step 5092, loss 0.05054, acc 0.984375\n",
      "2017-01-10T23:46:25.215204: step 5093, loss 0.000747253, acc 1\n",
      "2017-01-10T23:46:27.253788: step 5094, loss 0.0343786, acc 0.992188\n",
      "2017-01-10T23:46:29.316610: step 5095, loss 0.0352148, acc 0.984375\n",
      "2017-01-10T23:46:31.464870: step 5096, loss 0.0249389, acc 1\n",
      "2017-01-10T23:46:33.512413: step 5097, loss 0.0340102, acc 0.984375\n",
      "2017-01-10T23:46:35.581720: step 5098, loss 0.0269257, acc 0.984375\n",
      "2017-01-10T23:46:37.603620: step 5099, loss 0.0030596, acc 1\n",
      "2017-01-10T23:46:40.025060: step 5100, loss 0.00412803, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:47:01.494100: step 5100, loss 0.0729466, acc 0.97944\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5100\n",
      "\n",
      "2017-01-10T23:47:05.974217: step 5101, loss 0.0198661, acc 0.992188\n",
      "2017-01-10T23:47:07.994149: step 5102, loss 0.0486782, acc 0.984375\n",
      "2017-01-10T23:47:10.074928: step 5103, loss 0.0356622, acc 1\n",
      "2017-01-10T23:47:12.458840: step 5104, loss 0.0498773, acc 0.984375\n",
      "2017-01-10T23:47:15.438740: step 5105, loss 0.0203659, acc 0.992188\n",
      "2017-01-10T23:47:17.632749: step 5106, loss 0.036564, acc 0.984375\n",
      "2017-01-10T23:47:19.680401: step 5107, loss 0.0110235, acc 1\n",
      "2017-01-10T23:47:21.735404: step 5108, loss 0.00438171, acc 1\n",
      "2017-01-10T23:47:23.759890: step 5109, loss 0.199976, acc 0.953125\n",
      "2017-01-10T23:47:25.821459: step 5110, loss 0.0425441, acc 0.992188\n",
      "2017-01-10T23:47:27.900036: step 5111, loss 0.113993, acc 0.976562\n",
      "2017-01-10T23:47:29.939401: step 5112, loss 0.0102914, acc 0.992188\n",
      "2017-01-10T23:47:31.974198: step 5113, loss 0.0310089, acc 0.992188\n",
      "2017-01-10T23:47:33.984272: step 5114, loss 0.109604, acc 0.976562\n",
      "2017-01-10T23:47:36.102802: step 5115, loss 0.0706553, acc 0.976562\n",
      "2017-01-10T23:47:38.180517: step 5116, loss 0.121757, acc 0.960938\n",
      "2017-01-10T23:47:40.188418: step 5117, loss 0.087591, acc 0.992188\n",
      "2017-01-10T23:47:42.253549: step 5118, loss 0.0618721, acc 0.992188\n",
      "2017-01-10T23:47:44.656282: step 5119, loss 0.0584134, acc 0.976562\n",
      "2017-01-10T23:47:46.669326: step 5120, loss 0.0457535, acc 0.992188\n",
      "2017-01-10T23:47:48.680469: step 5121, loss 0.0369137, acc 0.992188\n",
      "2017-01-10T23:47:50.801754: step 5122, loss 0.0487882, acc 0.992188\n",
      "2017-01-10T23:47:52.890401: step 5123, loss 0.0209768, acc 0.992188\n",
      "2017-01-10T23:47:54.911519: step 5124, loss 0.0472277, acc 0.984375\n",
      "2017-01-10T23:47:56.959483: step 5125, loss 0.108933, acc 0.96875\n",
      "2017-01-10T23:47:58.977675: step 5126, loss 0.0240477, acc 0.992188\n",
      "2017-01-10T23:48:01.012210: step 5127, loss 0.12513, acc 0.976562\n",
      "2017-01-10T23:48:03.043476: step 5128, loss 0.0577914, acc 0.984375\n",
      "2017-01-10T23:48:05.325346: step 5129, loss 0.140211, acc 0.96875\n",
      "2017-01-10T23:48:07.346611: step 5130, loss 0.0548167, acc 0.984375\n",
      "2017-01-10T23:48:09.373784: step 5131, loss 0.048053, acc 0.984375\n",
      "2017-01-10T23:48:11.393079: step 5132, loss 0.0827069, acc 0.96875\n",
      "2017-01-10T23:48:13.393417: step 5133, loss 0.0388571, acc 0.984375\n",
      "2017-01-10T23:48:15.550697: step 5134, loss 0.0025796, acc 1\n",
      "2017-01-10T23:48:17.804948: step 5135, loss 0.032417, acc 0.984375\n",
      "2017-01-10T23:48:20.091518: step 5136, loss 0.110744, acc 0.953125\n",
      "2017-01-10T23:48:23.044146: step 5137, loss 0.0571775, acc 0.984375\n",
      "2017-01-10T23:48:25.079167: step 5138, loss 0.111067, acc 0.984375\n",
      "2017-01-10T23:48:27.144548: step 5139, loss 0.068194, acc 0.976562\n",
      "2017-01-10T23:48:29.176389: step 5140, loss 0.036325, acc 0.984375\n",
      "2017-01-10T23:48:31.341544: step 5141, loss 0.0409358, acc 0.992188\n",
      "2017-01-10T23:48:33.363532: step 5142, loss 0.0647707, acc 0.976562\n",
      "2017-01-10T23:48:35.425508: step 5143, loss 0.0353892, acc 0.992188\n",
      "2017-01-10T23:48:37.446584: step 5144, loss 0.0421634, acc 0.984375\n",
      "2017-01-10T23:48:39.494300: step 5145, loss 0.0423384, acc 0.984375\n",
      "2017-01-10T23:48:41.540160: step 5146, loss 0.0187074, acc 0.992188\n",
      "2017-01-10T23:48:43.548425: step 5147, loss 0.0562429, acc 0.984375\n",
      "2017-01-10T23:48:45.584424: step 5148, loss 0.0695158, acc 0.96875\n",
      "2017-01-10T23:48:47.674796: step 5149, loss 0.132767, acc 0.960938\n",
      "2017-01-10T23:48:49.956982: step 5150, loss 0.073972, acc 0.976562\n",
      "2017-01-10T23:48:52.126326: step 5151, loss 0.0475005, acc 0.992188\n",
      "2017-01-10T23:48:54.173514: step 5152, loss 0.0260254, acc 0.992188\n",
      "2017-01-10T23:48:56.177817: step 5153, loss 0.0717901, acc 0.984375\n",
      "2017-01-10T23:48:58.456139: step 5154, loss 0.0856355, acc 0.96875\n",
      "2017-01-10T23:49:00.487938: step 5155, loss 0.0537496, acc 0.976562\n",
      "2017-01-10T23:49:02.523522: step 5156, loss 0.0969383, acc 0.976562\n",
      "2017-01-10T23:49:04.601670: step 5157, loss 0.0534735, acc 0.976562\n",
      "2017-01-10T23:49:06.603993: step 5158, loss 0.127671, acc 0.960938\n",
      "2017-01-10T23:49:08.619379: step 5159, loss 0.0728232, acc 0.953125\n",
      "2017-01-10T23:49:10.647583: step 5160, loss 0.0661933, acc 0.976562\n",
      "2017-01-10T23:49:12.663214: step 5161, loss 0.00511563, acc 1\n",
      "2017-01-10T23:49:14.713324: step 5162, loss 0.00977438, acc 1\n",
      "2017-01-10T23:49:16.766469: step 5163, loss 0.0223956, acc 0.992188\n",
      "2017-01-10T23:49:18.775958: step 5164, loss 0.0168138, acc 0.992188\n",
      "2017-01-10T23:49:21.163201: step 5165, loss 0.0176268, acc 1\n",
      "2017-01-10T23:49:23.185060: step 5166, loss 0.0621348, acc 0.976562\n",
      "2017-01-10T23:49:25.224731: step 5167, loss 0.0854814, acc 0.984375\n",
      "2017-01-10T23:49:27.280044: step 5168, loss 0.0725408, acc 0.984375\n",
      "2017-01-10T23:49:29.299680: step 5169, loss 0.012597, acc 1\n",
      "2017-01-10T23:49:31.338593: step 5170, loss 0.073348, acc 0.976562\n",
      "2017-01-10T23:49:33.380030: step 5171, loss 0.0576697, acc 0.976562\n",
      "2017-01-10T23:49:35.400030: step 5172, loss 0.0667061, acc 0.976562\n",
      "2017-01-10T23:49:37.425115: step 5173, loss 0.0285178, acc 0.992188\n",
      "2017-01-10T23:49:39.435915: step 5174, loss 0.0403565, acc 0.984375\n",
      "2017-01-10T23:49:41.469002: step 5175, loss 0.0714022, acc 0.976562\n",
      "2017-01-10T23:49:43.493012: step 5176, loss 0.0439639, acc 0.984375\n",
      "2017-01-10T23:49:45.519954: step 5177, loss 0.07414, acc 0.976562\n",
      "2017-01-10T23:49:47.525762: step 5178, loss 0.0410441, acc 0.992188\n",
      "2017-01-10T23:49:49.603711: step 5179, loss 0.0737024, acc 0.992188\n",
      "2017-01-10T23:49:51.788486: step 5180, loss 0.031012, acc 0.992188\n",
      "2017-01-10T23:49:54.061123: step 5181, loss 0.0410517, acc 0.984375\n",
      "2017-01-10T23:49:56.059534: step 5182, loss 0.0304425, acc 0.984375\n",
      "2017-01-10T23:49:58.067098: step 5183, loss 0.0748969, acc 0.992188\n",
      "2017-01-10T23:50:00.095014: step 5184, loss 0.0270184, acc 0.992188\n",
      "2017-01-10T23:50:02.102080: step 5185, loss 0.0670108, acc 0.984375\n",
      "2017-01-10T23:50:04.120505: step 5186, loss 0.0468394, acc 0.992188\n",
      "2017-01-10T23:50:06.205288: step 5187, loss 0.0667304, acc 0.976562\n",
      "2017-01-10T23:50:08.214560: step 5188, loss 0.0332071, acc 0.992188\n",
      "2017-01-10T23:50:10.282840: step 5189, loss 0.0265451, acc 0.984375\n",
      "2017-01-10T23:50:12.307941: step 5190, loss 0.0935719, acc 0.960938\n",
      "2017-01-10T23:50:14.326292: step 5191, loss 0.0667106, acc 0.976562\n",
      "2017-01-10T23:50:16.334334: step 5192, loss 0.0652447, acc 0.976562\n",
      "2017-01-10T23:50:18.391574: step 5193, loss 0.0274795, acc 0.992188\n",
      "2017-01-10T23:50:20.513868: step 5194, loss 0.340479, acc 0.976562\n",
      "2017-01-10T23:50:22.583175: step 5195, loss 0.0552813, acc 0.984375\n",
      "2017-01-10T23:50:24.789317: step 5196, loss 0.0431014, acc 0.992188\n",
      "2017-01-10T23:50:26.927541: step 5197, loss 0.0703046, acc 0.976562\n",
      "2017-01-10T23:50:29.063051: step 5198, loss 0.0965004, acc 0.984375\n",
      "2017-01-10T23:50:31.213822: step 5199, loss 0.063196, acc 0.976562\n",
      "2017-01-10T23:50:33.225732: step 5200, loss 0.0431872, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:50:55.710490: step 5200, loss 0.0724847, acc 0.97936\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5200\n",
      "\n",
      "2017-01-10T23:51:00.327615: step 5201, loss 0.0437086, acc 0.992188\n",
      "2017-01-10T23:51:02.359215: step 5202, loss 0.135977, acc 0.96875\n",
      "2017-01-10T23:51:04.428457: step 5203, loss 0.0595747, acc 0.984375\n",
      "2017-01-10T23:51:06.464749: step 5204, loss 0.0309946, acc 1\n",
      "2017-01-10T23:51:08.515333: step 5205, loss 0.052402, acc 0.984375\n",
      "2017-01-10T23:51:10.559343: step 5206, loss 0.0928158, acc 0.96875\n",
      "2017-01-10T23:51:12.577811: step 5207, loss 0.0931897, acc 0.960938\n",
      "2017-01-10T23:51:14.629580: step 5208, loss 0.0486642, acc 0.984375\n",
      "2017-01-10T23:51:16.657455: step 5209, loss 0.137518, acc 0.96875\n",
      "2017-01-10T23:51:18.671853: step 5210, loss 0.0297784, acc 0.984375\n",
      "2017-01-10T23:51:20.721848: step 5211, loss 0.0794674, acc 0.96875\n",
      "2017-01-10T23:51:22.781739: step 5212, loss 0.0451702, acc 0.984375\n",
      "2017-01-10T23:51:24.786301: step 5213, loss 0.0868434, acc 0.96875\n",
      "2017-01-10T23:51:26.818752: step 5214, loss 0.0161431, acc 0.984375\n",
      "2017-01-10T23:51:29.000226: step 5215, loss 0.0308199, acc 0.992188\n",
      "2017-01-10T23:51:31.167479: step 5216, loss 0.00276619, acc 1\n",
      "2017-01-10T23:51:33.202688: step 5217, loss 0.00241081, acc 1\n",
      "2017-01-10T23:51:35.228397: step 5218, loss 0.0282417, acc 0.992188\n",
      "2017-01-10T23:51:37.273651: step 5219, loss 0.0832163, acc 0.976562\n",
      "2017-01-10T23:51:39.300583: step 5220, loss 0.0669724, acc 0.984375\n",
      "2017-01-10T23:51:41.338363: step 5221, loss 0.0118559, acc 0.992188\n",
      "2017-01-10T23:51:43.383218: step 5222, loss 0.0164627, acc 1\n",
      "2017-01-10T23:51:45.385889: step 5223, loss 0.0782033, acc 0.96875\n",
      "2017-01-10T23:51:47.398635: step 5224, loss 0.0518091, acc 0.984375\n",
      "2017-01-10T23:51:49.451782: step 5225, loss 0.0637389, acc 0.96875\n",
      "2017-01-10T23:51:51.515274: step 5226, loss 0.0322031, acc 0.984375\n",
      "2017-01-10T23:51:53.630701: step 5227, loss 0.0193322, acc 0.992188\n",
      "2017-01-10T23:51:55.672453: step 5228, loss 0.0931802, acc 0.96875\n",
      "2017-01-10T23:51:57.660799: step 5229, loss 0.0825496, acc 0.976562\n",
      "2017-01-10T23:51:59.695277: step 5230, loss 0.108118, acc 0.96875\n",
      "2017-01-10T23:52:02.055215: step 5231, loss 0.010914, acc 1\n",
      "2017-01-10T23:52:04.306573: step 5232, loss 0.00727669, acc 1\n",
      "2017-01-10T23:52:06.336789: step 5233, loss 0.0573971, acc 0.976562\n",
      "2017-01-10T23:52:08.363220: step 5234, loss 0.0547002, acc 0.976562\n",
      "2017-01-10T23:52:10.392165: step 5235, loss 0.0895848, acc 0.984375\n",
      "2017-01-10T23:52:12.394197: step 5236, loss 0.0992845, acc 0.96875\n",
      "2017-01-10T23:52:14.397235: step 5237, loss 0.0461212, acc 0.984375\n",
      "2017-01-10T23:52:16.453748: step 5238, loss 0.0663955, acc 0.976562\n",
      "2017-01-10T23:52:18.481939: step 5239, loss 0.0358877, acc 0.984375\n",
      "2017-01-10T23:52:21.426239: step 5240, loss 0.0580846, acc 0.984375\n",
      "2017-01-10T23:52:23.662327: step 5241, loss 0.0610069, acc 0.976562\n",
      "2017-01-10T23:52:25.679734: step 5242, loss 0.0307002, acc 0.984375\n",
      "2017-01-10T23:52:27.717496: step 5243, loss 0.043002, acc 0.984375\n",
      "2017-01-10T23:52:29.783934: step 5244, loss 0.0467137, acc 0.992188\n",
      "2017-01-10T23:52:31.928315: step 5245, loss 0.0602435, acc 0.976562\n",
      "2017-01-10T23:52:34.353312: step 5246, loss 0.0473766, acc 0.984375\n",
      "2017-01-10T23:52:36.365671: step 5247, loss 0.0725015, acc 0.976562\n",
      "2017-01-10T23:52:38.356802: step 5248, loss 0.0547662, acc 0.984375\n",
      "2017-01-10T23:52:40.413721: step 5249, loss 0.0556095, acc 0.976562\n",
      "2017-01-10T23:52:42.438028: step 5250, loss 0.0364682, acc 0.984375\n",
      "2017-01-10T23:52:44.468379: step 5251, loss 0.0919335, acc 0.96875\n",
      "2017-01-10T23:52:46.469157: step 5252, loss 0.0125278, acc 1\n",
      "2017-01-10T23:52:48.498301: step 5253, loss 0.0116035, acc 0.992188\n",
      "2017-01-10T23:52:50.518881: step 5254, loss 0.0562983, acc 0.992188\n",
      "2017-01-10T23:52:52.593055: step 5255, loss 0.00593401, acc 1\n",
      "2017-01-10T23:52:54.622754: step 5256, loss 0.0427338, acc 0.976562\n",
      "2017-01-10T23:52:56.653872: step 5257, loss 0.0452987, acc 0.984375\n",
      "2017-01-10T23:52:58.678828: step 5258, loss 0.00412654, acc 1\n",
      "2017-01-10T23:53:00.708258: step 5259, loss 0.0487373, acc 0.984375\n",
      "2017-01-10T23:53:02.750490: step 5260, loss 0.146299, acc 0.960938\n",
      "2017-01-10T23:53:04.927811: step 5261, loss 0.0799141, acc 0.976562\n",
      "2017-01-10T23:53:07.187065: step 5262, loss 0.0563279, acc 0.984375\n",
      "2017-01-10T23:53:09.235024: step 5263, loss 0.0531493, acc 0.984375\n",
      "2017-01-10T23:53:11.436395: step 5264, loss 0.0163689, acc 1\n",
      "2017-01-10T23:53:13.449116: step 5265, loss 0.118685, acc 0.960938\n",
      "2017-01-10T23:53:15.489771: step 5266, loss 0.0213125, acc 1\n",
      "2017-01-10T23:53:17.485067: step 5267, loss 0.035244, acc 0.984375\n",
      "2017-01-10T23:53:19.522597: step 5268, loss 0.0858607, acc 0.96875\n",
      "2017-01-10T23:53:21.590129: step 5269, loss 0.0558336, acc 0.984375\n",
      "2017-01-10T23:53:23.659135: step 5270, loss 0.104896, acc 0.984375\n",
      "2017-01-10T23:53:25.697285: step 5271, loss 0.0888741, acc 0.976562\n",
      "2017-01-10T23:53:28.719244: step 5272, loss 0.0381612, acc 0.984375\n",
      "2017-01-10T23:53:30.889030: step 5273, loss 0.0132063, acc 1\n",
      "2017-01-10T23:53:32.859361: step 5274, loss 0.0130063, acc 1\n",
      "2017-01-10T23:53:34.983934: step 5275, loss 0.0456759, acc 0.976562\n",
      "2017-01-10T23:53:37.050330: step 5276, loss 0.004703, acc 1\n",
      "2017-01-10T23:53:39.374212: step 5277, loss 0.0774308, acc 0.976562\n",
      "2017-01-10T23:53:41.393005: step 5278, loss 0.0165261, acc 1\n",
      "2017-01-10T23:53:43.384865: step 5279, loss 0.0366955, acc 0.992188\n",
      "2017-01-10T23:53:45.414779: step 5280, loss 0.0190032, acc 0.992188\n",
      "2017-01-10T23:53:47.447172: step 5281, loss 0.00252786, acc 1\n",
      "2017-01-10T23:53:49.491084: step 5282, loss 0.0129999, acc 1\n",
      "2017-01-10T23:53:51.529680: step 5283, loss 0.0206722, acc 0.992188\n",
      "2017-01-10T23:53:53.594138: step 5284, loss 0.041384, acc 0.992188\n",
      "2017-01-10T23:53:55.643446: step 5285, loss 0.0342151, acc 0.984375\n",
      "2017-01-10T23:53:57.646550: step 5286, loss 0.035276, acc 1\n",
      "2017-01-10T23:53:59.723584: step 5287, loss 0.0210647, acc 0.992188\n",
      "2017-01-10T23:54:01.762488: step 5288, loss 0.0554501, acc 0.984375\n",
      "2017-01-10T23:54:03.833952: step 5289, loss 0.0560503, acc 0.976562\n",
      "2017-01-10T23:54:05.917064: step 5290, loss 0.0319192, acc 0.992188\n",
      "2017-01-10T23:54:07.954502: step 5291, loss 0.00626416, acc 1\n",
      "2017-01-10T23:54:10.204099: step 5292, loss 0.0460428, acc 0.984375\n",
      "2017-01-10T23:54:12.297646: step 5293, loss 0.0407521, acc 0.992188\n",
      "2017-01-10T23:54:14.342953: step 5294, loss 0.0633344, acc 0.984375\n",
      "2017-01-10T23:54:16.339085: step 5295, loss 0.0225005, acc 1\n",
      "2017-01-10T23:54:18.388204: step 5296, loss 0.0985314, acc 0.984375\n",
      "2017-01-10T23:54:20.473771: step 5297, loss 0.0291865, acc 0.984375\n",
      "2017-01-10T23:54:22.532444: step 5298, loss 0.0454742, acc 0.992188\n",
      "2017-01-10T23:54:24.565276: step 5299, loss 0.0422652, acc 0.984375\n",
      "2017-01-10T23:54:26.602421: step 5300, loss 0.0321236, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:54:48.936058: step 5300, loss 0.071123, acc 0.9798\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5300\n",
      "\n",
      "2017-01-10T23:54:53.542143: step 5301, loss 0.0651748, acc 0.976562\n",
      "2017-01-10T23:54:55.536422: step 5302, loss 0.00767674, acc 1\n",
      "2017-01-10T23:54:57.536705: step 5303, loss 0.0692391, acc 0.976562\n",
      "2017-01-10T23:54:59.552732: step 5304, loss 0.00578061, acc 1\n",
      "2017-01-10T23:55:01.568456: step 5305, loss 0.0233797, acc 0.992188\n",
      "2017-01-10T23:55:03.564484: step 5306, loss 0.0554932, acc 0.976562\n",
      "2017-01-10T23:55:05.642028: step 5307, loss 0.0624419, acc 0.976562\n",
      "2017-01-10T23:55:07.674982: step 5308, loss 0.0387727, acc 0.984375\n",
      "2017-01-10T23:55:09.719012: step 5309, loss 0.0768338, acc 0.984375\n",
      "2017-01-10T23:55:11.776144: step 5310, loss 0.00659045, acc 1\n",
      "2017-01-10T23:55:13.874662: step 5311, loss 0.00496562, acc 1\n",
      "2017-01-10T23:55:16.133700: step 5312, loss 0.119453, acc 0.953125\n",
      "2017-01-10T23:55:18.152215: step 5313, loss 0.0773789, acc 0.984375\n",
      "2017-01-10T23:55:20.169519: step 5314, loss 0.125076, acc 0.960938\n",
      "2017-01-10T23:55:22.248293: step 5315, loss 0.0199702, acc 1\n",
      "2017-01-10T23:55:24.267696: step 5316, loss 0.0350875, acc 0.992188\n",
      "2017-01-10T23:55:26.314717: step 5317, loss 0.0399717, acc 0.984375\n",
      "2017-01-10T23:55:28.494786: step 5318, loss 0.010366, acc 1\n",
      "2017-01-10T23:55:30.729021: step 5319, loss 0.0355412, acc 0.984375\n",
      "2017-01-10T23:55:32.786695: step 5320, loss 0.0332536, acc 0.992188\n",
      "2017-01-10T23:55:34.841956: step 5321, loss 0.0338442, acc 0.992188\n",
      "2017-01-10T23:55:36.855086: step 5322, loss 0.0463568, acc 0.984375\n",
      "2017-01-10T23:55:38.893973: step 5323, loss 0.104026, acc 0.96875\n",
      "2017-01-10T23:55:40.925834: step 5324, loss 0.0709142, acc 0.976562\n",
      "2017-01-10T23:55:42.980906: step 5325, loss 0.0106369, acc 0.992188\n",
      "2017-01-10T23:55:45.590018: step 5326, loss 0.120022, acc 0.984375\n",
      "2017-01-10T23:55:48.519914: step 5327, loss 0.0860498, acc 0.984375\n",
      "2017-01-10T23:55:50.679569: step 5328, loss 0.0101624, acc 1\n",
      "2017-01-10T23:55:52.792643: step 5329, loss 0.0179313, acc 0.992188\n",
      "2017-01-10T23:55:54.915313: step 5330, loss 0.0547773, acc 0.984375\n",
      "2017-01-10T23:55:56.946995: step 5331, loss 0.0663025, acc 0.976562\n",
      "2017-01-10T23:55:58.974818: step 5332, loss 0.0406149, acc 0.984375\n",
      "2017-01-10T23:56:00.968576: step 5333, loss 0.0490416, acc 0.984375\n",
      "2017-01-10T23:56:02.990942: step 5334, loss 0.0356423, acc 1\n",
      "2017-01-10T23:56:04.985519: step 5335, loss 0.0472736, acc 0.984375\n",
      "2017-01-10T23:56:07.034333: step 5336, loss 0.0581786, acc 0.992188\n",
      "2017-01-10T23:56:09.032889: step 5337, loss 0.00773864, acc 1\n",
      "2017-01-10T23:56:11.073635: step 5338, loss 0.0242204, acc 0.992188\n",
      "2017-01-10T23:56:13.093912: step 5339, loss 0.0074678, acc 1\n",
      "2017-01-10T23:56:15.131076: step 5340, loss 0.0904301, acc 0.960938\n",
      "2017-01-10T23:56:17.169633: step 5341, loss 0.0651358, acc 0.984375\n",
      "2017-01-10T23:56:19.369397: step 5342, loss 0.0719403, acc 0.976562\n",
      "2017-01-10T23:56:21.605082: step 5343, loss 0.0472867, acc 0.992188\n",
      "2017-01-10T23:56:23.648955: step 5344, loss 0.0730009, acc 0.96875\n",
      "2017-01-10T23:56:25.698166: step 5345, loss 0.0197775, acc 0.992188\n",
      "2017-01-10T23:56:27.736684: step 5346, loss 0.11012, acc 0.960938\n",
      "2017-01-10T23:56:29.748833: step 5347, loss 0.0526307, acc 0.984375\n",
      "2017-01-10T23:56:31.870013: step 5348, loss 0.0471747, acc 0.984375\n",
      "2017-01-10T23:56:33.892569: step 5349, loss 0.0374601, acc 0.984375\n",
      "2017-01-10T23:56:35.944687: step 5350, loss 0.00591106, acc 1\n",
      "2017-01-10T23:56:37.967499: step 5351, loss 0.0433265, acc 0.984375\n",
      "2017-01-10T23:56:40.021437: step 5352, loss 0.0925059, acc 0.960938\n",
      "2017-01-10T23:56:42.027760: step 5353, loss 0.0671155, acc 0.976562\n",
      "2017-01-10T23:56:44.049128: step 5354, loss 0.0909408, acc 0.960938\n",
      "2017-01-10T23:56:46.094833: step 5355, loss 0.0603607, acc 0.96875\n",
      "2017-01-10T23:56:48.177585: step 5356, loss 0.0143592, acc 0.992188\n",
      "2017-01-10T23:56:50.227369: step 5357, loss 0.0683347, acc 0.984375\n",
      "2017-01-10T23:56:52.609846: step 5358, loss 0.0960637, acc 0.96875\n",
      "2017-01-10T23:56:54.693911: step 5359, loss 0.0379371, acc 0.992188\n",
      "2017-01-10T23:56:56.708659: step 5360, loss 0.00703181, acc 0.992188\n",
      "2017-01-10T23:56:58.747354: step 5361, loss 0.0173481, acc 0.992188\n",
      "2017-01-10T23:57:00.785934: step 5362, loss 0.0253301, acc 0.992188\n",
      "2017-01-10T23:57:02.841897: step 5363, loss 0.0287949, acc 0.992188\n",
      "2017-01-10T23:57:04.890215: step 5364, loss 0.0587816, acc 0.992188\n",
      "2017-01-10T23:57:06.936120: step 5365, loss 0.0236794, acc 0.992188\n",
      "2017-01-10T23:57:09.085475: step 5366, loss 0.0324691, acc 1\n",
      "2017-01-10T23:57:11.225307: step 5367, loss 0.0817561, acc 0.976562\n",
      "2017-01-10T23:57:13.257952: step 5368, loss 0.00602325, acc 1\n",
      "2017-01-10T23:57:15.278412: step 5369, loss 0.0454107, acc 0.984375\n",
      "2017-01-10T23:57:17.299581: step 5370, loss 0.105529, acc 0.96875\n",
      "2017-01-10T23:57:19.314292: step 5371, loss 0.0184043, acc 1\n",
      "2017-01-10T23:57:21.398055: step 5372, loss 0.0306114, acc 0.992188\n",
      "2017-01-10T23:57:23.524590: step 5373, loss 0.077197, acc 0.976562\n",
      "2017-01-10T23:57:26.667928: step 5374, loss 0.0563867, acc 0.976562\n",
      "2017-01-10T23:57:28.913049: step 5375, loss 0.0158091, acc 0.992188\n",
      "2017-01-10T23:57:30.924327: step 5376, loss 0.0234694, acc 1\n",
      "2017-01-10T23:57:32.979849: step 5377, loss 0.0264636, acc 0.992188\n",
      "2017-01-10T23:57:35.053546: step 5378, loss 0.0301053, acc 0.992188\n",
      "2017-01-10T23:57:37.095638: step 5379, loss 0.00338417, acc 1\n",
      "2017-01-10T23:57:39.121953: step 5380, loss 0.021441, acc 1\n",
      "2017-01-10T23:57:41.138297: step 5381, loss 0.058972, acc 0.976562\n",
      "2017-01-10T23:57:43.164117: step 5382, loss 0.0100648, acc 1\n",
      "2017-01-10T23:57:45.190815: step 5383, loss 0.0887202, acc 0.96875\n",
      "2017-01-10T23:57:47.222842: step 5384, loss 0.0447975, acc 0.992188\n",
      "2017-01-10T23:57:49.216418: step 5385, loss 0.0519095, acc 0.992188\n",
      "2017-01-10T23:57:51.298848: step 5386, loss 0.0343779, acc 0.984375\n",
      "2017-01-10T23:57:53.332771: step 5387, loss 0.103564, acc 0.976562\n",
      "2017-01-10T23:57:55.481777: step 5388, loss 0.0371226, acc 1\n",
      "2017-01-10T23:57:57.877472: step 5389, loss 0.0267552, acc 1\n",
      "2017-01-10T23:57:59.889021: step 5390, loss 0.0714916, acc 0.984375\n",
      "2017-01-10T23:58:01.929921: step 5391, loss 0.021372, acc 0.992188\n",
      "2017-01-10T23:58:04.021686: step 5392, loss 0.0510251, acc 0.984375\n",
      "2017-01-10T23:58:06.101718: step 5393, loss 0.0470379, acc 0.984375\n",
      "2017-01-10T23:58:08.095864: step 5394, loss 0.0659563, acc 0.984375\n",
      "2017-01-10T23:58:10.132150: step 5395, loss 0.0327712, acc 1\n",
      "2017-01-10T23:58:12.176679: step 5396, loss 0.0139051, acc 0.992188\n",
      "2017-01-10T23:58:14.157568: step 5397, loss 0.0753145, acc 0.976562\n",
      "2017-01-10T23:58:16.354354: step 5398, loss 0.0941596, acc 0.976562\n",
      "2017-01-10T23:58:18.376426: step 5399, loss 0.0151304, acc 0.992188\n",
      "2017-01-10T23:58:20.446942: step 5400, loss 0.0696618, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-10T23:58:47.819169: step 5400, loss 0.0705363, acc 0.9802\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5400\n",
      "\n",
      "2017-01-10T23:58:52.687237: step 5401, loss 0.0505413, acc 0.984375\n",
      "2017-01-10T23:58:54.767962: step 5402, loss 0.0172731, acc 1\n",
      "2017-01-10T23:58:56.830726: step 5403, loss 0.0368926, acc 0.992188\n",
      "2017-01-10T23:58:58.849125: step 5404, loss 0.0605203, acc 0.984375\n",
      "2017-01-10T23:59:01.019150: step 5405, loss 0.00653938, acc 1\n",
      "2017-01-10T23:59:03.357994: step 5406, loss 0.00106021, acc 1\n",
      "2017-01-10T23:59:05.416662: step 5407, loss 0.0336974, acc 0.992188\n",
      "2017-01-10T23:59:07.427781: step 5408, loss 0.136427, acc 0.960938\n",
      "2017-01-10T23:59:09.468958: step 5409, loss 0.0672996, acc 0.96875\n",
      "2017-01-10T23:59:11.486466: step 5410, loss 0.0438247, acc 0.992188\n",
      "2017-01-10T23:59:13.522527: step 5411, loss 0.0657748, acc 0.984375\n",
      "2017-01-10T23:59:15.541503: step 5412, loss 0.0758987, acc 0.976562\n",
      "2017-01-10T23:59:17.564703: step 5413, loss 0.0415582, acc 0.984375\n",
      "2017-01-10T23:59:19.611294: step 5414, loss 0.0688639, acc 0.984375\n",
      "2017-01-10T23:59:21.642566: step 5415, loss 0.039356, acc 0.992188\n",
      "2017-01-10T23:59:23.660233: step 5416, loss 0.015871, acc 0.992188\n",
      "2017-01-10T23:59:25.705260: step 5417, loss 0.0287964, acc 0.992188\n",
      "2017-01-10T23:59:27.770249: step 5418, loss 0.0270734, acc 0.992188\n",
      "2017-01-10T23:59:29.810561: step 5419, loss 0.0138952, acc 0.992188\n",
      "2017-01-10T23:59:31.790575: step 5420, loss 0.0755894, acc 0.976562\n",
      "2017-01-10T23:59:33.828671: step 5421, loss 0.0296447, acc 0.992188\n",
      "2017-01-10T23:59:36.131972: step 5422, loss 0.0386, acc 0.992188\n",
      "2017-01-10T23:59:38.138567: step 5423, loss 0.0447467, acc 0.992188\n",
      "2017-01-10T23:59:40.175243: step 5424, loss 0.0508451, acc 0.976562\n",
      "2017-01-10T23:59:42.186478: step 5425, loss 0.0387805, acc 0.992188\n",
      "2017-01-10T23:59:44.229321: step 5426, loss 0.0215751, acc 0.992188\n",
      "2017-01-10T23:59:46.256799: step 5427, loss 0.0911468, acc 0.960938\n",
      "2017-01-10T23:59:48.287795: step 5428, loss 0.052556, acc 0.984375\n",
      "2017-01-10T23:59:50.323273: step 5429, loss 0.0469401, acc 0.984375\n",
      "2017-01-10T23:59:52.399649: step 5430, loss 0.0695274, acc 0.976562\n",
      "2017-01-10T23:59:54.435374: step 5431, loss 0.0304473, acc 0.992188\n",
      "2017-01-10T23:59:56.598640: step 5432, loss 0.0352256, acc 0.992188\n",
      "2017-01-10T23:59:58.646108: step 5433, loss 0.0893689, acc 0.976562\n",
      "2017-01-11T00:00:00.896311: step 5434, loss 0.077169, acc 0.984375\n",
      "2017-01-11T00:00:03.834155: step 5435, loss 0.0336133, acc 0.992188\n",
      "2017-01-11T00:00:06.366560: step 5436, loss 0.0271894, acc 0.992188\n",
      "2017-01-11T00:00:08.988962: step 5437, loss 0.00321662, acc 1\n",
      "2017-01-11T00:00:11.465667: step 5438, loss 0.0192737, acc 0.992188\n",
      "2017-01-11T00:00:13.739878: step 5439, loss 0.043191, acc 0.984375\n",
      "2017-01-11T00:00:16.000260: step 5440, loss 0.0305884, acc 0.992188\n",
      "2017-01-11T00:00:18.439285: step 5441, loss 0.0253984, acc 0.992188\n",
      "2017-01-11T00:00:20.634902: step 5442, loss 0.0172184, acc 1\n",
      "2017-01-11T00:00:22.862240: step 5443, loss 0.0313785, acc 0.992188\n",
      "2017-01-11T00:00:25.326349: step 5444, loss 0.0505332, acc 0.984375\n",
      "2017-01-11T00:00:27.760903: step 5445, loss 0.0426357, acc 0.992188\n",
      "2017-01-11T00:00:30.172788: step 5446, loss 0.0526956, acc 0.976562\n",
      "2017-01-11T00:00:32.457497: step 5447, loss 0.0525167, acc 0.984375\n",
      "2017-01-11T00:00:35.930494: step 5448, loss 0.0161464, acc 1\n",
      "2017-01-11T00:00:38.469769: step 5449, loss 0.0381099, acc 0.984375\n",
      "2017-01-11T00:00:41.235378: step 5450, loss 0.024978, acc 0.992188\n",
      "2017-01-11T00:00:43.441876: step 5451, loss 0.0211865, acc 0.992188\n",
      "2017-01-11T00:00:45.616131: step 5452, loss 0.000742156, acc 1\n",
      "2017-01-11T00:00:48.090686: step 5453, loss 0.028873, acc 0.992188\n",
      "2017-01-11T00:00:50.417874: step 5454, loss 0.0465669, acc 0.984375\n",
      "2017-01-11T00:00:52.946698: step 5455, loss 0.0406353, acc 0.992188\n",
      "2017-01-11T00:00:55.284732: step 5456, loss 0.0277478, acc 0.992188\n",
      "2017-01-11T00:00:58.616701: step 5457, loss 0.022489, acc 1\n",
      "2017-01-11T00:01:01.261329: step 5458, loss 0.0863608, acc 0.976562\n",
      "2017-01-11T00:01:03.715751: step 5459, loss 0.0041586, acc 1\n",
      "2017-01-11T00:01:06.098796: step 5460, loss 0.0798245, acc 0.984375\n",
      "2017-01-11T00:01:08.359565: step 5461, loss 0.0114191, acc 1\n",
      "2017-01-11T00:01:10.657332: step 5462, loss 0.0267937, acc 0.992188\n",
      "2017-01-11T00:01:13.311391: step 5463, loss 0.0522374, acc 0.992188\n",
      "2017-01-11T00:01:15.552692: step 5464, loss 0.0420977, acc 0.992188\n",
      "2017-01-11T00:01:17.891401: step 5465, loss 0.0316538, acc 0.984375\n",
      "2017-01-11T00:01:20.365661: step 5466, loss 0.0228227, acc 0.992188\n",
      "2017-01-11T00:01:22.786434: step 5467, loss 0.0174932, acc 1\n",
      "2017-01-11T00:01:25.089679: step 5468, loss 0.0594203, acc 0.976562\n",
      "2017-01-11T00:01:27.449888: step 5469, loss 0.104206, acc 0.96875\n",
      "2017-01-11T00:01:29.538066: step 5470, loss 0.123678, acc 0.96875\n",
      "2017-01-11T00:01:31.945978: step 5471, loss 0.0262733, acc 0.992188\n",
      "2017-01-11T00:01:34.396243: step 5472, loss 0.00724872, acc 1\n",
      "2017-01-11T00:01:36.963767: step 5473, loss 0.0138986, acc 1\n",
      "2017-01-11T00:01:39.113955: step 5474, loss 0.0423291, acc 0.984375\n",
      "2017-01-11T00:01:41.400065: step 5475, loss 0.0354975, acc 0.992188\n",
      "2017-01-11T00:01:43.720850: step 5476, loss 0.0308096, acc 0.992188\n",
      "2017-01-11T00:01:46.300474: step 5477, loss 0.072138, acc 0.984375\n",
      "2017-01-11T00:01:48.484040: step 5478, loss 0.0480625, acc 0.984375\n",
      "2017-01-11T00:01:50.847747: step 5479, loss 0.0337463, acc 0.992188\n",
      "2017-01-11T00:01:53.377987: step 5480, loss 0.0282697, acc 0.992188\n",
      "2017-01-11T00:01:55.802117: step 5481, loss 0.0628649, acc 0.976562\n",
      "2017-01-11T00:01:58.282720: step 5482, loss 0.038873, acc 0.984375\n",
      "2017-01-11T00:02:00.591292: step 5483, loss 0.0651391, acc 0.976562\n",
      "2017-01-11T00:02:03.026324: step 5484, loss 0.0343615, acc 0.984375\n",
      "2017-01-11T00:02:05.489422: step 5485, loss 0.0161989, acc 1\n",
      "2017-01-11T00:02:07.857338: step 5486, loss 0.0528469, acc 0.984375\n",
      "2017-01-11T00:02:10.059275: step 5487, loss 0.0518207, acc 0.976562\n",
      "2017-01-11T00:02:12.297098: step 5488, loss 0.0500598, acc 0.984375\n",
      "2017-01-11T00:02:14.760241: step 5489, loss 0.0274922, acc 0.992188\n",
      "2017-01-11T00:02:17.326215: step 5490, loss 0.0230002, acc 1\n",
      "2017-01-11T00:02:19.456431: step 5491, loss 0.0753815, acc 0.984375\n",
      "2017-01-11T00:02:21.892909: step 5492, loss 0.0564677, acc 0.984375\n",
      "2017-01-11T00:02:24.192456: step 5493, loss 0.0593105, acc 0.976562\n",
      "2017-01-11T00:02:26.550190: step 5494, loss 0.074978, acc 0.976562\n",
      "2017-01-11T00:02:29.125912: step 5495, loss 0.0465923, acc 0.984375\n",
      "2017-01-11T00:02:31.991167: step 5496, loss 0.0734022, acc 0.976562\n",
      "2017-01-11T00:02:34.967572: step 5497, loss 0.0183287, acc 0.992188\n",
      "2017-01-11T00:02:37.348382: step 5498, loss 0.0511085, acc 0.984375\n",
      "2017-01-11T00:02:39.679886: step 5499, loss 0.045629, acc 0.984375\n",
      "2017-01-11T00:02:41.926121: step 5500, loss 0.0335555, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:03:06.285896: step 5500, loss 0.0703521, acc 0.9802\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5500\n",
      "\n",
      "2017-01-11T00:03:11.008668: step 5501, loss 0.0331655, acc 0.992188\n",
      "2017-01-11T00:03:13.303458: step 5502, loss 0.0449219, acc 0.984375\n",
      "2017-01-11T00:03:15.767835: step 5503, loss 0.0969373, acc 0.960938\n",
      "2017-01-11T00:03:18.201880: step 5504, loss 0.0831926, acc 0.976562\n",
      "2017-01-11T00:03:20.485731: step 5505, loss 0.0174114, acc 0.992188\n",
      "2017-01-11T00:03:23.065861: step 5506, loss 0.0256635, acc 0.992188\n",
      "2017-01-11T00:03:25.422875: step 5507, loss 0.071717, acc 0.984375\n",
      "2017-01-11T00:03:27.941472: step 5508, loss 0.0271709, acc 0.992188\n",
      "2017-01-11T00:03:30.180786: step 5509, loss 0.00581167, acc 1\n",
      "2017-01-11T00:03:32.561646: step 5510, loss 0.0909688, acc 0.96875\n",
      "2017-01-11T00:03:34.780942: step 5511, loss 0.0579971, acc 0.984375\n",
      "2017-01-11T00:03:37.250320: step 5512, loss 0.0432095, acc 0.992188\n",
      "2017-01-11T00:03:40.426923: step 5513, loss 0.0210712, acc 0.992188\n",
      "2017-01-11T00:03:42.871185: step 5514, loss 0.0403895, acc 0.992188\n",
      "2017-01-11T00:03:45.356556: step 5515, loss 0.0586526, acc 0.984375\n",
      "2017-01-11T00:03:47.808659: step 5516, loss 0.0333095, acc 0.992188\n",
      "2017-01-11T00:03:50.264214: step 5517, loss 0.0293264, acc 0.992188\n",
      "2017-01-11T00:03:52.674394: step 5518, loss 0.0501527, acc 0.984375\n",
      "2017-01-11T00:03:55.023985: step 5519, loss 0.0327582, acc 0.992188\n",
      "2017-01-11T00:03:57.599333: step 5520, loss 0.0261609, acc 0.984375\n",
      "2017-01-11T00:03:59.980016: step 5521, loss 0.0246905, acc 0.992188\n",
      "2017-01-11T00:04:02.215485: step 5522, loss 0.02728, acc 0.992188\n",
      "2017-01-11T00:04:04.748224: step 5523, loss 0.00778725, acc 1\n",
      "2017-01-11T00:04:07.222102: step 5524, loss 0.0610638, acc 0.976562\n",
      "2017-01-11T00:04:09.524548: step 5525, loss 0.0444829, acc 0.992188\n",
      "2017-01-11T00:04:11.877894: step 5526, loss 0.0548734, acc 0.984375\n",
      "2017-01-11T00:04:14.122620: step 5527, loss 0.0213472, acc 0.992188\n",
      "2017-01-11T00:04:16.621177: step 5528, loss 0.0358697, acc 0.984375\n",
      "2017-01-11T00:04:18.921577: step 5529, loss 0.0102164, acc 1\n",
      "2017-01-11T00:04:21.507712: step 5530, loss 0.0340022, acc 0.992188\n",
      "2017-01-11T00:04:23.998065: step 5531, loss 0.0441345, acc 0.984375\n",
      "2017-01-11T00:04:26.569944: step 5532, loss 0.0429031, acc 0.984375\n",
      "2017-01-11T00:04:29.171596: step 5533, loss 0.0785651, acc 0.984375\n",
      "2017-01-11T00:04:31.678312: step 5534, loss 0.00640624, acc 1\n",
      "2017-01-11T00:04:34.159929: step 5535, loss 0.0578397, acc 0.992188\n",
      "2017-01-11T00:04:36.604695: step 5536, loss 0.0276916, acc 0.992188\n",
      "2017-01-11T00:04:38.837437: step 5537, loss 0.0995512, acc 0.960938\n",
      "2017-01-11T00:04:41.269986: step 5538, loss 0.0651457, acc 0.976562\n",
      "2017-01-11T00:04:43.795344: step 5539, loss 0.0695236, acc 0.96875\n",
      "2017-01-11T00:04:46.214489: step 5540, loss 0.0588481, acc 0.984375\n",
      "2017-01-11T00:04:48.584124: step 5541, loss 0.025167, acc 0.992188\n",
      "2017-01-11T00:04:50.756877: step 5542, loss 0.0340233, acc 0.992188\n",
      "2017-01-11T00:04:53.294080: step 5543, loss 0.0387587, acc 0.984375\n",
      "2017-01-11T00:04:55.766193: step 5544, loss 0.0196234, acc 1\n",
      "2017-01-11T00:04:58.223358: step 5545, loss 0.13406, acc 0.976562\n",
      "2017-01-11T00:05:00.683780: step 5546, loss 0.0546371, acc 0.992188\n",
      "2017-01-11T00:05:03.037850: step 5547, loss 0.029725, acc 1\n",
      "2017-01-11T00:05:05.530247: step 5548, loss 0.0296944, acc 0.992188\n",
      "2017-01-11T00:05:08.038901: step 5549, loss 0.0605042, acc 0.976562\n",
      "2017-01-11T00:05:10.536351: step 5550, loss 0.0322386, acc 0.992188\n",
      "2017-01-11T00:05:12.763821: step 5551, loss 0.122139, acc 0.96875\n",
      "2017-01-11T00:05:15.073557: step 5552, loss 0.094394, acc 0.96875\n",
      "2017-01-11T00:05:17.632684: step 5553, loss 0.0524109, acc 0.976562\n",
      "2017-01-11T00:05:20.072227: step 5554, loss 0.0929468, acc 0.96875\n",
      "2017-01-11T00:05:22.542809: step 5555, loss 0.00740652, acc 1\n",
      "2017-01-11T00:05:25.033270: step 5556, loss 0.0548266, acc 0.984375\n",
      "2017-01-11T00:05:27.401349: step 5557, loss 0.0487574, acc 0.992188\n",
      "2017-01-11T00:05:29.791593: step 5558, loss 0.0325847, acc 0.984375\n",
      "2017-01-11T00:05:32.210732: step 5559, loss 0.0162499, acc 1\n",
      "2017-01-11T00:05:34.747821: step 5560, loss 0.061861, acc 0.976562\n",
      "2017-01-11T00:05:37.240059: step 5561, loss 0.00367491, acc 1\n",
      "2017-01-11T00:05:39.685879: step 5562, loss 0.0577792, acc 0.984375\n",
      "2017-01-11T00:05:42.139198: step 5563, loss 0.00926978, acc 0.992188\n",
      "2017-01-11T00:05:45.099854: step 5564, loss 0.00478633, acc 1\n",
      "2017-01-11T00:05:47.990550: step 5565, loss 0.00783519, acc 0.992188\n",
      "2017-01-11T00:05:50.568685: step 5566, loss 0.0574677, acc 0.984375\n",
      "2017-01-11T00:05:53.099980: step 5567, loss 0.0469519, acc 0.984375\n",
      "2017-01-11T00:05:55.641029: step 5568, loss 0.0214115, acc 0.992188\n",
      "2017-01-11T00:05:58.044627: step 5569, loss 0.00544768, acc 1\n",
      "2017-01-11T00:06:00.516918: step 5570, loss 0.0294795, acc 0.992188\n",
      "2017-01-11T00:06:02.613184: step 5571, loss 0.0633448, acc 0.984375\n",
      "2017-01-11T00:06:05.101571: step 5572, loss 0.0180271, acc 0.992188\n",
      "2017-01-11T00:06:07.559542: step 5573, loss 0.0354996, acc 0.984375\n",
      "2017-01-11T00:06:09.972881: step 5574, loss 0.031508, acc 0.992188\n",
      "2017-01-11T00:06:12.281568: step 5575, loss 0.0206433, acc 0.992188\n",
      "2017-01-11T00:06:14.633872: step 5576, loss 0.0309376, acc 0.992188\n",
      "2017-01-11T00:06:17.241982: step 5577, loss 0.0110368, acc 1\n",
      "2017-01-11T00:06:19.572796: step 5578, loss 0.0201319, acc 0.992188\n",
      "2017-01-11T00:06:22.009587: step 5579, loss 0.0504353, acc 0.984375\n",
      "2017-01-11T00:06:24.400097: step 5580, loss 0.0210803, acc 1\n",
      "2017-01-11T00:06:26.767065: step 5581, loss 0.0754631, acc 0.976562\n",
      "2017-01-11T00:06:29.145172: step 5582, loss 0.0451629, acc 0.96875\n",
      "2017-01-11T00:06:31.413851: step 5583, loss 0.0234341, acc 0.984375\n",
      "2017-01-11T00:06:33.986646: step 5584, loss 0.0327665, acc 0.984375\n",
      "2017-01-11T00:06:36.413239: step 5585, loss 0.0239953, acc 0.984375\n",
      "2017-01-11T00:06:39.211239: step 5586, loss 0.0170298, acc 0.992188\n",
      "2017-01-11T00:06:41.758781: step 5587, loss 0.0885775, acc 0.976562\n",
      "2017-01-11T00:06:44.012988: step 5588, loss 0.0173917, acc 1\n",
      "2017-01-11T00:06:46.349650: step 5589, loss 0.0768999, acc 0.96875\n",
      "2017-01-11T00:06:48.671666: step 5590, loss 0.0293993, acc 0.984375\n",
      "2017-01-11T00:06:51.030298: step 5591, loss 0.0630096, acc 0.984375\n",
      "2017-01-11T00:06:53.273617: step 5592, loss 0.0715202, acc 0.976562\n",
      "2017-01-11T00:06:55.799238: step 5593, loss 0.0202722, acc 1\n",
      "2017-01-11T00:06:58.269588: step 5594, loss 0.0499893, acc 0.992188\n",
      "2017-01-11T00:07:00.748648: step 5595, loss 0.0584578, acc 0.984375\n",
      "2017-01-11T00:07:02.926774: step 5596, loss 0.0401889, acc 0.984375\n",
      "2017-01-11T00:07:05.292826: step 5597, loss 0.020414, acc 0.992188\n",
      "2017-01-11T00:07:07.632667: step 5598, loss 0.0475683, acc 0.984375\n",
      "2017-01-11T00:07:10.205296: step 5599, loss 0.0504872, acc 0.984375\n",
      "2017-01-11T00:07:12.649017: step 5600, loss 0.0451187, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:07:43.765548: step 5600, loss 0.0704735, acc 0.9806\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5600\n",
      "\n",
      "2017-01-11T00:07:48.357479: step 5601, loss 0.0751702, acc 0.976562\n",
      "2017-01-11T00:07:50.655194: step 5602, loss 0.0399139, acc 0.984375\n",
      "2017-01-11T00:07:53.137024: step 5603, loss 0.0223147, acc 1\n",
      "2017-01-11T00:07:55.530013: step 5604, loss 0.022083, acc 0.992188\n",
      "2017-01-11T00:07:57.772647: step 5605, loss 0.04059, acc 0.984375\n",
      "2017-01-11T00:08:00.048693: step 5606, loss 0.0346293, acc 0.992188\n",
      "2017-01-11T00:08:02.148063: step 5607, loss 0.0436549, acc 0.976562\n",
      "2017-01-11T00:08:04.462422: step 5608, loss 0.0120077, acc 0.992188\n",
      "2017-01-11T00:08:06.920797: step 5609, loss 0.0166707, acc 0.992188\n",
      "2017-01-11T00:08:09.098888: step 5610, loss 0.0129509, acc 1\n",
      "2017-01-11T00:08:11.288301: step 5611, loss 0.317867, acc 0.984375\n",
      "2017-01-11T00:08:13.485443: step 5612, loss 0.025991, acc 1\n",
      "2017-01-11T00:08:15.707278: step 5613, loss 0.0180854, acc 0.992188\n",
      "2017-01-11T00:08:18.440144: step 5614, loss 0.00222741, acc 1\n",
      "2017-01-11T00:08:20.642343: step 5615, loss 0.00987358, acc 1\n",
      "2017-01-11T00:08:22.943372: step 5616, loss 0.005065, acc 1\n",
      "2017-01-11T00:08:25.375927: step 5617, loss 0.052268, acc 0.976562\n",
      "2017-01-11T00:08:27.814634: step 5618, loss 0.027091, acc 0.984375\n",
      "2017-01-11T00:08:30.245842: step 5619, loss 0.0355327, acc 0.992188\n",
      "2017-01-11T00:08:32.530846: step 5620, loss 0.141045, acc 0.976562\n",
      "2017-01-11T00:08:34.961767: step 5621, loss 0.0639608, acc 0.96875\n",
      "2017-01-11T00:08:37.291447: step 5622, loss 0.0634303, acc 0.984375\n",
      "2017-01-11T00:08:39.454991: step 5623, loss 0.0348618, acc 0.992188\n",
      "2017-01-11T00:08:41.875601: step 5624, loss 0.0362854, acc 0.992188\n",
      "2017-01-11T00:08:45.169721: step 5625, loss 0.0599026, acc 0.984375\n",
      "2017-01-11T00:08:47.884105: step 5626, loss 0.0180248, acc 0.992188\n",
      "2017-01-11T00:08:50.450083: step 5627, loss 0.0463941, acc 0.976562\n",
      "2017-01-11T00:08:52.752650: step 5628, loss 0.0224113, acc 0.992188\n",
      "2017-01-11T00:08:55.147331: step 5629, loss 0.0677245, acc 0.984375\n",
      "2017-01-11T00:08:57.553462: step 5630, loss 0.00104924, acc 1\n",
      "2017-01-11T00:08:59.885749: step 5631, loss 0.0124328, acc 1\n",
      "2017-01-11T00:09:02.014022: step 5632, loss 0.0353376, acc 0.992188\n",
      "2017-01-11T00:09:04.506034: step 5633, loss 0.0151188, acc 1\n",
      "2017-01-11T00:09:06.528945: step 5634, loss 0.0174441, acc 0.992188\n",
      "2017-01-11T00:09:08.928438: step 5635, loss 0.00489448, acc 1\n",
      "2017-01-11T00:09:11.194496: step 5636, loss 0.0778986, acc 0.96875\n",
      "2017-01-11T00:09:13.571972: step 5637, loss 0.00800952, acc 1\n",
      "2017-01-11T00:09:15.927440: step 5638, loss 0.0316576, acc 0.984375\n",
      "2017-01-11T00:09:18.281213: step 5639, loss 0.0162513, acc 0.992188\n",
      "2017-01-11T00:09:20.809706: step 5640, loss 0.0250151, acc 0.992188\n",
      "2017-01-11T00:09:23.374267: step 5641, loss 0.00271708, acc 1\n",
      "2017-01-11T00:09:25.770928: step 5642, loss 0.000673875, acc 1\n",
      "2017-01-11T00:09:28.084362: step 5643, loss 0.0146658, acc 0.992188\n",
      "2017-01-11T00:09:30.385535: step 5644, loss 0.0860873, acc 0.96875\n",
      "2017-01-11T00:09:32.638400: step 5645, loss 0.092716, acc 0.984375\n",
      "2017-01-11T00:09:35.080910: step 5646, loss 0.0858641, acc 0.976562\n",
      "2017-01-11T00:09:37.432958: step 5647, loss 0.0462061, acc 0.984375\n",
      "2017-01-11T00:09:39.821919: step 5648, loss 0.00640674, acc 1\n",
      "2017-01-11T00:09:42.088970: step 5649, loss 0.0441739, acc 0.984375\n",
      "2017-01-11T00:09:44.328352: step 5650, loss 0.0377678, acc 0.976562\n",
      "2017-01-11T00:09:46.781455: step 5651, loss 0.00329399, acc 1\n",
      "2017-01-11T00:09:48.941783: step 5652, loss 0.0429084, acc 0.984375\n",
      "2017-01-11T00:09:51.259472: step 5653, loss 0.0792212, acc 0.976562\n",
      "2017-01-11T00:09:53.635017: step 5654, loss 0.0425355, acc 0.984375\n",
      "2017-01-11T00:09:56.220610: step 5655, loss 0.0600086, acc 0.984375\n",
      "2017-01-11T00:09:58.594169: step 5656, loss 0.0127863, acc 0.992188\n",
      "2017-01-11T00:10:00.952290: step 5657, loss 0.15242, acc 0.976562\n",
      "2017-01-11T00:10:03.364429: step 5658, loss 0.0101657, acc 1\n",
      "2017-01-11T00:10:05.674673: step 5659, loss 0.014907, acc 0.992188\n",
      "2017-01-11T00:10:07.932498: step 5660, loss 0.0277212, acc 0.992188\n",
      "2017-01-11T00:10:10.269390: step 5661, loss 0.0513932, acc 0.984375\n",
      "2017-01-11T00:10:12.531120: step 5662, loss 0.0474163, acc 0.984375\n",
      "2017-01-11T00:10:14.777260: step 5663, loss 0.00196105, acc 1\n",
      "2017-01-11T00:10:16.890926: step 5664, loss 0.0333306, acc 0.992188\n",
      "2017-01-11T00:10:19.228976: step 5665, loss 0.056693, acc 0.984375\n",
      "2017-01-11T00:10:21.562028: step 5666, loss 0.0222377, acc 0.992188\n",
      "2017-01-11T00:10:23.676926: step 5667, loss 0.0951603, acc 0.976562\n",
      "2017-01-11T00:10:26.064391: step 5668, loss 0.00129658, acc 1\n",
      "2017-01-11T00:10:28.659555: step 5669, loss 0.0400791, acc 0.984375\n",
      "2017-01-11T00:10:31.145441: step 5670, loss 0.01911, acc 0.992188\n",
      "2017-01-11T00:10:33.458524: step 5671, loss 0.0110446, acc 1\n",
      "2017-01-11T00:10:35.736319: step 5672, loss 0.0571938, acc 0.984375\n",
      "2017-01-11T00:10:38.166417: step 5673, loss 0.0102952, acc 1\n",
      "2017-01-11T00:10:40.545810: step 5674, loss 0.0428635, acc 0.984375\n",
      "2017-01-11T00:10:42.778539: step 5675, loss 0.0852739, acc 0.976562\n",
      "2017-01-11T00:10:45.828061: step 5676, loss 0.0157055, acc 0.992188\n",
      "2017-01-11T00:10:48.242628: step 5677, loss 0.0256987, acc 0.992188\n",
      "2017-01-11T00:10:50.667072: step 5678, loss 0.0658017, acc 0.976562\n",
      "2017-01-11T00:10:52.737409: step 5679, loss 0.0962808, acc 0.96875\n",
      "2017-01-11T00:10:54.820717: step 5680, loss 0.0359618, acc 0.992188\n",
      "2017-01-11T00:10:56.861357: step 5681, loss 0.0469085, acc 0.984375\n",
      "2017-01-11T00:10:59.076267: step 5682, loss 0.0152146, acc 0.992188\n",
      "2017-01-11T00:11:01.292281: step 5683, loss 0.000374658, acc 1\n",
      "2017-01-11T00:11:03.340710: step 5684, loss 0.00309515, acc 1\n",
      "2017-01-11T00:11:05.408359: step 5685, loss 0.00436631, acc 1\n",
      "2017-01-11T00:11:07.404128: step 5686, loss 0.070484, acc 0.976562\n",
      "2017-01-11T00:11:09.361839: step 5687, loss 0.0482603, acc 0.992188\n",
      "2017-01-11T00:11:11.402081: step 5688, loss 0.0186186, acc 0.992188\n",
      "2017-01-11T00:11:13.447951: step 5689, loss 0.0688614, acc 0.992188\n",
      "2017-01-11T00:11:15.462248: step 5690, loss 0.00858671, acc 1\n",
      "2017-01-11T00:11:17.485169: step 5691, loss 0.0225474, acc 0.992188\n",
      "2017-01-11T00:11:19.515151: step 5692, loss 0.0651226, acc 0.984375\n",
      "2017-01-11T00:11:21.559663: step 5693, loss 0.0135693, acc 1\n",
      "2017-01-11T00:11:23.566038: step 5694, loss 0.084706, acc 0.976562\n",
      "2017-01-11T00:11:25.557025: step 5695, loss 0.0473721, acc 0.984375\n",
      "2017-01-11T00:11:27.581294: step 5696, loss 0.0317011, acc 0.984375\n",
      "2017-01-11T00:11:29.601208: step 5697, loss 0.0539761, acc 0.984375\n",
      "2017-01-11T00:11:31.930212: step 5698, loss 0.0403122, acc 0.976562\n",
      "2017-01-11T00:11:33.941883: step 5699, loss 0.0339479, acc 0.984375\n",
      "2017-01-11T00:11:36.027262: step 5700, loss 0.0222079, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:11:59.766343: step 5700, loss 0.0712622, acc 0.98008\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5700\n",
      "\n",
      "2017-01-11T00:12:04.578457: step 5701, loss 0.046272, acc 0.992188\n",
      "2017-01-11T00:12:06.643641: step 5702, loss 0.00632016, acc 1\n",
      "2017-01-11T00:12:08.671172: step 5703, loss 0.0214334, acc 0.992188\n",
      "2017-01-11T00:12:10.711391: step 5704, loss 0.0594323, acc 0.976562\n",
      "2017-01-11T00:12:12.701652: step 5705, loss 0.04523, acc 0.984375\n",
      "2017-01-11T00:12:14.721350: step 5706, loss 0.0488577, acc 0.984375\n",
      "2017-01-11T00:12:16.747618: step 5707, loss 0.0042548, acc 1\n",
      "2017-01-11T00:12:18.767565: step 5708, loss 0.00359075, acc 1\n",
      "2017-01-11T00:12:20.840245: step 5709, loss 0.0544174, acc 0.984375\n",
      "2017-01-11T00:12:22.929657: step 5710, loss 0.012869, acc 0.992188\n",
      "2017-01-11T00:12:24.983631: step 5711, loss 0.0367364, acc 0.984375\n",
      "2017-01-11T00:12:27.067429: step 5712, loss 0.0150415, acc 0.992188\n",
      "2017-01-11T00:12:29.275521: step 5713, loss 0.0923604, acc 0.976562\n",
      "2017-01-11T00:12:31.388554: step 5714, loss 0.034536, acc 0.984375\n",
      "2017-01-11T00:12:33.419352: step 5715, loss 0.0263394, acc 0.992188\n",
      "2017-01-11T00:12:35.565883: step 5716, loss 0.0183359, acc 1\n",
      "2017-01-11T00:12:37.799786: step 5717, loss 0.039744, acc 0.992188\n",
      "2017-01-11T00:12:39.769000: step 5718, loss 0.070075, acc 0.984375\n",
      "2017-01-11T00:12:41.841299: step 5719, loss 0.0399236, acc 0.984375\n",
      "2017-01-11T00:12:44.616097: step 5720, loss 0.0618198, acc 0.976562\n",
      "2017-01-11T00:12:46.998155: step 5721, loss 0.0256872, acc 1\n",
      "2017-01-11T00:12:49.060270: step 5722, loss 0.0399426, acc 0.992188\n",
      "2017-01-11T00:12:51.135211: step 5723, loss 0.0427121, acc 0.992188\n",
      "2017-01-11T00:12:53.155372: step 5724, loss 0.0303775, acc 0.984375\n",
      "2017-01-11T00:12:55.236123: step 5725, loss 0.0345791, acc 0.992188\n",
      "2017-01-11T00:12:57.276047: step 5726, loss 0.0560476, acc 0.976562\n",
      "2017-01-11T00:12:59.268263: step 5727, loss 0.0105484, acc 0.992188\n",
      "2017-01-11T00:13:01.378140: step 5728, loss 0.0352268, acc 0.992188\n",
      "2017-01-11T00:13:03.426544: step 5729, loss 0.00369144, acc 1\n",
      "2017-01-11T00:13:05.471126: step 5730, loss 0.0235767, acc 0.992188\n",
      "2017-01-11T00:13:07.511753: step 5731, loss 0.0221524, acc 0.992188\n",
      "2017-01-11T00:13:09.867085: step 5732, loss 0.105603, acc 0.976562\n",
      "2017-01-11T00:13:11.906657: step 5733, loss 0.049675, acc 0.984375\n",
      "2017-01-11T00:13:13.978142: step 5734, loss 0.00895845, acc 1\n",
      "2017-01-11T00:13:15.995068: step 5735, loss 0.00749572, acc 1\n",
      "2017-01-11T00:13:18.026558: step 5736, loss 0.0308529, acc 0.984375\n",
      "2017-01-11T00:13:20.125701: step 5737, loss 0.0157505, acc 0.992188\n",
      "2017-01-11T00:13:22.175910: step 5738, loss 0.0765016, acc 0.984375\n",
      "2017-01-11T00:13:24.227244: step 5739, loss 0.0354077, acc 0.992188\n",
      "2017-01-11T00:13:26.252447: step 5740, loss 0.0721004, acc 0.976562\n",
      "2017-01-11T00:13:28.282911: step 5741, loss 0.00142159, acc 1\n",
      "2017-01-11T00:13:30.271243: step 5742, loss 0.052701, acc 0.984375\n",
      "2017-01-11T00:13:32.304356: step 5743, loss 0.066325, acc 0.984375\n",
      "2017-01-11T00:13:34.561149: step 5744, loss 0.0427894, acc 0.992188\n",
      "2017-01-11T00:13:36.560623: step 5745, loss 0.0401471, acc 0.984375\n",
      "2017-01-11T00:13:38.569701: step 5746, loss 0.0350482, acc 0.992188\n",
      "2017-01-11T00:13:40.909615: step 5747, loss 0.0390078, acc 0.992188\n",
      "2017-01-11T00:13:42.912399: step 5748, loss 0.0547756, acc 0.984375\n",
      "2017-01-11T00:13:44.981340: step 5749, loss 0.0382295, acc 0.984375\n",
      "2017-01-11T00:13:47.028118: step 5750, loss 0.0066934, acc 1\n",
      "2017-01-11T00:13:49.080327: step 5751, loss 0.0515868, acc 0.992188\n",
      "2017-01-11T00:13:52.225638: step 5752, loss 0.0221379, acc 0.992188\n",
      "2017-01-11T00:13:54.241976: step 5753, loss 0.0624059, acc 0.984375\n",
      "2017-01-11T00:13:56.269991: step 5754, loss 0.0251555, acc 0.992188\n",
      "2017-01-11T00:13:58.299968: step 5755, loss 0.0446902, acc 0.984375\n",
      "2017-01-11T00:14:00.370799: step 5756, loss 0.0173073, acc 1\n",
      "2017-01-11T00:14:02.526863: step 5757, loss 0.0452109, acc 0.984375\n",
      "2017-01-11T00:14:04.593766: step 5758, loss 0.0395983, acc 0.984375\n",
      "2017-01-11T00:14:06.623517: step 5759, loss 0.0281063, acc 1\n",
      "2017-01-11T00:14:08.662379: step 5760, loss 0.00860561, acc 0.992188\n",
      "2017-01-11T00:14:10.672702: step 5761, loss 0.222452, acc 0.984375\n",
      "2017-01-11T00:14:13.012149: step 5762, loss 0.0462809, acc 0.984375\n",
      "2017-01-11T00:14:15.019235: step 5763, loss 0.0278307, acc 0.992188\n",
      "2017-01-11T00:14:17.049061: step 5764, loss 0.127092, acc 0.984375\n",
      "2017-01-11T00:14:19.137198: step 5765, loss 0.0149134, acc 0.992188\n",
      "2017-01-11T00:14:21.219605: step 5766, loss 0.0259386, acc 0.984375\n",
      "2017-01-11T00:14:23.260791: step 5767, loss 0.05245, acc 0.984375\n",
      "2017-01-11T00:14:25.283272: step 5768, loss 0.0434152, acc 0.984375\n",
      "2017-01-11T00:14:27.301656: step 5769, loss 0.0456728, acc 0.992188\n",
      "2017-01-11T00:14:29.340342: step 5770, loss 0.0245339, acc 0.984375\n",
      "2017-01-11T00:14:31.448877: step 5771, loss 0.0392875, acc 0.984375\n",
      "2017-01-11T00:14:33.452898: step 5772, loss 0.034309, acc 0.992188\n",
      "2017-01-11T00:14:35.479675: step 5773, loss 0.0799327, acc 0.976562\n",
      "2017-01-11T00:14:37.486428: step 5774, loss 0.0249217, acc 0.992188\n",
      "2017-01-11T00:14:39.512240: step 5775, loss 0.0662421, acc 0.976562\n",
      "2017-01-11T00:14:41.546256: step 5776, loss 0.0750764, acc 0.976562\n",
      "2017-01-11T00:14:43.744580: step 5777, loss 0.0208408, acc 0.992188\n",
      "2017-01-11T00:14:45.911021: step 5778, loss 0.0220095, acc 0.992188\n",
      "2017-01-11T00:14:47.953532: step 5779, loss 0.0541207, acc 0.984375\n",
      "2017-01-11T00:14:49.972896: step 5780, loss 0.0328278, acc 0.992188\n",
      "2017-01-11T00:14:52.005815: step 5781, loss 0.0173326, acc 1\n",
      "2017-01-11T00:14:54.003405: step 5782, loss 0.0477273, acc 0.984375\n",
      "2017-01-11T00:14:56.077436: step 5783, loss 0.0345326, acc 0.984375\n",
      "2017-01-11T00:14:58.065444: step 5784, loss 0.0627131, acc 0.976562\n",
      "2017-01-11T00:15:00.113353: step 5785, loss 0.112578, acc 0.960938\n",
      "2017-01-11T00:15:02.237415: step 5786, loss 0.0192096, acc 0.992188\n",
      "2017-01-11T00:15:04.280123: step 5787, loss 0.0401886, acc 0.992188\n",
      "2017-01-11T00:15:06.357259: step 5788, loss 0.0268958, acc 0.992188\n",
      "2017-01-11T00:15:08.470232: step 5789, loss 0.0601162, acc 0.984375\n",
      "2017-01-11T00:15:10.598595: step 5790, loss 0.0404368, acc 0.984375\n",
      "2017-01-11T00:15:12.683583: step 5791, loss 0.0668776, acc 0.984375\n",
      "2017-01-11T00:15:14.812968: step 5792, loss 0.0576175, acc 0.984375\n",
      "2017-01-11T00:15:17.239397: step 5793, loss 0.0261948, acc 1\n",
      "2017-01-11T00:15:19.371504: step 5794, loss 0.0366696, acc 0.984375\n",
      "2017-01-11T00:15:21.505762: step 5795, loss 0.0111333, acc 1\n",
      "2017-01-11T00:15:23.658512: step 5796, loss 0.0266305, acc 0.992188\n",
      "2017-01-11T00:15:25.810037: step 5797, loss 0.0528616, acc 0.984375\n",
      "2017-01-11T00:15:27.943118: step 5798, loss 0.0163484, acc 1\n",
      "2017-01-11T00:15:30.462464: step 5799, loss 0.004472, acc 1\n",
      "2017-01-11T00:15:32.571972: step 5800, loss 0.0183097, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:15:56.428441: step 5800, loss 0.0702104, acc 0.98052\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5800\n",
      "\n",
      "2017-01-11T00:16:01.054834: step 5801, loss 0.0782665, acc 0.984375\n",
      "2017-01-11T00:16:03.214224: step 5802, loss 0.00173042, acc 1\n",
      "2017-01-11T00:16:05.359053: step 5803, loss 0.0107629, acc 1\n",
      "2017-01-11T00:16:07.534873: step 5804, loss 0.0268748, acc 0.992188\n",
      "2017-01-11T00:16:09.645705: step 5805, loss 0.00240454, acc 1\n",
      "2017-01-11T00:16:11.815051: step 5806, loss 0.0272933, acc 0.992188\n",
      "2017-01-11T00:16:13.953937: step 5807, loss 0.0692285, acc 0.984375\n",
      "2017-01-11T00:16:16.052187: step 5808, loss 0.0517771, acc 0.984375\n",
      "2017-01-11T00:16:18.148332: step 5809, loss 0.00899198, acc 0.992188\n",
      "2017-01-11T00:16:20.290333: step 5810, loss 0.0230117, acc 0.992188\n",
      "2017-01-11T00:16:22.723189: step 5811, loss 0.0664186, acc 0.984375\n",
      "2017-01-11T00:16:24.802316: step 5812, loss 0.0415835, acc 0.984375\n",
      "2017-01-11T00:16:26.992534: step 5813, loss 0.0359049, acc 0.992188\n",
      "2017-01-11T00:16:29.202816: step 5814, loss 0.0618044, acc 0.96875\n",
      "2017-01-11T00:16:31.486950: step 5815, loss 0.0330699, acc 0.984375\n",
      "2017-01-11T00:16:33.698750: step 5816, loss 0.100557, acc 0.96875\n",
      "2017-01-11T00:16:35.852212: step 5817, loss 0.0276233, acc 0.992188\n",
      "2017-01-11T00:16:38.004964: step 5818, loss 0.0395388, acc 0.984375\n",
      "2017-01-11T00:16:40.098036: step 5819, loss 0.0313158, acc 0.992188\n",
      "2017-01-11T00:16:42.238173: step 5820, loss 0.044331, acc 0.984375\n",
      "2017-01-11T00:16:44.320263: step 5821, loss 0.0424092, acc 0.976562\n",
      "2017-01-11T00:16:46.442961: step 5822, loss 0.0177411, acc 1\n",
      "2017-01-11T00:16:48.558636: step 5823, loss 0.0297656, acc 0.992188\n",
      "2017-01-11T00:16:50.705489: step 5824, loss 0.0304039, acc 0.992188\n",
      "2017-01-11T00:16:52.870968: step 5825, loss 0.0203405, acc 1\n",
      "2017-01-11T00:16:55.323244: step 5826, loss 0.0528289, acc 0.992188\n",
      "2017-01-11T00:16:57.361267: step 5827, loss 0.0122221, acc 0.992188\n",
      "2017-01-11T00:16:59.464810: step 5828, loss 0.0695321, acc 0.984375\n",
      "2017-01-11T00:17:01.598001: step 5829, loss 0.0551652, acc 0.976562\n",
      "2017-01-11T00:17:03.820264: step 5830, loss 0.0200152, acc 1\n",
      "2017-01-11T00:17:06.015077: step 5831, loss 0.00621516, acc 1\n",
      "2017-01-11T00:17:08.138384: step 5832, loss 0.0619838, acc 0.984375\n",
      "2017-01-11T00:17:10.285595: step 5833, loss 0.0239262, acc 0.992188\n",
      "2017-01-11T00:17:12.357936: step 5834, loss 0.0337456, acc 0.992188\n",
      "2017-01-11T00:17:14.444699: step 5835, loss 0.0336866, acc 0.992188\n",
      "2017-01-11T00:17:16.516334: step 5836, loss 0.00367665, acc 1\n",
      "2017-01-11T00:17:18.613509: step 5837, loss 0.0677732, acc 0.976562\n",
      "2017-01-11T00:17:20.775715: step 5838, loss 0.00231468, acc 1\n",
      "2017-01-11T00:17:22.938682: step 5839, loss 0.0225007, acc 0.992188\n",
      "2017-01-11T00:17:25.043855: step 5840, loss 0.0557766, acc 0.96875\n",
      "2017-01-11T00:17:27.554765: step 5841, loss 0.0744561, acc 0.96875\n",
      "2017-01-11T00:17:29.682337: step 5842, loss 0.106137, acc 0.960938\n",
      "2017-01-11T00:17:31.800637: step 5843, loss 0.0414686, acc 0.992188\n",
      "2017-01-11T00:17:34.163890: step 5844, loss 0.041984, acc 0.992188\n",
      "2017-01-11T00:17:36.344621: step 5845, loss 0.0449082, acc 0.984375\n",
      "2017-01-11T00:17:38.513077: step 5846, loss 0.0364782, acc 0.992188\n",
      "2017-01-11T00:17:40.811782: step 5847, loss 0.0403303, acc 0.984375\n",
      "2017-01-11T00:17:43.289825: step 5848, loss 0.0816165, acc 0.976562\n",
      "2017-01-11T00:17:45.792164: step 5849, loss 0.0459021, acc 0.984375\n",
      "2017-01-11T00:17:47.906957: step 5850, loss 0.0287276, acc 0.992188\n",
      "2017-01-11T00:17:51.255700: step 5851, loss 0.0433676, acc 0.984375\n",
      "2017-01-11T00:17:53.699192: step 5852, loss 0.064211, acc 0.976562\n",
      "2017-01-11T00:17:55.899933: step 5853, loss 0.0498552, acc 0.984375\n",
      "2017-01-11T00:17:58.257831: step 5854, loss 0.0542222, acc 0.984375\n",
      "2017-01-11T00:18:00.598943: step 5855, loss 0.0293316, acc 0.992188\n",
      "2017-01-11T00:18:02.747007: step 5856, loss 0.0163523, acc 1\n",
      "2017-01-11T00:18:04.973674: step 5857, loss 0.0252959, acc 0.992188\n",
      "2017-01-11T00:18:07.056138: step 5858, loss 0.0431182, acc 0.984375\n",
      "2017-01-11T00:18:09.125506: step 5859, loss 0.105805, acc 0.984375\n",
      "2017-01-11T00:18:11.154708: step 5860, loss 0.0702536, acc 0.984375\n",
      "2017-01-11T00:18:13.177454: step 5861, loss 0.0842918, acc 0.96875\n",
      "2017-01-11T00:18:15.220785: step 5862, loss 0.0815054, acc 0.976562\n",
      "2017-01-11T00:18:17.257599: step 5863, loss 0.0299867, acc 0.992188\n",
      "2017-01-11T00:18:19.486955: step 5864, loss 0.046555, acc 0.984375\n",
      "2017-01-11T00:18:21.593289: step 5865, loss 0.0209971, acc 1\n",
      "2017-01-11T00:18:23.623790: step 5866, loss 0.0074814, acc 1\n",
      "2017-01-11T00:18:25.659582: step 5867, loss 0.0371344, acc 0.992188\n",
      "2017-01-11T00:18:27.687747: step 5868, loss 0.0365902, acc 0.992188\n",
      "2017-01-11T00:18:29.810220: step 5869, loss 0.0300079, acc 0.992188\n",
      "2017-01-11T00:18:32.122400: step 5870, loss 0.00554583, acc 1\n",
      "2017-01-11T00:18:34.136159: step 5871, loss 0.0662331, acc 0.984375\n",
      "2017-01-11T00:18:36.198810: step 5872, loss 0.150601, acc 0.992188\n",
      "2017-01-11T00:18:38.232725: step 5873, loss 0.013244, acc 1\n",
      "2017-01-11T00:18:40.429710: step 5874, loss 0.0305268, acc 0.984375\n",
      "2017-01-11T00:18:42.449262: step 5875, loss 0.0827153, acc 0.984375\n",
      "2017-01-11T00:18:44.507596: step 5876, loss 0.010343, acc 1\n",
      "2017-01-11T00:18:46.523585: step 5877, loss 0.0516408, acc 0.976562\n",
      "2017-01-11T00:18:48.560206: step 5878, loss 0.0162925, acc 1\n",
      "2017-01-11T00:18:50.658694: step 5879, loss 0.126401, acc 0.984375\n",
      "2017-01-11T00:18:52.736619: step 5880, loss 0.0167687, acc 1\n",
      "2017-01-11T00:18:54.769482: step 5881, loss 0.0852178, acc 0.976562\n",
      "2017-01-11T00:18:57.579682: step 5882, loss 0.0247991, acc 0.992188\n",
      "2017-01-11T00:18:59.819402: step 5883, loss 0.0460239, acc 0.984375\n",
      "2017-01-11T00:19:01.919098: step 5884, loss 0.00136282, acc 1\n",
      "2017-01-11T00:19:04.366685: step 5885, loss 0.0319288, acc 0.992188\n",
      "2017-01-11T00:19:06.406342: step 5886, loss 0.0490084, acc 0.984375\n",
      "2017-01-11T00:19:08.446088: step 5887, loss 0.018858, acc 0.992188\n",
      "2017-01-11T00:19:10.489184: step 5888, loss 0.0575628, acc 0.992188\n",
      "2017-01-11T00:19:12.509816: step 5889, loss 0.0747671, acc 0.96875\n",
      "2017-01-11T00:19:14.534052: step 5890, loss 0.0546006, acc 0.976562\n",
      "2017-01-11T00:19:16.558091: step 5891, loss 0.00401573, acc 1\n",
      "2017-01-11T00:19:18.589785: step 5892, loss 0.0180304, acc 1\n",
      "2017-01-11T00:19:20.644477: step 5893, loss 0.0537977, acc 0.984375\n",
      "2017-01-11T00:19:22.692912: step 5894, loss 0.00219503, acc 1\n",
      "2017-01-11T00:19:24.722915: step 5895, loss 0.0885927, acc 0.960938\n",
      "2017-01-11T00:19:26.759360: step 5896, loss 0.0710541, acc 0.984375\n",
      "2017-01-11T00:19:28.790132: step 5897, loss 0.0189941, acc 0.992188\n",
      "2017-01-11T00:19:30.838177: step 5898, loss 0.0227921, acc 0.992188\n",
      "2017-01-11T00:19:32.847014: step 5899, loss 0.0272979, acc 0.984375\n",
      "2017-01-11T00:19:35.186915: step 5900, loss 0.0587412, acc 0.960938\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:19:57.081694: step 5900, loss 0.0712193, acc 0.9804\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-5900\n",
      "\n",
      "2017-01-11T00:20:01.772761: step 5901, loss 0.028701, acc 0.992188\n",
      "2017-01-11T00:20:03.883632: step 5902, loss 0.0610442, acc 0.984375\n",
      "2017-01-11T00:20:06.054930: step 5903, loss 0.0355982, acc 0.992188\n",
      "2017-01-11T00:20:08.443545: step 5904, loss 0.02617, acc 0.992188\n",
      "2017-01-11T00:20:10.497390: step 5905, loss 0.0265173, acc 0.992188\n",
      "2017-01-11T00:20:12.523421: step 5906, loss 0.0128513, acc 1\n",
      "2017-01-11T00:20:14.572822: step 5907, loss 0.0145812, acc 0.992188\n",
      "2017-01-11T00:20:16.590833: step 5908, loss 0.0107971, acc 0.992188\n",
      "2017-01-11T00:20:18.606071: step 5909, loss 0.0702234, acc 0.984375\n",
      "2017-01-11T00:20:20.669311: step 5910, loss 0.00410231, acc 1\n",
      "2017-01-11T00:20:22.744221: step 5911, loss 0.00202047, acc 1\n",
      "2017-01-11T00:20:24.766812: step 5912, loss 0.0199696, acc 1\n",
      "2017-01-11T00:20:26.779620: step 5913, loss 0.0570568, acc 0.984375\n",
      "2017-01-11T00:20:29.007712: step 5914, loss 0.0182025, acc 0.992188\n",
      "2017-01-11T00:20:31.182782: step 5915, loss 0.0602937, acc 0.984375\n",
      "2017-01-11T00:20:33.187427: step 5916, loss 0.00568899, acc 1\n",
      "2017-01-11T00:20:35.207319: step 5917, loss 0.0581552, acc 0.984375\n",
      "2017-01-11T00:20:37.248063: step 5918, loss 0.0637466, acc 0.96875\n",
      "2017-01-11T00:20:39.415180: step 5919, loss 0.102774, acc 0.96875\n",
      "2017-01-11T00:20:41.581197: step 5920, loss 0.00529957, acc 1\n",
      "2017-01-11T00:20:43.893575: step 5921, loss 0.0840607, acc 0.976562\n",
      "2017-01-11T00:20:46.822229: step 5922, loss 0.0708039, acc 0.992188\n",
      "2017-01-11T00:20:48.839830: step 5923, loss 0.0429888, acc 0.984375\n",
      "2017-01-11T00:20:50.902695: step 5924, loss 0.00728295, acc 1\n",
      "2017-01-11T00:20:52.991208: step 5925, loss 0.0227603, acc 0.992188\n",
      "2017-01-11T00:20:54.996908: step 5926, loss 0.0320489, acc 0.992188\n",
      "2017-01-11T00:20:57.019016: step 5927, loss 0.0435052, acc 0.984375\n",
      "2017-01-11T00:20:59.050225: step 5928, loss 0.022313, acc 0.992188\n",
      "2017-01-11T00:21:01.155226: step 5929, loss 0.00253629, acc 1\n",
      "2017-01-11T00:21:03.172174: step 5930, loss 0.00940067, acc 1\n",
      "2017-01-11T00:21:05.346375: step 5931, loss 0.127271, acc 0.960938\n",
      "2017-01-11T00:21:07.367965: step 5932, loss 0.0590293, acc 0.984375\n",
      "2017-01-11T00:21:09.446490: step 5933, loss 0.0334472, acc 0.992188\n",
      "2017-01-11T00:21:11.574098: step 5934, loss 0.0366123, acc 0.992188\n",
      "2017-01-11T00:21:13.837530: step 5935, loss 0.0332655, acc 0.992188\n",
      "2017-01-11T00:21:15.872292: step 5936, loss 0.0605279, acc 0.984375\n",
      "2017-01-11T00:21:17.897429: step 5937, loss 0.0193453, acc 1\n",
      "2017-01-11T00:21:19.950448: step 5938, loss 0.00784742, acc 1\n",
      "2017-01-11T00:21:22.038798: step 5939, loss 0.0110831, acc 1\n",
      "2017-01-11T00:21:24.066240: step 5940, loss 0.0590302, acc 0.984375\n",
      "2017-01-11T00:21:26.107808: step 5941, loss 0.0645854, acc 0.984375\n",
      "2017-01-11T00:21:28.141181: step 5942, loss 0.0394076, acc 0.984375\n",
      "2017-01-11T00:21:30.174438: step 5943, loss 0.00580084, acc 1\n",
      "2017-01-11T00:21:32.218295: step 5944, loss 0.01529, acc 0.992188\n",
      "2017-01-11T00:21:34.233722: step 5945, loss 0.0189707, acc 0.992188\n",
      "2017-01-11T00:21:36.309673: step 5946, loss 0.0685395, acc 0.96875\n",
      "2017-01-11T00:21:38.319048: step 5947, loss 0.0211239, acc 0.992188\n",
      "2017-01-11T00:21:40.305662: step 5948, loss 0.0410449, acc 0.984375\n",
      "2017-01-11T00:21:42.343326: step 5949, loss 0.00937767, acc 0.992188\n",
      "2017-01-11T00:21:44.667744: step 5950, loss 0.0013271, acc 1\n",
      "2017-01-11T00:21:46.688668: step 5951, loss 0.199273, acc 0.960938\n",
      "2017-01-11T00:21:48.733587: step 5952, loss 0.0352537, acc 0.992188\n",
      "2017-01-11T00:21:50.792132: step 5953, loss 0.0734182, acc 0.992188\n",
      "2017-01-11T00:21:52.861284: step 5954, loss 0.0426792, acc 0.984375\n",
      "2017-01-11T00:21:54.905769: step 5955, loss 0.0688593, acc 0.984375\n",
      "2017-01-11T00:21:56.957062: step 5956, loss 0.0450466, acc 0.984375\n",
      "2017-01-11T00:21:58.979408: step 5957, loss 0.033212, acc 0.984375\n",
      "2017-01-11T00:22:01.047806: step 5958, loss 0.0144316, acc 1\n",
      "2017-01-11T00:22:03.089159: step 5959, loss 0.0642607, acc 0.984375\n",
      "2017-01-11T00:22:05.208495: step 5960, loss 0.033988, acc 0.992188\n",
      "2017-01-11T00:22:07.199459: step 5961, loss 0.0552742, acc 0.96875\n",
      "2017-01-11T00:22:09.258954: step 5962, loss 0.0118574, acc 1\n",
      "2017-01-11T00:22:11.271079: step 5963, loss 0.0387113, acc 0.984375\n",
      "2017-01-11T00:22:13.305644: step 5964, loss 0.0439644, acc 0.992188\n",
      "2017-01-11T00:22:15.521900: step 5965, loss 0.056211, acc 0.976562\n",
      "2017-01-11T00:22:17.683428: step 5966, loss 0.00407451, acc 1\n",
      "2017-01-11T00:22:19.710710: step 5967, loss 0.0332597, acc 0.984375\n",
      "2017-01-11T00:22:21.787489: step 5968, loss 0.0204515, acc 0.992188\n",
      "2017-01-11T00:22:23.847748: step 5969, loss 0.0811668, acc 0.976562\n",
      "2017-01-11T00:22:25.889369: step 5970, loss 0.0408483, acc 0.992188\n",
      "2017-01-11T00:22:27.946370: step 5971, loss 0.0304399, acc 0.984375\n",
      "2017-01-11T00:22:30.028419: step 5972, loss 0.0076436, acc 1\n",
      "2017-01-11T00:22:32.095845: step 5973, loss 0.0278159, acc 1\n",
      "2017-01-11T00:22:34.123382: step 5974, loss 0.0504104, acc 0.984375\n",
      "2017-01-11T00:22:36.183649: step 5975, loss 0.0262405, acc 0.984375\n",
      "2017-01-11T00:22:38.204821: step 5976, loss 0.020514, acc 0.992188\n",
      "2017-01-11T00:22:40.412434: step 5977, loss 0.0199993, acc 0.992188\n",
      "2017-01-11T00:22:42.449646: step 5978, loss 0.0482918, acc 0.984375\n",
      "2017-01-11T00:22:44.470340: step 5979, loss 0.0544533, acc 0.984375\n",
      "2017-01-11T00:22:46.523335: step 5980, loss 0.00551442, acc 1\n",
      "2017-01-11T00:22:48.821696: step 5981, loss 0.0601691, acc 0.984375\n",
      "2017-01-11T00:22:50.932793: step 5982, loss 0.00115558, acc 1\n",
      "2017-01-11T00:22:52.982741: step 5983, loss 0.0244697, acc 0.992188\n",
      "2017-01-11T00:22:55.393391: step 5984, loss 0.00142734, acc 1\n",
      "2017-01-11T00:22:58.151602: step 5985, loss 0.105335, acc 0.96875\n",
      "2017-01-11T00:23:00.181376: step 5986, loss 0.0405167, acc 0.992188\n",
      "2017-01-11T00:23:02.197134: step 5987, loss 0.0664591, acc 0.976562\n",
      "2017-01-11T00:23:04.279087: step 5988, loss 0.0488014, acc 0.984375\n",
      "2017-01-11T00:23:06.493260: step 5989, loss 0.0190502, acc 0.992188\n",
      "2017-01-11T00:23:08.551451: step 5990, loss 0.0262968, acc 0.984375\n",
      "2017-01-11T00:23:10.599429: step 5991, loss 0.0344645, acc 0.984375\n",
      "2017-01-11T00:23:12.631919: step 5992, loss 0.0872184, acc 0.976562\n",
      "2017-01-11T00:23:14.704320: step 5993, loss 0.0753682, acc 0.96875\n",
      "2017-01-11T00:23:16.730735: step 5994, loss 0.0533168, acc 0.96875\n",
      "2017-01-11T00:23:18.782941: step 5995, loss 0.00494237, acc 1\n",
      "2017-01-11T00:23:21.167888: step 5996, loss 0.0533748, acc 0.976562\n",
      "2017-01-11T00:23:23.208731: step 5997, loss 0.0196938, acc 1\n",
      "2017-01-11T00:23:25.262212: step 5998, loss 0.0130006, acc 0.992188\n",
      "2017-01-11T00:23:27.311804: step 5999, loss 0.00125409, acc 1\n",
      "2017-01-11T00:23:29.328295: step 6000, loss 0.0273658, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:23:53.365600: step 6000, loss 0.0724315, acc 0.9804\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6000\n",
      "\n",
      "2017-01-11T00:23:57.826164: step 6001, loss 0.00062558, acc 1\n",
      "2017-01-11T00:23:59.829919: step 6002, loss 0.0249353, acc 1\n",
      "2017-01-11T00:24:02.561507: step 6003, loss 0.0660612, acc 0.976562\n",
      "2017-01-11T00:24:04.978070: step 6004, loss 0.0882522, acc 0.953125\n",
      "2017-01-11T00:24:07.083961: step 6005, loss 0.0104594, acc 1\n",
      "2017-01-11T00:24:09.120244: step 6006, loss 0.0883601, acc 0.96875\n",
      "2017-01-11T00:24:11.150836: step 6007, loss 0.122113, acc 0.96875\n",
      "2017-01-11T00:24:13.243034: step 6008, loss 0.0157457, acc 1\n",
      "2017-01-11T00:24:15.280467: step 6009, loss 0.028179, acc 0.984375\n",
      "2017-01-11T00:24:17.349854: step 6010, loss 0.0525361, acc 0.984375\n",
      "2017-01-11T00:24:19.404998: step 6011, loss 0.0759423, acc 0.984375\n",
      "2017-01-11T00:24:21.498733: step 6012, loss 0.0620072, acc 0.976562\n",
      "2017-01-11T00:24:23.546625: step 6013, loss 0.000320184, acc 1\n",
      "2017-01-11T00:24:25.888030: step 6014, loss 0.0259165, acc 0.984375\n",
      "2017-01-11T00:24:27.955597: step 6015, loss 0.138022, acc 0.984375\n",
      "2017-01-11T00:24:30.017731: step 6016, loss 0.0319407, acc 0.992188\n",
      "2017-01-11T00:24:32.086686: step 6017, loss 0.0546919, acc 0.984375\n",
      "2017-01-11T00:24:34.097059: step 6018, loss 0.0306157, acc 0.992188\n",
      "2017-01-11T00:24:36.175339: step 6019, loss 0.0653074, acc 0.976562\n",
      "2017-01-11T00:24:38.182302: step 6020, loss 0.0354247, acc 0.992188\n",
      "2017-01-11T00:24:40.251334: step 6021, loss 0.061584, acc 0.984375\n",
      "2017-01-11T00:24:42.286602: step 6022, loss 0.0386198, acc 0.992188\n",
      "2017-01-11T00:24:44.321271: step 6023, loss 0.0346361, acc 0.992188\n",
      "2017-01-11T00:24:46.333687: step 6024, loss 0.0178454, acc 0.992188\n",
      "2017-01-11T00:24:48.373312: step 6025, loss 0.0332229, acc 0.992188\n",
      "2017-01-11T00:24:50.451554: step 6026, loss 0.0364253, acc 0.984375\n",
      "2017-01-11T00:24:52.517877: step 6027, loss 0.00122835, acc 1\n",
      "2017-01-11T00:24:54.570203: step 6028, loss 0.0980629, acc 0.96875\n",
      "2017-01-11T00:24:56.910373: step 6029, loss 0.00440185, acc 1\n",
      "2017-01-11T00:24:58.947925: step 6030, loss 0.0362182, acc 0.984375\n",
      "2017-01-11T00:25:00.987314: step 6031, loss 0.0463622, acc 0.984375\n",
      "2017-01-11T00:25:03.045152: step 6032, loss 0.054167, acc 0.992188\n",
      "2017-01-11T00:25:05.115866: step 6033, loss 0.0249573, acc 0.984375\n",
      "2017-01-11T00:25:07.239129: step 6034, loss 0.0299032, acc 0.992188\n",
      "2017-01-11T00:25:09.281558: step 6035, loss 0.067629, acc 0.984375\n",
      "2017-01-11T00:25:11.299022: step 6036, loss 0.0156377, acc 0.992188\n",
      "2017-01-11T00:25:13.363777: step 6037, loss 0.0032972, acc 1\n",
      "2017-01-11T00:25:15.382352: step 6038, loss 0.048265, acc 0.984375\n",
      "2017-01-11T00:25:17.451492: step 6039, loss 0.0641972, acc 0.96875\n",
      "2017-01-11T00:25:19.486195: step 6040, loss 0.0448695, acc 0.984375\n",
      "2017-01-11T00:25:21.591942: step 6041, loss 0.0851731, acc 0.960938\n",
      "2017-01-11T00:25:23.602317: step 6042, loss 0.0205098, acc 0.992188\n",
      "2017-01-11T00:25:25.681118: step 6043, loss 0.0605599, acc 0.96875\n",
      "2017-01-11T00:25:27.866560: step 6044, loss 0.036571, acc 0.992188\n",
      "2017-01-11T00:25:30.464340: step 6045, loss 0.0562113, acc 0.976562\n",
      "2017-01-11T00:25:32.479326: step 6046, loss 0.00529449, acc 1\n",
      "2017-01-11T00:25:34.529312: step 6047, loss 0.048333, acc 0.984375\n",
      "2017-01-11T00:25:36.535944: step 6048, loss 0.0493232, acc 0.984375\n",
      "2017-01-11T00:25:38.551129: step 6049, loss 4.44678e-05, acc 1\n",
      "2017-01-11T00:25:40.569028: step 6050, loss 0.0527136, acc 0.984375\n",
      "2017-01-11T00:25:42.609605: step 6051, loss 0.0926699, acc 0.960938\n",
      "2017-01-11T00:25:45.002164: step 6052, loss 0.000259458, acc 1\n",
      "2017-01-11T00:25:47.802288: step 6053, loss 0.0240773, acc 0.992188\n",
      "2017-01-11T00:25:49.846087: step 6054, loss 0.0609156, acc 0.984375\n",
      "2017-01-11T00:25:51.950625: step 6055, loss 0.076311, acc 0.96875\n",
      "2017-01-11T00:25:54.009229: step 6056, loss 0.0823644, acc 0.984375\n",
      "2017-01-11T00:25:56.076685: step 6057, loss 0.131758, acc 0.984375\n",
      "2017-01-11T00:25:58.119040: step 6058, loss 0.0458305, acc 0.976562\n",
      "2017-01-11T00:26:00.242046: step 6059, loss 0.0809384, acc 0.976562\n",
      "2017-01-11T00:26:02.480302: step 6060, loss 0.073408, acc 0.976562\n",
      "2017-01-11T00:26:04.555253: step 6061, loss 0.0591586, acc 0.984375\n",
      "2017-01-11T00:26:06.632662: step 6062, loss 0.0374795, acc 0.992188\n",
      "2017-01-11T00:26:08.678589: step 6063, loss 0.045704, acc 0.976562\n",
      "2017-01-11T00:26:10.727671: step 6064, loss 0.0411686, acc 0.992188\n",
      "2017-01-11T00:26:12.757845: step 6065, loss 0.0783572, acc 0.96875\n",
      "2017-01-11T00:26:14.778364: step 6066, loss 0.0373456, acc 0.992188\n",
      "2017-01-11T00:26:16.845804: step 6067, loss 0.0419221, acc 0.992188\n",
      "2017-01-11T00:26:18.866695: step 6068, loss 0.0389761, acc 0.984375\n",
      "2017-01-11T00:26:20.964203: step 6069, loss 0.0557804, acc 0.976562\n",
      "2017-01-11T00:26:23.035071: step 6070, loss 0.0673024, acc 0.976562\n",
      "2017-01-11T00:26:25.060278: step 6071, loss 0.0194181, acc 0.992188\n",
      "2017-01-11T00:26:27.108594: step 6072, loss 0.0667318, acc 0.976562\n",
      "2017-01-11T00:26:29.145321: step 6073, loss 0.0475271, acc 0.992188\n",
      "2017-01-11T00:26:31.287131: step 6074, loss 0.0322897, acc 0.992188\n",
      "2017-01-11T00:26:33.629677: step 6075, loss 0.0813797, acc 0.96875\n",
      "2017-01-11T00:26:35.675228: step 6076, loss 0.00713619, acc 1\n",
      "2017-01-11T00:26:37.710523: step 6077, loss 0.0300501, acc 0.984375\n",
      "2017-01-11T00:26:39.743193: step 6078, loss 0.145019, acc 0.953125\n",
      "2017-01-11T00:26:41.802877: step 6079, loss 0.0789019, acc 0.96875\n",
      "2017-01-11T00:26:43.834055: step 6080, loss 0.0224353, acc 1\n",
      "2017-01-11T00:26:45.850895: step 6081, loss 0.0269177, acc 0.992188\n",
      "2017-01-11T00:26:47.880646: step 6082, loss 0.0287879, acc 0.984375\n",
      "2017-01-11T00:26:49.900016: step 6083, loss 0.0520053, acc 0.984375\n",
      "2017-01-11T00:26:51.982487: step 6084, loss 0.030715, acc 0.992188\n",
      "2017-01-11T00:26:54.052989: step 6085, loss 0.00112928, acc 1\n",
      "2017-01-11T00:26:56.077875: step 6086, loss 0.0460007, acc 0.984375\n",
      "2017-01-11T00:26:58.120060: step 6087, loss 0.0415718, acc 0.976562\n",
      "2017-01-11T00:27:00.154280: step 6088, loss 0.0586071, acc 0.976562\n",
      "2017-01-11T00:27:02.218432: step 6089, loss 0.0137543, acc 0.992188\n",
      "2017-01-11T00:27:04.338975: step 6090, loss 0.0738014, acc 0.976562\n",
      "2017-01-11T00:27:06.609287: step 6091, loss 0.00344222, acc 1\n",
      "2017-01-11T00:27:08.732935: step 6092, loss 0.0664426, acc 0.976562\n",
      "2017-01-11T00:27:10.794072: step 6093, loss 0.00705756, acc 1\n",
      "2017-01-11T00:27:12.835574: step 6094, loss 0.0532145, acc 0.976562\n",
      "2017-01-11T00:27:14.888710: step 6095, loss 0.0651593, acc 0.984375\n",
      "2017-01-11T00:27:16.927745: step 6096, loss 0.0158666, acc 0.992188\n",
      "2017-01-11T00:27:18.952992: step 6097, loss 0.0173466, acc 0.992188\n",
      "2017-01-11T00:27:21.003693: step 6098, loss 0.038834, acc 0.992188\n",
      "2017-01-11T00:27:23.049994: step 6099, loss 0.00859936, acc 1\n",
      "2017-01-11T00:27:25.088982: step 6100, loss 0.03047, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:27:49.827334: step 6100, loss 0.0693508, acc 0.98104\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6100\n",
      "\n",
      "2017-01-11T00:27:54.426109: step 6101, loss 0.00689193, acc 1\n",
      "2017-01-11T00:27:56.487781: step 6102, loss 0.0944874, acc 0.976562\n",
      "2017-01-11T00:27:58.545820: step 6103, loss 0.0497815, acc 0.984375\n",
      "2017-01-11T00:28:00.544457: step 6104, loss 0.0420483, acc 0.984375\n",
      "2017-01-11T00:28:03.355217: step 6105, loss 0.04117, acc 0.984375\n",
      "2017-01-11T00:28:05.634348: step 6106, loss 0.0230122, acc 1\n",
      "2017-01-11T00:28:07.714313: step 6107, loss 0.0251539, acc 0.992188\n",
      "2017-01-11T00:28:10.064801: step 6108, loss 0.0365733, acc 0.984375\n",
      "2017-01-11T00:28:12.113583: step 6109, loss 0.107825, acc 0.96875\n",
      "2017-01-11T00:28:14.174713: step 6110, loss 0.0644509, acc 0.96875\n",
      "2017-01-11T00:28:16.230195: step 6111, loss 0.0292104, acc 0.992188\n",
      "2017-01-11T00:28:18.274058: step 6112, loss 0.0297381, acc 0.992188\n",
      "2017-01-11T00:28:20.361541: step 6113, loss 0.0410474, acc 0.992188\n",
      "2017-01-11T00:28:22.448030: step 6114, loss 0.0107746, acc 1\n",
      "2017-01-11T00:28:24.482992: step 6115, loss 0.0268705, acc 0.984375\n",
      "2017-01-11T00:28:26.496248: step 6116, loss 0.059199, acc 0.984375\n",
      "2017-01-11T00:28:28.521176: step 6117, loss 0.00868054, acc 1\n",
      "2017-01-11T00:28:30.650535: step 6118, loss 0.057504, acc 0.984375\n",
      "2017-01-11T00:28:32.682036: step 6119, loss 0.0484451, acc 0.984375\n",
      "2017-01-11T00:28:34.755125: step 6120, loss 0.0250444, acc 0.992188\n",
      "2017-01-11T00:28:36.805542: step 6121, loss 0.00607603, acc 1\n",
      "2017-01-11T00:28:38.874151: step 6122, loss 0.016908, acc 0.992188\n",
      "2017-01-11T00:28:41.180816: step 6123, loss 0.0701974, acc 0.96875\n",
      "2017-01-11T00:28:43.245409: step 6124, loss 0.0393657, acc 0.984375\n",
      "2017-01-11T00:28:45.271404: step 6125, loss 0.00335072, acc 1\n",
      "2017-01-11T00:28:47.301331: step 6126, loss 0.039142, acc 0.992188\n",
      "2017-01-11T00:28:49.393864: step 6127, loss 0.113074, acc 0.953125\n",
      "2017-01-11T00:28:51.581382: step 6128, loss 0.0171965, acc 1\n",
      "2017-01-11T00:28:53.709371: step 6129, loss 0.0296118, acc 0.984375\n",
      "2017-01-11T00:28:55.758025: step 6130, loss 0.0918645, acc 0.960938\n",
      "2017-01-11T00:28:57.755744: step 6131, loss 0.0349924, acc 0.976562\n",
      "2017-01-11T00:28:59.765207: step 6132, loss 0.0199131, acc 1\n",
      "2017-01-11T00:29:01.782568: step 6133, loss 0.0268309, acc 0.992188\n",
      "2017-01-11T00:29:03.855489: step 6134, loss 0.0235116, acc 1\n",
      "2017-01-11T00:29:05.961017: step 6135, loss 0.111504, acc 0.960938\n",
      "2017-01-11T00:29:08.952592: step 6136, loss 0.0360738, acc 0.992188\n",
      "2017-01-11T00:29:11.241367: step 6137, loss 0.0646593, acc 0.984375\n",
      "2017-01-11T00:29:13.634249: step 6138, loss 0.0274689, acc 0.992188\n",
      "2017-01-11T00:29:15.656567: step 6139, loss 0.0127165, acc 1\n",
      "2017-01-11T00:29:17.688523: step 6140, loss 0.0224765, acc 0.992188\n",
      "2017-01-11T00:29:19.765066: step 6141, loss 0.039456, acc 0.984375\n",
      "2017-01-11T00:29:21.872193: step 6142, loss 0.0276184, acc 0.992188\n",
      "2017-01-11T00:29:23.918916: step 6143, loss 0.0460692, acc 0.984375\n",
      "2017-01-11T00:29:25.910701: step 6144, loss 0.00721154, acc 1\n",
      "2017-01-11T00:29:27.921232: step 6145, loss 0.0424602, acc 0.984375\n",
      "2017-01-11T00:29:29.960072: step 6146, loss 0.0586495, acc 0.984375\n",
      "2017-01-11T00:29:32.011705: step 6147, loss 0.12426, acc 0.960938\n",
      "2017-01-11T00:29:34.041964: step 6148, loss 0.0977293, acc 0.96875\n",
      "2017-01-11T00:29:36.097436: step 6149, loss 0.0022787, acc 1\n",
      "2017-01-11T00:29:38.124695: step 6150, loss 0.027157, acc 0.992188\n",
      "2017-01-11T00:29:40.207793: step 6151, loss 0.0185097, acc 1\n",
      "2017-01-11T00:29:42.269549: step 6152, loss 0.0851353, acc 0.976562\n",
      "2017-01-11T00:29:44.510439: step 6153, loss 0.00149032, acc 1\n",
      "2017-01-11T00:29:46.652621: step 6154, loss 0.0453918, acc 0.984375\n",
      "2017-01-11T00:29:48.702517: step 6155, loss 0.0883023, acc 0.976562\n",
      "2017-01-11T00:29:50.752104: step 6156, loss 0.0124817, acc 0.992188\n",
      "2017-01-11T00:29:52.856335: step 6157, loss 0.0149963, acc 0.992188\n",
      "2017-01-11T00:29:54.913175: step 6158, loss 0.0179717, acc 1\n",
      "2017-01-11T00:29:56.942411: step 6159, loss 0.0498508, acc 0.992188\n",
      "2017-01-11T00:29:58.988547: step 6160, loss 0.0388854, acc 0.984375\n",
      "2017-01-11T00:30:01.041556: step 6161, loss 0.00947231, acc 1\n",
      "2017-01-11T00:30:03.099038: step 6162, loss 0.0352801, acc 0.992188\n",
      "2017-01-11T00:30:05.198125: step 6163, loss 0.00878042, acc 1\n",
      "2017-01-11T00:30:07.261441: step 6164, loss 0.0172993, acc 0.992188\n",
      "2017-01-11T00:30:09.400157: step 6165, loss 0.0773917, acc 0.992188\n",
      "2017-01-11T00:30:11.442312: step 6166, loss 0.0335586, acc 0.992188\n",
      "2017-01-11T00:30:13.436153: step 6167, loss 0.041672, acc 0.976562\n",
      "2017-01-11T00:30:15.481368: step 6168, loss 0.00753919, acc 1\n",
      "2017-01-11T00:30:17.874774: step 6169, loss 0.0454961, acc 0.984375\n",
      "2017-01-11T00:30:19.966008: step 6170, loss 0.0426642, acc 0.984375\n",
      "2017-01-11T00:30:22.084115: step 6171, loss 0.0174098, acc 0.984375\n",
      "2017-01-11T00:30:24.150188: step 6172, loss 0.0311716, acc 0.992188\n",
      "2017-01-11T00:30:26.189875: step 6173, loss 0.0509088, acc 0.984375\n",
      "2017-01-11T00:30:28.362821: step 6174, loss 0.0264731, acc 0.984375\n",
      "2017-01-11T00:30:30.544510: step 6175, loss 0.0730331, acc 0.976562\n",
      "2017-01-11T00:30:32.622275: step 6176, loss 0.0118117, acc 0.992188\n",
      "2017-01-11T00:30:34.733818: step 6177, loss 0.0272574, acc 0.992188\n",
      "2017-01-11T00:30:36.806161: step 6178, loss 0.0228153, acc 0.992188\n",
      "2017-01-11T00:30:38.905155: step 6179, loss 0.052631, acc 0.984375\n",
      "2017-01-11T00:30:41.041647: step 6180, loss 0.00430508, acc 1\n",
      "2017-01-11T00:30:43.146945: step 6181, loss 0.0831864, acc 0.976562\n",
      "2017-01-11T00:30:46.309258: step 6182, loss 0.0503196, acc 0.976562\n",
      "2017-01-11T00:30:48.723299: step 6183, loss 0.104431, acc 0.984375\n",
      "2017-01-11T00:30:50.907148: step 6184, loss 0.027391, acc 0.984375\n",
      "2017-01-11T00:30:53.109452: step 6185, loss 0.0223426, acc 0.984375\n",
      "2017-01-11T00:30:55.159985: step 6186, loss 0.047102, acc 0.992188\n",
      "2017-01-11T00:30:57.198079: step 6187, loss 0.00167224, acc 1\n",
      "2017-01-11T00:30:59.243851: step 6188, loss 0.0210927, acc 0.992188\n",
      "2017-01-11T00:31:01.290574: step 6189, loss 0.0811475, acc 0.976562\n",
      "2017-01-11T00:31:03.338423: step 6190, loss 0.00194844, acc 1\n",
      "2017-01-11T00:31:05.415599: step 6191, loss 0.0618553, acc 0.984375\n",
      "2017-01-11T00:31:07.424265: step 6192, loss 0.0580417, acc 0.984375\n",
      "2017-01-11T00:31:09.519959: step 6193, loss 0.0528178, acc 0.992188\n",
      "2017-01-11T00:31:11.519329: step 6194, loss 0.025798, acc 0.992188\n",
      "2017-01-11T00:31:13.553062: step 6195, loss 0.00183186, acc 1\n",
      "2017-01-11T00:31:15.574405: step 6196, loss 0.00597517, acc 1\n",
      "2017-01-11T00:31:17.609290: step 6197, loss 0.0296861, acc 0.984375\n",
      "2017-01-11T00:31:19.703441: step 6198, loss 0.065248, acc 0.976562\n",
      "2017-01-11T00:31:22.067000: step 6199, loss 0.0542158, acc 0.984375\n",
      "2017-01-11T00:31:24.093299: step 6200, loss 0.0364658, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:31:47.246564: step 6200, loss 0.0706939, acc 0.98096\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6200\n",
      "\n",
      "2017-01-11T00:31:51.853231: step 6201, loss 0.0104906, acc 0.992188\n",
      "2017-01-11T00:31:54.263060: step 6202, loss 0.0324142, acc 0.992188\n",
      "2017-01-11T00:31:56.375577: step 6203, loss 0.025051, acc 0.992188\n",
      "2017-01-11T00:31:58.398794: step 6204, loss 0.057337, acc 0.976562\n",
      "2017-01-11T00:32:00.450485: step 6205, loss 0.0480867, acc 0.984375\n",
      "2017-01-11T00:32:02.464343: step 6206, loss 0.0151901, acc 0.992188\n",
      "2017-01-11T00:32:04.564088: step 6207, loss 0.065432, acc 0.976562\n",
      "2017-01-11T00:32:06.560957: step 6208, loss 0.0416052, acc 0.984375\n",
      "2017-01-11T00:32:08.604812: step 6209, loss 0.0358257, acc 0.992188\n",
      "2017-01-11T00:32:10.707025: step 6210, loss 0.0258193, acc 0.992188\n",
      "2017-01-11T00:32:12.726951: step 6211, loss 0.139477, acc 0.976562\n",
      "2017-01-11T00:32:14.775759: step 6212, loss 0.0598988, acc 0.976562\n",
      "2017-01-11T00:32:16.847387: step 6213, loss 0.0419962, acc 0.984375\n",
      "2017-01-11T00:32:18.885262: step 6214, loss 0.0142885, acc 1\n",
      "2017-01-11T00:32:20.991547: step 6215, loss 0.0322324, acc 0.992188\n",
      "2017-01-11T00:32:23.086700: step 6216, loss 0.0566303, acc 0.984375\n",
      "2017-01-11T00:32:25.125981: step 6217, loss 0.0561897, acc 0.984375\n",
      "2017-01-11T00:32:27.504434: step 6218, loss 0.0358598, acc 0.992188\n",
      "2017-01-11T00:32:29.553239: step 6219, loss 0.0115049, acc 1\n",
      "2017-01-11T00:32:31.746669: step 6220, loss 0.0454543, acc 0.984375\n",
      "2017-01-11T00:32:33.777659: step 6221, loss 0.0327831, acc 0.992188\n",
      "2017-01-11T00:32:35.859733: step 6222, loss 0.00891683, acc 1\n",
      "2017-01-11T00:32:37.906687: step 6223, loss 0.0574262, acc 0.984375\n",
      "2017-01-11T00:32:39.964004: step 6224, loss 0.0491752, acc 0.976562\n",
      "2017-01-11T00:32:42.052361: step 6225, loss 0.114916, acc 0.960938\n",
      "2017-01-11T00:32:44.107041: step 6226, loss 0.035234, acc 0.984375\n",
      "2017-01-11T00:32:46.115843: step 6227, loss 0.0548379, acc 0.984375\n",
      "2017-01-11T00:32:48.183721: step 6228, loss 0.0585501, acc 0.984375\n",
      "2017-01-11T00:32:50.248690: step 6229, loss 0.0102213, acc 1\n",
      "2017-01-11T00:32:52.541411: step 6230, loss 0.055207, acc 0.984375\n",
      "2017-01-11T00:32:54.572343: step 6231, loss 0.0281013, acc 0.992188\n",
      "2017-01-11T00:32:56.601853: step 6232, loss 0.049444, acc 0.992188\n",
      "2017-01-11T00:32:58.974249: step 6233, loss 0.071952, acc 0.976562\n",
      "2017-01-11T00:33:01.002523: step 6234, loss 0.0276762, acc 0.992188\n",
      "2017-01-11T00:33:03.055511: step 6235, loss 0.0751139, acc 0.976562\n",
      "2017-01-11T00:33:05.178534: step 6236, loss 5.4577e-05, acc 1\n",
      "2017-01-11T00:33:07.509594: step 6237, loss 0.0322053, acc 0.984375\n",
      "2017-01-11T00:33:10.505742: step 6238, loss 0.00614031, acc 1\n",
      "2017-01-11T00:33:12.528158: step 6239, loss 0.0367647, acc 0.984375\n",
      "2017-01-11T00:33:14.597252: step 6240, loss 0.000609072, acc 1\n",
      "2017-01-11T00:33:16.614970: step 6241, loss 0.0483824, acc 0.984375\n",
      "2017-01-11T00:33:18.652462: step 6242, loss 0.157261, acc 0.992188\n",
      "2017-01-11T00:33:20.722431: step 6243, loss 0.0326341, acc 0.992188\n",
      "2017-01-11T00:33:22.759671: step 6244, loss 0.0225057, acc 0.992188\n",
      "2017-01-11T00:33:24.779849: step 6245, loss 0.0274365, acc 0.984375\n",
      "2017-01-11T00:33:26.802369: step 6246, loss 0.00968712, acc 0.992188\n",
      "2017-01-11T00:33:28.796209: step 6247, loss 0.0449685, acc 0.992188\n",
      "2017-01-11T00:33:31.177419: step 6248, loss 0.00759195, acc 1\n",
      "2017-01-11T00:33:33.208697: step 6249, loss 0.0124213, acc 1\n",
      "2017-01-11T00:33:35.262523: step 6250, loss 0.0643575, acc 0.984375\n",
      "2017-01-11T00:33:37.291579: step 6251, loss 0.0990507, acc 0.976562\n",
      "2017-01-11T00:33:39.307930: step 6252, loss 0.0207647, acc 0.984375\n",
      "2017-01-11T00:33:41.362873: step 6253, loss 0.0177713, acc 1\n",
      "2017-01-11T00:33:43.408284: step 6254, loss 0.00202701, acc 1\n",
      "2017-01-11T00:33:45.432641: step 6255, loss 0.049677, acc 0.984375\n",
      "2017-01-11T00:33:47.455429: step 6256, loss 0.0225779, acc 0.992188\n",
      "2017-01-11T00:33:49.521451: step 6257, loss 0.0283588, acc 0.992188\n",
      "2017-01-11T00:33:51.586045: step 6258, loss 0.0216888, acc 1\n",
      "2017-01-11T00:33:53.599097: step 6259, loss 0.0359481, acc 0.984375\n",
      "2017-01-11T00:33:55.655223: step 6260, loss 0.0354653, acc 0.992188\n",
      "2017-01-11T00:33:57.883084: step 6261, loss 0.00975011, acc 0.992188\n",
      "2017-01-11T00:33:59.909729: step 6262, loss 0.0389942, acc 0.992188\n",
      "2017-01-11T00:34:02.293274: step 6263, loss 0.00693819, acc 1\n",
      "2017-01-11T00:34:04.340494: step 6264, loss 0.056584, acc 0.992188\n",
      "2017-01-11T00:34:06.421697: step 6265, loss 0.0357003, acc 0.992188\n",
      "2017-01-11T00:34:08.493291: step 6266, loss 0.01016, acc 1\n",
      "2017-01-11T00:34:10.670526: step 6267, loss 0.0673857, acc 0.976562\n",
      "2017-01-11T00:34:12.704647: step 6268, loss 0.0192909, acc 1\n",
      "2017-01-11T00:34:15.797276: step 6269, loss 0.0611613, acc 0.984375\n",
      "2017-01-11T00:34:17.921378: step 6270, loss 0.0890481, acc 0.976562\n",
      "2017-01-11T00:34:19.977664: step 6271, loss 0.0453093, acc 0.992188\n",
      "2017-01-11T00:34:22.040936: step 6272, loss 0.0271324, acc 1\n",
      "2017-01-11T00:34:24.039483: step 6273, loss 0.0323917, acc 0.992188\n",
      "2017-01-11T00:34:26.080600: step 6274, loss 0.0929665, acc 0.953125\n",
      "2017-01-11T00:34:28.113748: step 6275, loss 0.0421687, acc 0.984375\n",
      "2017-01-11T00:34:30.243401: step 6276, loss 0.0226512, acc 1\n",
      "2017-01-11T00:34:32.224588: step 6277, loss 0.0316136, acc 0.984375\n",
      "2017-01-11T00:34:34.647689: step 6278, loss 0.0103536, acc 1\n",
      "2017-01-11T00:34:36.676060: step 6279, loss 0.0335991, acc 0.992188\n",
      "2017-01-11T00:34:38.703229: step 6280, loss 0.021043, acc 0.992188\n",
      "2017-01-11T00:34:40.747533: step 6281, loss 0.108735, acc 0.992188\n",
      "2017-01-11T00:34:42.801463: step 6282, loss 0.0534686, acc 0.992188\n",
      "2017-01-11T00:34:44.851003: step 6283, loss 0.0179248, acc 1\n",
      "2017-01-11T00:34:46.889503: step 6284, loss 0.0277725, acc 0.992188\n",
      "2017-01-11T00:34:49.195064: step 6285, loss 0.0401431, acc 0.976562\n",
      "2017-01-11T00:34:51.271839: step 6286, loss 0.0214975, acc 0.992188\n",
      "2017-01-11T00:34:53.310410: step 6287, loss 0.0980413, acc 0.976562\n",
      "2017-01-11T00:34:55.321173: step 6288, loss 0.00712305, acc 1\n",
      "2017-01-11T00:34:57.354206: step 6289, loss 0.0139848, acc 0.992188\n",
      "2017-01-11T00:34:59.399081: step 6290, loss 0.0289052, acc 0.992188\n",
      "2017-01-11T00:35:01.431910: step 6291, loss 0.0649651, acc 0.976562\n",
      "2017-01-11T00:35:03.547381: step 6292, loss 0.0308785, acc 0.984375\n",
      "2017-01-11T00:35:05.973660: step 6293, loss 0.0178869, acc 0.992188\n",
      "2017-01-11T00:35:08.004618: step 6294, loss 0.0794418, acc 0.976562\n",
      "2017-01-11T00:35:10.064522: step 6295, loss 0.040827, acc 0.992188\n",
      "2017-01-11T00:35:12.201432: step 6296, loss 0.021642, acc 0.992188\n",
      "2017-01-11T00:35:14.231255: step 6297, loss 0.0151756, acc 0.992188\n",
      "2017-01-11T00:35:16.263912: step 6298, loss 0.049897, acc 0.984375\n",
      "2017-01-11T00:35:18.306972: step 6299, loss 0.0281908, acc 0.992188\n",
      "2017-01-11T00:35:20.377742: step 6300, loss 0.00701089, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:35:43.943871: step 6300, loss 0.0685342, acc 0.98132\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6300\n",
      "\n",
      "2017-01-11T00:35:49.476164: step 6301, loss 0.0455257, acc 0.984375\n",
      "2017-01-11T00:35:51.594745: step 6302, loss 0.0479575, acc 0.984375\n",
      "2017-01-11T00:35:53.653215: step 6303, loss 0.0245374, acc 1\n",
      "2017-01-11T00:35:55.663950: step 6304, loss 0.0282417, acc 0.992188\n",
      "2017-01-11T00:35:57.743173: step 6305, loss 0.0329148, acc 0.992188\n",
      "2017-01-11T00:35:59.777973: step 6306, loss 0.014328, acc 1\n",
      "2017-01-11T00:36:01.875675: step 6307, loss 0.0182813, acc 0.992188\n",
      "2017-01-11T00:36:03.932583: step 6308, loss 0.0214423, acc 0.992188\n",
      "2017-01-11T00:36:06.027028: step 6309, loss 0.0326338, acc 0.992188\n",
      "2017-01-11T00:36:08.114391: step 6310, loss 0.103468, acc 0.976562\n",
      "2017-01-11T00:36:10.517975: step 6311, loss 0.0169002, acc 0.992188\n",
      "2017-01-11T00:36:12.685520: step 6312, loss 0.0632698, acc 0.976562\n",
      "2017-01-11T00:36:14.791118: step 6313, loss 0.0359657, acc 0.984375\n",
      "2017-01-11T00:36:16.919209: step 6314, loss 0.0309821, acc 0.992188\n",
      "2017-01-11T00:36:19.051263: step 6315, loss 0.0666813, acc 0.984375\n",
      "2017-01-11T00:36:21.164261: step 6316, loss 0.0326357, acc 0.992188\n",
      "2017-01-11T00:36:23.302613: step 6317, loss 0.0292339, acc 0.992188\n",
      "2017-01-11T00:36:25.457341: step 6318, loss 0.000109725, acc 1\n",
      "2017-01-11T00:36:27.605624: step 6319, loss 0.0575319, acc 0.976562\n",
      "2017-01-11T00:36:29.702367: step 6320, loss 0.0453562, acc 0.984375\n",
      "2017-01-11T00:36:31.953539: step 6321, loss 0.0844981, acc 0.984375\n",
      "2017-01-11T00:36:34.083838: step 6322, loss 0.00707965, acc 1\n",
      "2017-01-11T00:36:36.256088: step 6323, loss 0.0309068, acc 0.992188\n",
      "2017-01-11T00:36:38.350217: step 6324, loss 0.0121213, acc 0.992188\n",
      "2017-01-11T00:36:40.446000: step 6325, loss 0.000990853, acc 1\n",
      "2017-01-11T00:36:42.897799: step 6326, loss 0.00370779, acc 1\n",
      "2017-01-11T00:36:45.030835: step 6327, loss 0.021345, acc 1\n",
      "2017-01-11T00:36:47.136286: step 6328, loss 0.0283998, acc 0.992188\n",
      "2017-01-11T00:36:49.275983: step 6329, loss 0.0341628, acc 0.992188\n",
      "2017-01-11T00:36:51.450720: step 6330, loss 0.0208909, acc 1\n",
      "2017-01-11T00:36:53.586666: step 6331, loss 0.0386889, acc 0.992188\n",
      "2017-01-11T00:36:55.705167: step 6332, loss 0.0252907, acc 0.992188\n",
      "2017-01-11T00:36:57.773979: step 6333, loss 0.040116, acc 0.984375\n",
      "2017-01-11T00:36:59.947637: step 6334, loss 0.0847125, acc 0.976562\n",
      "2017-01-11T00:37:02.065095: step 6335, loss 0.0463134, acc 0.984375\n",
      "2017-01-11T00:37:04.230396: step 6336, loss 0.0843668, acc 0.976562\n",
      "2017-01-11T00:37:06.319969: step 6337, loss 0.0310252, acc 1\n",
      "2017-01-11T00:37:08.474410: step 6338, loss 0.0303949, acc 0.992188\n",
      "2017-01-11T00:37:10.600242: step 6339, loss 0.000969669, acc 1\n",
      "2017-01-11T00:37:12.864091: step 6340, loss 0.0754529, acc 0.976562\n",
      "2017-01-11T00:37:15.342536: step 6341, loss 0.0406162, acc 0.984375\n",
      "2017-01-11T00:37:17.451043: step 6342, loss 0.0431306, acc 0.984375\n",
      "2017-01-11T00:37:19.630433: step 6343, loss 0.0626811, acc 0.984375\n",
      "2017-01-11T00:37:21.774631: step 6344, loss 0.0644274, acc 0.96875\n",
      "2017-01-11T00:37:23.937521: step 6345, loss 0.0221044, acc 0.992188\n",
      "2017-01-11T00:37:26.074892: step 6346, loss 0.00158988, acc 1\n",
      "2017-01-11T00:37:28.175872: step 6347, loss 0.0482066, acc 0.976562\n",
      "2017-01-11T00:37:30.419714: step 6348, loss 0.0433647, acc 0.984375\n",
      "2017-01-11T00:37:32.766738: step 6349, loss 0.0416561, acc 0.984375\n",
      "2017-01-11T00:37:35.033726: step 6350, loss 0.0393596, acc 0.984375\n",
      "2017-01-11T00:37:37.129465: step 6351, loss 0.0190128, acc 0.992188\n",
      "2017-01-11T00:37:39.490175: step 6352, loss 0.00524058, acc 1\n",
      "2017-01-11T00:37:41.594337: step 6353, loss 0.113287, acc 0.96875\n",
      "2017-01-11T00:37:43.765709: step 6354, loss 0.00366799, acc 1\n",
      "2017-01-11T00:37:46.140665: step 6355, loss 0.0327268, acc 0.992188\n",
      "2017-01-11T00:37:48.302072: step 6356, loss 0.0485688, acc 0.976562\n",
      "2017-01-11T00:37:50.340696: step 6357, loss 0.0139065, acc 1\n",
      "2017-01-11T00:37:52.435912: step 6358, loss 0.0235798, acc 0.992188\n",
      "2017-01-11T00:37:54.488044: step 6359, loss 0.0412514, acc 0.992188\n",
      "2017-01-11T00:37:56.543894: step 6360, loss 0.0252816, acc 0.992188\n",
      "2017-01-11T00:37:58.830364: step 6361, loss 0.0483114, acc 0.984375\n",
      "2017-01-11T00:38:00.858041: step 6362, loss 0.056001, acc 0.984375\n",
      "2017-01-11T00:38:02.900382: step 6363, loss 0.00142737, acc 1\n",
      "2017-01-11T00:38:04.965087: step 6364, loss 0.0235978, acc 0.992188\n",
      "2017-01-11T00:38:07.007308: step 6365, loss 0.0246085, acc 0.992188\n",
      "2017-01-11T00:38:09.061283: step 6366, loss 0.0195755, acc 0.992188\n",
      "2017-01-11T00:38:11.097711: step 6367, loss 0.223835, acc 0.984375\n",
      "2017-01-11T00:38:13.470945: step 6368, loss 0.0293894, acc 0.992188\n",
      "2017-01-11T00:38:16.400348: step 6369, loss 0.0413718, acc 0.984375\n",
      "2017-01-11T00:38:18.758749: step 6370, loss 0.000656253, acc 1\n",
      "2017-01-11T00:38:20.805454: step 6371, loss 0.0221993, acc 0.992188\n",
      "2017-01-11T00:38:22.865370: step 6372, loss 0.0858082, acc 0.976562\n",
      "2017-01-11T00:38:24.911391: step 6373, loss 0.0579797, acc 0.984375\n",
      "2017-01-11T00:38:26.953893: step 6374, loss 0.0275393, acc 0.984375\n",
      "2017-01-11T00:38:29.005942: step 6375, loss 0.0314538, acc 0.992188\n",
      "2017-01-11T00:38:31.132942: step 6376, loss 0.105635, acc 0.96875\n",
      "2017-01-11T00:38:33.182535: step 6377, loss 0.0445135, acc 0.992188\n",
      "2017-01-11T00:38:35.255228: step 6378, loss 0.0559731, acc 0.984375\n",
      "2017-01-11T00:38:37.255854: step 6379, loss 0.0375499, acc 0.984375\n",
      "2017-01-11T00:38:39.307838: step 6380, loss 0.0483098, acc 0.976562\n",
      "2017-01-11T00:38:41.361919: step 6381, loss 0.00856342, acc 1\n",
      "2017-01-11T00:38:43.386575: step 6382, loss 0.0252337, acc 0.984375\n",
      "2017-01-11T00:38:45.412067: step 6383, loss 0.0369672, acc 0.984375\n",
      "2017-01-11T00:38:47.406344: step 6384, loss 0.0155253, acc 1\n",
      "2017-01-11T00:38:49.582545: step 6385, loss 0.0739348, acc 0.984375\n",
      "2017-01-11T00:38:51.822783: step 6386, loss 0.0240816, acc 0.984375\n",
      "2017-01-11T00:38:53.817848: step 6387, loss 0.0384439, acc 0.992188\n",
      "2017-01-11T00:38:55.848552: step 6388, loss 0.0189994, acc 0.992188\n",
      "2017-01-11T00:38:57.890762: step 6389, loss 0.00152227, acc 1\n",
      "2017-01-11T00:38:59.943939: step 6390, loss 0.027894, acc 0.992188\n",
      "2017-01-11T00:39:02.000234: step 6391, loss 0.0598235, acc 0.992188\n",
      "2017-01-11T00:39:04.249479: step 6392, loss 0.12526, acc 0.976562\n",
      "2017-01-11T00:39:06.352880: step 6393, loss 0.0568926, acc 0.96875\n",
      "2017-01-11T00:39:08.390482: step 6394, loss 0.00968668, acc 0.992188\n",
      "2017-01-11T00:39:10.404279: step 6395, loss 0.0419815, acc 0.984375\n",
      "2017-01-11T00:39:12.405233: step 6396, loss 0.00985221, acc 1\n",
      "2017-01-11T00:39:14.561338: step 6397, loss 0.051002, acc 0.984375\n",
      "2017-01-11T00:39:16.657509: step 6398, loss 0.0275968, acc 0.984375\n",
      "2017-01-11T00:39:18.899531: step 6399, loss 0.00894189, acc 1\n",
      "2017-01-11T00:39:22.000465: step 6400, loss 0.0401746, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:39:54.764458: step 6400, loss 0.0693253, acc 0.98104\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6400\n",
      "\n",
      "2017-01-11T00:39:59.447016: step 6401, loss 0.0279082, acc 0.992188\n",
      "2017-01-11T00:40:01.501602: step 6402, loss 0.13108, acc 0.96875\n",
      "2017-01-11T00:40:03.562804: step 6403, loss 0.0201605, acc 0.992188\n",
      "2017-01-11T00:40:05.652712: step 6404, loss 0.0331387, acc 0.992188\n",
      "2017-01-11T00:40:07.710103: step 6405, loss 0.152934, acc 0.976562\n",
      "2017-01-11T00:40:09.735691: step 6406, loss 0.00620403, acc 0.992188\n",
      "2017-01-11T00:40:11.823428: step 6407, loss 0.00314884, acc 1\n",
      "2017-01-11T00:40:13.991350: step 6408, loss 0.00329754, acc 1\n",
      "2017-01-11T00:40:16.161018: step 6409, loss 0.0670106, acc 0.992188\n",
      "2017-01-11T00:40:18.458730: step 6410, loss 0.0547906, acc 0.992188\n",
      "2017-01-11T00:40:21.134859: step 6411, loss 0.0867974, acc 0.96875\n",
      "2017-01-11T00:40:24.251248: step 6412, loss 0.0253577, acc 0.992188\n",
      "2017-01-11T00:40:26.492958: step 6413, loss 0.0631735, acc 0.976562\n",
      "2017-01-11T00:40:28.861150: step 6414, loss 0.000992318, acc 1\n",
      "2017-01-11T00:40:31.223200: step 6415, loss 0.0199774, acc 0.992188\n",
      "2017-01-11T00:40:33.262190: step 6416, loss 0.013433, acc 1\n",
      "2017-01-11T00:40:35.343111: step 6417, loss 0.0436647, acc 0.984375\n",
      "2017-01-11T00:40:37.380357: step 6418, loss 0.0485171, acc 0.992188\n",
      "2017-01-11T00:40:39.430741: step 6419, loss 0.00545046, acc 1\n",
      "2017-01-11T00:40:41.488145: step 6420, loss 0.00221657, acc 1\n",
      "2017-01-11T00:40:43.532673: step 6421, loss 0.0290648, acc 0.992188\n",
      "2017-01-11T00:40:46.673963: step 6422, loss 0.0436022, acc 0.984375\n",
      "2017-01-11T00:40:48.736637: step 6423, loss 0.0377114, acc 0.984375\n",
      "2017-01-11T00:40:50.812637: step 6424, loss 0.0222333, acc 1\n",
      "2017-01-11T00:40:52.897342: step 6425, loss 0.0118083, acc 1\n",
      "2017-01-11T00:40:54.909225: step 6426, loss 0.0321389, acc 0.984375\n",
      "2017-01-11T00:40:56.972907: step 6427, loss 0.0197862, acc 0.992188\n",
      "2017-01-11T00:40:58.995402: step 6428, loss 0.0364693, acc 0.992188\n",
      "2017-01-11T00:41:01.116454: step 6429, loss 0.0886235, acc 0.976562\n",
      "2017-01-11T00:41:03.402430: step 6430, loss 0.0254919, acc 0.992188\n",
      "2017-01-11T00:41:05.468435: step 6431, loss 0.00202548, acc 1\n",
      "2017-01-11T00:41:07.502115: step 6432, loss 0.0114309, acc 1\n",
      "2017-01-11T00:41:09.545198: step 6433, loss 0.00508674, acc 1\n",
      "2017-01-11T00:41:11.548804: step 6434, loss 0.00466175, acc 1\n",
      "2017-01-11T00:41:13.700916: step 6435, loss 0.140136, acc 0.976562\n",
      "2017-01-11T00:41:15.735009: step 6436, loss 0.0431078, acc 0.984375\n",
      "2017-01-11T00:41:17.723364: step 6437, loss 0.0855192, acc 0.976562\n",
      "2017-01-11T00:41:19.796541: step 6438, loss 0.0433627, acc 0.984375\n",
      "2017-01-11T00:41:21.886033: step 6439, loss 0.0164012, acc 0.992188\n",
      "2017-01-11T00:41:23.929511: step 6440, loss 0.0287178, acc 0.992188\n",
      "2017-01-11T00:41:26.004311: step 6441, loss 0.0307247, acc 0.984375\n",
      "2017-01-11T00:41:28.048997: step 6442, loss 0.0263732, acc 1\n",
      "2017-01-11T00:41:30.060812: step 6443, loss 0.00428344, acc 1\n",
      "2017-01-11T00:41:32.112066: step 6444, loss 0.0785943, acc 0.96875\n",
      "2017-01-11T00:41:34.519937: step 6445, loss 0.0186183, acc 0.992188\n",
      "2017-01-11T00:41:36.534127: step 6446, loss 0.0179133, acc 0.992188\n",
      "2017-01-11T00:41:38.544432: step 6447, loss 0.0616254, acc 0.976562\n",
      "2017-01-11T00:41:40.600630: step 6448, loss 0.0172018, acc 0.984375\n",
      "2017-01-11T00:41:42.638658: step 6449, loss 0.0158768, acc 0.992188\n",
      "2017-01-11T00:41:44.683817: step 6450, loss 0.0253097, acc 0.992188\n",
      "2017-01-11T00:41:46.764937: step 6451, loss 0.0334251, acc 0.976562\n",
      "2017-01-11T00:41:48.788958: step 6452, loss 0.018025, acc 0.992188\n",
      "2017-01-11T00:41:50.818498: step 6453, loss 0.0629332, acc 0.96875\n",
      "2017-01-11T00:41:52.872872: step 6454, loss 0.222531, acc 0.9375\n",
      "2017-01-11T00:41:54.948231: step 6455, loss 0.0109242, acc 0.992188\n",
      "2017-01-11T00:41:56.972170: step 6456, loss 0.0259471, acc 0.992188\n",
      "2017-01-11T00:41:59.007176: step 6457, loss 0.0181244, acc 0.992188\n",
      "2017-01-11T00:42:01.065141: step 6458, loss 0.0138705, acc 1\n",
      "2017-01-11T00:42:03.126128: step 6459, loss 0.0232843, acc 0.992188\n",
      "2017-01-11T00:42:05.367472: step 6460, loss 0.0716047, acc 0.984375\n",
      "2017-01-11T00:42:07.539971: step 6461, loss 0.0553159, acc 0.976562\n",
      "2017-01-11T00:42:09.555204: step 6462, loss 0.0289194, acc 0.992188\n",
      "2017-01-11T00:42:11.616118: step 6463, loss 0.0209533, acc 0.992188\n",
      "2017-01-11T00:42:13.699873: step 6464, loss 0.0596095, acc 0.976562\n",
      "2017-01-11T00:42:15.804193: step 6465, loss 0.0144748, acc 0.992188\n",
      "2017-01-11T00:42:17.889920: step 6466, loss 0.0328862, acc 0.992188\n",
      "2017-01-11T00:42:19.989809: step 6467, loss 0.0400543, acc 0.984375\n",
      "2017-01-11T00:42:22.117004: step 6468, loss 0.0263183, acc 0.992188\n",
      "2017-01-11T00:42:24.172971: step 6469, loss 0.0982492, acc 0.96875\n",
      "2017-01-11T00:42:26.259038: step 6470, loss 0.0135308, acc 0.992188\n",
      "2017-01-11T00:42:28.280843: step 6471, loss 0.0762363, acc 0.976562\n",
      "2017-01-11T00:42:30.394629: step 6472, loss 0.0380225, acc 0.984375\n",
      "2017-01-11T00:42:32.374563: step 6473, loss 0.000455929, acc 1\n",
      "2017-01-11T00:42:34.415640: step 6474, loss 0.0496469, acc 0.984375\n",
      "2017-01-11T00:42:36.484340: step 6475, loss 0.0200544, acc 0.992188\n",
      "2017-01-11T00:42:38.825828: step 6476, loss 0.0484156, acc 0.984375\n",
      "2017-01-11T00:42:40.814116: step 6477, loss 0.0753232, acc 0.976562\n",
      "2017-01-11T00:42:42.833404: step 6478, loss 0.00358781, acc 1\n",
      "2017-01-11T00:42:44.854504: step 6479, loss 0.0484664, acc 0.992188\n",
      "2017-01-11T00:42:46.890974: step 6480, loss 0.0312682, acc 0.992188\n",
      "2017-01-11T00:42:48.917504: step 6481, loss 0.0435468, acc 0.984375\n",
      "2017-01-11T00:42:50.999410: step 6482, loss 0.0739831, acc 0.992188\n",
      "2017-01-11T00:42:53.105786: step 6483, loss 0.0231735, acc 0.992188\n",
      "2017-01-11T00:42:55.100910: step 6484, loss 0.0150895, acc 1\n",
      "2017-01-11T00:42:57.118493: step 6485, loss 0.00458644, acc 1\n",
      "2017-01-11T00:42:59.131803: step 6486, loss 0.152435, acc 0.992188\n",
      "2017-01-11T00:43:01.184002: step 6487, loss 0.0107234, acc 1\n",
      "2017-01-11T00:43:03.270042: step 6488, loss 0.0371374, acc 0.984375\n",
      "2017-01-11T00:43:05.545971: step 6489, loss 0.0459668, acc 0.992188\n",
      "2017-01-11T00:43:07.610728: step 6490, loss 0.0652485, acc 0.976562\n",
      "2017-01-11T00:43:09.961508: step 6491, loss 0.0470192, acc 0.984375\n",
      "2017-01-11T00:43:11.972907: step 6492, loss 0.0210617, acc 1\n",
      "2017-01-11T00:43:13.982102: step 6493, loss 0.043136, acc 0.992188\n",
      "2017-01-11T00:43:16.149155: step 6494, loss 0.0296449, acc 0.992188\n",
      "2017-01-11T00:43:18.219261: step 6495, loss 0.0713968, acc 0.984375\n",
      "2017-01-11T00:43:21.090682: step 6496, loss 0.00311942, acc 1\n",
      "2017-01-11T00:43:23.336413: step 6497, loss 0.0312008, acc 0.992188\n",
      "2017-01-11T00:43:25.368534: step 6498, loss 0.0467443, acc 0.984375\n",
      "2017-01-11T00:43:27.406397: step 6499, loss 0.0257418, acc 0.992188\n",
      "2017-01-11T00:43:29.431963: step 6500, loss 0.0389056, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:43:58.149238: step 6500, loss 0.0686848, acc 0.98168\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6500\n",
      "\n",
      "2017-01-11T00:44:02.599612: step 6501, loss 0.0156308, acc 0.992188\n",
      "2017-01-11T00:44:04.712427: step 6502, loss 0.0269447, acc 0.992188\n",
      "2017-01-11T00:44:06.772257: step 6503, loss 0.0197404, acc 0.992188\n",
      "2017-01-11T00:44:08.889072: step 6504, loss 0.00789433, acc 0.992188\n",
      "2017-01-11T00:44:11.088230: step 6505, loss 0.0397034, acc 0.984375\n",
      "2017-01-11T00:44:13.283668: step 6506, loss 0.00123056, acc 1\n",
      "2017-01-11T00:44:15.603565: step 6507, loss 0.00312252, acc 1\n",
      "2017-01-11T00:44:17.686198: step 6508, loss 0.0205244, acc 0.992188\n",
      "2017-01-11T00:44:19.775997: step 6509, loss 0.025566, acc 0.992188\n",
      "2017-01-11T00:44:21.898194: step 6510, loss 0.0027956, acc 1\n",
      "2017-01-11T00:44:23.910027: step 6511, loss 0.043036, acc 0.984375\n",
      "2017-01-11T00:44:26.723485: step 6512, loss 0.0246162, acc 0.992188\n",
      "2017-01-11T00:44:28.989598: step 6513, loss 0.00157913, acc 1\n",
      "2017-01-11T00:44:31.150086: step 6514, loss 0.00703919, acc 1\n",
      "2017-01-11T00:44:33.186437: step 6515, loss 0.0732857, acc 0.976562\n",
      "2017-01-11T00:44:35.249878: step 6516, loss 0.0289262, acc 0.992188\n",
      "2017-01-11T00:44:37.303776: step 6517, loss 0.0255552, acc 1\n",
      "2017-01-11T00:44:39.320952: step 6518, loss 0.0246803, acc 0.992188\n",
      "2017-01-11T00:44:41.368958: step 6519, loss 0.0204173, acc 0.992188\n",
      "2017-01-11T00:44:43.422207: step 6520, loss 0.0553187, acc 0.984375\n",
      "2017-01-11T00:44:45.455783: step 6521, loss 0.0445967, acc 0.984375\n",
      "2017-01-11T00:44:47.852603: step 6522, loss 0.0415385, acc 0.984375\n",
      "2017-01-11T00:44:49.859225: step 6523, loss 0.0572405, acc 0.984375\n",
      "2017-01-11T00:44:51.852838: step 6524, loss 0.019175, acc 0.992188\n",
      "2017-01-11T00:44:53.906717: step 6525, loss 0.0422639, acc 0.984375\n",
      "2017-01-11T00:44:55.949892: step 6526, loss 0.0397724, acc 0.984375\n",
      "2017-01-11T00:44:57.996349: step 6527, loss 0.00608236, acc 1\n",
      "2017-01-11T00:44:59.998384: step 6528, loss 0.0284675, acc 1\n",
      "2017-01-11T00:45:02.041707: step 6529, loss 0.215216, acc 0.984375\n",
      "2017-01-11T00:45:04.072076: step 6530, loss 0.0313427, acc 0.984375\n",
      "2017-01-11T00:45:06.162164: step 6531, loss 0.0178746, acc 0.992188\n",
      "2017-01-11T00:45:08.180682: step 6532, loss 0.0168295, acc 0.992188\n",
      "2017-01-11T00:45:10.213411: step 6533, loss 0.0637067, acc 0.96875\n",
      "2017-01-11T00:45:12.259395: step 6534, loss 0.0157346, acc 0.992188\n",
      "2017-01-11T00:45:14.314852: step 6535, loss 0.0751985, acc 0.976562\n",
      "2017-01-11T00:45:16.469954: step 6536, loss 0.0115293, acc 1\n",
      "2017-01-11T00:45:18.820542: step 6537, loss 0.0296541, acc 1\n",
      "2017-01-11T00:45:20.877955: step 6538, loss 0.0317605, acc 0.984375\n",
      "2017-01-11T00:45:22.921491: step 6539, loss 0.0719602, acc 0.976562\n",
      "2017-01-11T00:45:24.971949: step 6540, loss 0.0607411, acc 0.984375\n",
      "2017-01-11T00:45:27.019550: step 6541, loss 0.02775, acc 0.992188\n",
      "2017-01-11T00:45:29.273575: step 6542, loss 0.0030246, acc 1\n",
      "2017-01-11T00:45:31.305753: step 6543, loss 0.0336779, acc 0.984375\n",
      "2017-01-11T00:45:33.375020: step 6544, loss 0.0631787, acc 0.976562\n",
      "2017-01-11T00:45:35.463904: step 6545, loss 0.00910173, acc 1\n",
      "2017-01-11T00:45:37.504405: step 6546, loss 0.0465008, acc 0.984375\n",
      "2017-01-11T00:45:39.545600: step 6547, loss 0.0392973, acc 0.984375\n",
      "2017-01-11T00:45:41.610376: step 6548, loss 0.0138729, acc 0.992188\n",
      "2017-01-11T00:45:43.652964: step 6549, loss 0.0547102, acc 0.984375\n",
      "2017-01-11T00:45:46.580922: step 6550, loss 0.0136589, acc 0.992188\n",
      "2017-01-11T00:45:48.792010: step 6551, loss 0.0202804, acc 0.992188\n",
      "2017-01-11T00:45:51.152669: step 6552, loss 0.00809684, acc 0.992188\n",
      "2017-01-11T00:45:53.179870: step 6553, loss 0.0390632, acc 0.992188\n",
      "2017-01-11T00:45:55.224234: step 6554, loss 0.111709, acc 0.984375\n",
      "2017-01-11T00:45:57.234329: step 6555, loss 0.0254151, acc 0.992188\n",
      "2017-01-11T00:45:59.272905: step 6556, loss 0.0660372, acc 0.984375\n",
      "2017-01-11T00:46:01.312080: step 6557, loss 0.017071, acc 1\n",
      "2017-01-11T00:46:03.359473: step 6558, loss 0.0886512, acc 0.96875\n",
      "2017-01-11T00:46:05.436973: step 6559, loss 0.0255721, acc 0.992188\n",
      "2017-01-11T00:46:07.476117: step 6560, loss 0.024287, acc 0.992188\n",
      "2017-01-11T00:46:09.523676: step 6561, loss 0.0291561, acc 0.992188\n",
      "2017-01-11T00:46:11.562290: step 6562, loss 0.0781036, acc 0.984375\n",
      "2017-01-11T00:46:13.630716: step 6563, loss 0.00103907, acc 1\n",
      "2017-01-11T00:46:15.677085: step 6564, loss 0.0201571, acc 0.992188\n",
      "2017-01-11T00:46:17.819614: step 6565, loss 0.00786054, acc 1\n",
      "2017-01-11T00:46:19.883700: step 6566, loss 0.0499511, acc 0.984375\n",
      "2017-01-11T00:46:22.149032: step 6567, loss 0.0266397, acc 0.992188\n",
      "2017-01-11T00:46:24.282816: step 6568, loss 0.0058771, acc 1\n",
      "2017-01-11T00:46:26.289832: step 6569, loss 0.0412183, acc 0.984375\n",
      "2017-01-11T00:46:28.382900: step 6570, loss 0.0372754, acc 0.976562\n",
      "2017-01-11T00:46:30.485321: step 6571, loss 0.025042, acc 0.992188\n",
      "2017-01-11T00:46:32.527384: step 6572, loss 0.00815652, acc 1\n",
      "2017-01-11T00:46:34.621145: step 6573, loss 0.0126066, acc 1\n",
      "2017-01-11T00:46:36.651297: step 6574, loss 0.0833102, acc 0.976562\n",
      "2017-01-11T00:46:38.667105: step 6575, loss 0.0206953, acc 0.992188\n",
      "2017-01-11T00:46:40.689447: step 6576, loss 0.0346925, acc 0.984375\n",
      "2017-01-11T00:46:42.765587: step 6577, loss 0.0608116, acc 0.984375\n",
      "2017-01-11T00:46:44.794224: step 6578, loss 0.0237069, acc 0.992188\n",
      "2017-01-11T00:46:46.810509: step 6579, loss 0.0248147, acc 0.992188\n",
      "2017-01-11T00:46:48.853573: step 6580, loss 0.0647616, acc 0.976562\n",
      "2017-01-11T00:46:50.901164: step 6581, loss 0.0261782, acc 0.992188\n",
      "2017-01-11T00:46:52.998000: step 6582, loss 0.00519856, acc 1\n",
      "2017-01-11T00:46:55.379752: step 6583, loss 0.0926207, acc 0.984375\n",
      "2017-01-11T00:46:57.414256: step 6584, loss 0.0460799, acc 0.976562\n",
      "2017-01-11T00:46:59.489002: step 6585, loss 0.0717154, acc 0.976562\n",
      "2017-01-11T00:47:01.544080: step 6586, loss 0.034357, acc 0.984375\n",
      "2017-01-11T00:47:03.573551: step 6587, loss 0.0440646, acc 0.992188\n",
      "2017-01-11T00:47:05.629585: step 6588, loss 0.0223653, acc 1\n",
      "2017-01-11T00:47:07.682253: step 6589, loss 0.0712102, acc 0.984375\n",
      "2017-01-11T00:47:09.732530: step 6590, loss 0.0225051, acc 0.992188\n",
      "2017-01-11T00:47:11.774655: step 6591, loss 0.0947711, acc 0.976562\n",
      "2017-01-11T00:47:13.810849: step 6592, loss 0.0748009, acc 0.976562\n",
      "2017-01-11T00:47:15.871257: step 6593, loss 0.00775455, acc 1\n",
      "2017-01-11T00:47:17.998560: step 6594, loss 0.000552147, acc 1\n",
      "2017-01-11T00:47:20.039035: step 6595, loss 0.0237985, acc 0.992188\n",
      "2017-01-11T00:47:22.112376: step 6596, loss 0.0200967, acc 0.992188\n",
      "2017-01-11T00:47:24.166125: step 6597, loss 0.0523228, acc 0.992188\n",
      "2017-01-11T00:47:26.543420: step 6598, loss 0.0142009, acc 0.992188\n",
      "2017-01-11T00:47:28.559783: step 6599, loss 0.029932, acc 0.992188\n",
      "2017-01-11T00:47:30.592908: step 6600, loss 0.0699981, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:48:05.032730: step 6600, loss 0.0684401, acc 0.9814\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6600\n",
      "\n",
      "2017-01-11T00:48:09.842902: step 6601, loss 0.0326804, acc 0.984375\n",
      "2017-01-11T00:48:11.979459: step 6602, loss 0.0224605, acc 0.992188\n",
      "2017-01-11T00:48:14.034530: step 6603, loss 0.0517454, acc 0.976562\n",
      "2017-01-11T00:48:16.070163: step 6604, loss 0.0812905, acc 0.976562\n",
      "2017-01-11T00:48:18.161422: step 6605, loss 0.00562196, acc 1\n",
      "2017-01-11T00:48:20.254296: step 6606, loss 0.0635368, acc 0.976562\n",
      "2017-01-11T00:48:22.356631: step 6607, loss 0.0346567, acc 0.984375\n",
      "2017-01-11T00:48:24.369848: step 6608, loss 0.0341978, acc 0.984375\n",
      "2017-01-11T00:48:27.263636: step 6609, loss 0.104357, acc 0.96875\n",
      "2017-01-11T00:48:29.514691: step 6610, loss 0.0179948, acc 0.992188\n",
      "2017-01-11T00:48:31.690136: step 6611, loss 0.0169422, acc 0.992188\n",
      "2017-01-11T00:48:33.888740: step 6612, loss 0.0650645, acc 0.976562\n",
      "2017-01-11T00:48:36.292284: step 6613, loss 0.0659428, acc 0.984375\n",
      "2017-01-11T00:48:38.399566: step 6614, loss 0.0182742, acc 1\n",
      "2017-01-11T00:48:40.444526: step 6615, loss 0.00113012, acc 1\n",
      "2017-01-11T00:48:42.500651: step 6616, loss 0.108058, acc 0.96875\n",
      "2017-01-11T00:48:44.559125: step 6617, loss 0.116971, acc 0.976562\n",
      "2017-01-11T00:48:46.606922: step 6618, loss 0.0088108, acc 0.992188\n",
      "2017-01-11T00:48:48.644604: step 6619, loss 0.0345934, acc 0.992188\n",
      "2017-01-11T00:48:50.719171: step 6620, loss 0.033775, acc 0.992188\n",
      "2017-01-11T00:48:52.798398: step 6621, loss 0.0105301, acc 1\n",
      "2017-01-11T00:48:54.803406: step 6622, loss 0.0395268, acc 0.984375\n",
      "2017-01-11T00:48:56.823271: step 6623, loss 0.0290238, acc 0.992188\n",
      "2017-01-11T00:48:58.836478: step 6624, loss 0.0275764, acc 0.992188\n",
      "2017-01-11T00:49:00.867860: step 6625, loss 0.0216324, acc 0.992188\n",
      "2017-01-11T00:49:02.888645: step 6626, loss 0.0436494, acc 0.984375\n",
      "2017-01-11T00:49:05.015844: step 6627, loss 0.0577488, acc 0.984375\n",
      "2017-01-11T00:49:07.376834: step 6628, loss 0.0249889, acc 0.992188\n",
      "2017-01-11T00:49:09.382943: step 6629, loss 0.0224165, acc 1\n",
      "2017-01-11T00:49:11.381899: step 6630, loss 0.0538028, acc 0.984375\n",
      "2017-01-11T00:49:13.437845: step 6631, loss 0.0373743, acc 0.984375\n",
      "2017-01-11T00:49:15.645770: step 6632, loss 0.010469, acc 1\n",
      "2017-01-11T00:49:17.770420: step 6633, loss 0.0489407, acc 0.984375\n",
      "2017-01-11T00:49:19.859254: step 6634, loss 0.0178329, acc 1\n",
      "2017-01-11T00:49:21.935725: step 6635, loss 0.0325013, acc 0.984375\n",
      "2017-01-11T00:49:23.956935: step 6636, loss 0.0537296, acc 0.976562\n",
      "2017-01-11T00:49:26.022766: step 6637, loss 0.0250459, acc 0.984375\n",
      "2017-01-11T00:49:28.085451: step 6638, loss 0.0853392, acc 0.96875\n",
      "2017-01-11T00:49:30.126709: step 6639, loss 0.0341049, acc 0.984375\n",
      "2017-01-11T00:49:33.064814: step 6640, loss 0.0531106, acc 0.992188\n",
      "2017-01-11T00:49:35.293901: step 6641, loss 0.11211, acc 0.960938\n",
      "2017-01-11T00:49:37.318175: step 6642, loss 0.0407557, acc 0.992188\n",
      "2017-01-11T00:49:39.689656: step 6643, loss 0.0461067, acc 0.984375\n",
      "2017-01-11T00:49:41.723250: step 6644, loss 0.0316767, acc 0.984375\n",
      "2017-01-11T00:49:43.753303: step 6645, loss 0.101349, acc 0.960938\n",
      "2017-01-11T00:49:45.758621: step 6646, loss 0.0473826, acc 0.984375\n",
      "2017-01-11T00:49:47.757906: step 6647, loss 0.0179112, acc 0.992188\n",
      "2017-01-11T00:49:49.823008: step 6648, loss 0.00447755, acc 1\n",
      "2017-01-11T00:49:51.902654: step 6649, loss 0.0862914, acc 0.976562\n",
      "2017-01-11T00:49:53.980156: step 6650, loss 0.00206011, acc 1\n",
      "2017-01-11T00:49:56.034277: step 6651, loss 0.0516639, acc 0.992188\n",
      "2017-01-11T00:49:58.079116: step 6652, loss 0.0818349, acc 0.96875\n",
      "2017-01-11T00:50:00.138205: step 6653, loss 0.0363544, acc 0.984375\n",
      "2017-01-11T00:50:02.196932: step 6654, loss 0.0335627, acc 0.992188\n",
      "2017-01-11T00:50:04.265198: step 6655, loss 0.055424, acc 0.984375\n",
      "2017-01-11T00:50:06.337139: step 6656, loss 0.0525481, acc 0.984375\n",
      "2017-01-11T00:50:08.378854: step 6657, loss 0.041995, acc 0.992188\n",
      "2017-01-11T00:50:10.711183: step 6658, loss 0.000363182, acc 1\n",
      "2017-01-11T00:50:12.739214: step 6659, loss 0.1091, acc 0.984375\n",
      "2017-01-11T00:50:14.801760: step 6660, loss 0.0182182, acc 0.992188\n",
      "2017-01-11T00:50:16.863680: step 6661, loss 0.062708, acc 0.976562\n",
      "2017-01-11T00:50:19.151169: step 6662, loss 0.0115317, acc 1\n",
      "2017-01-11T00:50:21.244006: step 6663, loss 0.00794423, acc 0.992188\n",
      "2017-01-11T00:50:23.281610: step 6664, loss 0.111237, acc 0.96875\n",
      "2017-01-11T00:50:25.333633: step 6665, loss 0.00113594, acc 1\n",
      "2017-01-11T00:50:27.369164: step 6666, loss 0.10615, acc 0.96875\n",
      "2017-01-11T00:50:29.688992: step 6667, loss 0.0417676, acc 0.992188\n",
      "2017-01-11T00:50:31.819395: step 6668, loss 0.0535658, acc 0.984375\n",
      "2017-01-11T00:50:33.883714: step 6669, loss 0.0190657, acc 1\n",
      "2017-01-11T00:50:35.957187: step 6670, loss 0.0367169, acc 0.984375\n",
      "2017-01-11T00:50:38.012699: step 6671, loss 0.0172854, acc 0.992188\n",
      "2017-01-11T00:50:40.064245: step 6672, loss 0.0102256, acc 1\n",
      "2017-01-11T00:50:42.428490: step 6673, loss 0.0386419, acc 0.992188\n",
      "2017-01-11T00:50:45.026401: step 6674, loss 0.0587571, acc 0.976562\n",
      "2017-01-11T00:50:47.568510: step 6675, loss 0.1053, acc 0.960938\n",
      "2017-01-11T00:50:49.602713: step 6676, loss 0.0515291, acc 0.984375\n",
      "2017-01-11T00:50:51.646757: step 6677, loss 0.0243867, acc 0.992188\n",
      "2017-01-11T00:50:53.696566: step 6678, loss 0.0439614, acc 0.976562\n",
      "2017-01-11T00:50:55.764883: step 6679, loss 0.0331273, acc 0.992188\n",
      "2017-01-11T00:50:57.775507: step 6680, loss 0.0199788, acc 0.992188\n",
      "2017-01-11T00:50:59.815853: step 6681, loss 0.0283242, acc 0.992188\n",
      "2017-01-11T00:51:01.886705: step 6682, loss 0.000258641, acc 1\n",
      "2017-01-11T00:51:03.940554: step 6683, loss 0.0235859, acc 0.992188\n",
      "2017-01-11T00:51:06.023049: step 6684, loss 0.0257636, acc 0.992188\n",
      "2017-01-11T00:51:08.072821: step 6685, loss 0.0630111, acc 0.96875\n",
      "2017-01-11T00:51:10.125993: step 6686, loss 0.0514019, acc 0.984375\n",
      "2017-01-11T00:51:12.178432: step 6687, loss 0.0516402, acc 0.984375\n",
      "2017-01-11T00:51:14.539460: step 6688, loss 0.0492122, acc 0.976562\n",
      "2017-01-11T00:51:16.572054: step 6689, loss 0.0226911, acc 0.992188\n",
      "2017-01-11T00:51:18.729217: step 6690, loss 0.0705075, acc 0.976562\n",
      "2017-01-11T00:51:20.788062: step 6691, loss 0.0625292, acc 0.976562\n",
      "2017-01-11T00:51:22.835816: step 6692, loss 0.0117188, acc 1\n",
      "2017-01-11T00:51:24.902089: step 6693, loss 0.0178023, acc 0.992188\n",
      "2017-01-11T00:51:26.929391: step 6694, loss 0.0243378, acc 0.984375\n",
      "2017-01-11T00:51:28.948749: step 6695, loss 0.011485, acc 0.992188\n",
      "2017-01-11T00:51:30.987370: step 6696, loss 0.0905721, acc 0.984375\n",
      "2017-01-11T00:51:33.058843: step 6697, loss 0.0287681, acc 0.992188\n",
      "2017-01-11T00:51:35.104099: step 6698, loss 0.0790117, acc 0.96875\n",
      "2017-01-11T00:51:37.059550: step 6699, loss 0.0645172, acc 0.976562\n",
      "2017-01-11T00:51:39.085298: step 6700, loss 0.0114891, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:52:17.764520: step 6700, loss 0.0671052, acc 0.98196\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6700\n",
      "\n",
      "2017-01-11T00:52:23.415873: step 6701, loss 0.0706928, acc 0.976562\n",
      "2017-01-11T00:52:25.472168: step 6702, loss 0.0122432, acc 0.992188\n",
      "2017-01-11T00:52:27.489128: step 6703, loss 0.0475523, acc 0.992188\n",
      "2017-01-11T00:52:29.570722: step 6704, loss 0.0240793, acc 0.992188\n",
      "2017-01-11T00:52:31.846801: step 6705, loss 0.000433092, acc 1\n",
      "2017-01-11T00:52:33.949951: step 6706, loss 0.000721895, acc 1\n",
      "2017-01-11T00:52:36.130963: step 6707, loss 0.0241321, acc 1\n",
      "2017-01-11T00:52:38.228202: step 6708, loss 0.0250557, acc 1\n",
      "2017-01-11T00:52:40.266892: step 6709, loss 0.116914, acc 0.976562\n",
      "2017-01-11T00:52:42.298168: step 6710, loss 0.141943, acc 0.96875\n",
      "2017-01-11T00:52:44.323942: step 6711, loss 0.0963227, acc 0.976562\n",
      "2017-01-11T00:52:46.373585: step 6712, loss 0.0276439, acc 0.992188\n",
      "2017-01-11T00:52:48.428818: step 6713, loss 0.0301698, acc 0.992188\n",
      "2017-01-11T00:52:50.506494: step 6714, loss 0.0562596, acc 0.984375\n",
      "2017-01-11T00:52:52.586005: step 6715, loss 0.0694771, acc 0.976562\n",
      "2017-01-11T00:52:54.911558: step 6716, loss 0.0290083, acc 0.992188\n",
      "2017-01-11T00:52:56.953682: step 6717, loss 0.0594627, acc 0.976562\n",
      "2017-01-11T00:52:59.010097: step 6718, loss 0.0702673, acc 0.96875\n",
      "2017-01-11T00:53:01.064208: step 6719, loss 0.009593, acc 1\n",
      "2017-01-11T00:53:03.111785: step 6720, loss 0.019645, acc 0.992188\n",
      "2017-01-11T00:53:05.207207: step 6721, loss 0.0428627, acc 0.984375\n",
      "2017-01-11T00:53:07.207577: step 6722, loss 0.0327735, acc 0.992188\n",
      "2017-01-11T00:53:09.286468: step 6723, loss 0.024296, acc 0.992188\n",
      "2017-01-11T00:53:11.322465: step 6724, loss 0.0363368, acc 0.992188\n",
      "2017-01-11T00:53:13.389402: step 6725, loss 0.011305, acc 1\n",
      "2017-01-11T00:53:15.475045: step 6726, loss 0.0328734, acc 0.984375\n",
      "2017-01-11T00:53:17.683396: step 6727, loss 0.117957, acc 0.960938\n",
      "2017-01-11T00:53:19.825925: step 6728, loss 0.0228659, acc 0.984375\n",
      "2017-01-11T00:53:21.862335: step 6729, loss 0.0203554, acc 0.992188\n",
      "2017-01-11T00:53:23.887920: step 6730, loss 0.0532198, acc 0.992188\n",
      "2017-01-11T00:53:26.273923: step 6731, loss 0.0412167, acc 0.992188\n",
      "2017-01-11T00:53:28.337132: step 6732, loss 0.0134124, acc 0.992188\n",
      "2017-01-11T00:53:30.379700: step 6733, loss 0.130611, acc 0.976562\n",
      "2017-01-11T00:53:33.327448: step 6734, loss 0.0493933, acc 0.992188\n",
      "2017-01-11T00:53:35.543403: step 6735, loss 0.00986355, acc 0.992188\n",
      "2017-01-11T00:53:37.613425: step 6736, loss 0.00220264, acc 1\n",
      "2017-01-11T00:53:39.688840: step 6737, loss 0.0222633, acc 0.992188\n",
      "2017-01-11T00:53:41.751801: step 6738, loss 0.00704474, acc 1\n",
      "2017-01-11T00:53:43.801332: step 6739, loss 0.106661, acc 0.96875\n",
      "2017-01-11T00:53:46.108099: step 6740, loss 0.0600645, acc 0.984375\n",
      "2017-01-11T00:53:48.433022: step 6741, loss 0.060783, acc 0.984375\n",
      "2017-01-11T00:53:50.688665: step 6742, loss 0.0637977, acc 0.976562\n",
      "2017-01-11T00:53:52.868002: step 6743, loss 0.0422645, acc 0.992188\n",
      "2017-01-11T00:53:55.070289: step 6744, loss 0.0429598, acc 0.992188\n",
      "2017-01-11T00:53:57.249859: step 6745, loss 0.08905, acc 0.96875\n",
      "2017-01-11T00:53:59.456505: step 6746, loss 0.00944811, acc 1\n",
      "2017-01-11T00:54:01.493538: step 6747, loss 0.115845, acc 0.984375\n",
      "2017-01-11T00:54:03.575120: step 6748, loss 0.0398965, acc 0.992188\n",
      "2017-01-11T00:54:05.663185: step 6749, loss 0.00475986, acc 1\n",
      "2017-01-11T00:54:07.727455: step 6750, loss 0.00142829, acc 1\n",
      "2017-01-11T00:54:09.812308: step 6751, loss 0.0116219, acc 0.992188\n",
      "2017-01-11T00:54:11.912982: step 6752, loss 0.0283647, acc 0.984375\n",
      "2017-01-11T00:54:13.964724: step 6753, loss 0.166585, acc 0.96875\n",
      "2017-01-11T00:54:16.046420: step 6754, loss 0.0206416, acc 0.992188\n",
      "2017-01-11T00:54:18.113644: step 6755, loss 0.0363191, acc 0.992188\n",
      "2017-01-11T00:54:20.261023: step 6756, loss 0.0182759, acc 0.992188\n",
      "2017-01-11T00:54:22.538539: step 6757, loss 0.00175911, acc 1\n",
      "2017-01-11T00:54:24.562950: step 6758, loss 0.063913, acc 0.984375\n",
      "2017-01-11T00:54:26.615128: step 6759, loss 0.0507104, acc 0.984375\n",
      "2017-01-11T00:54:28.667603: step 6760, loss 0.0142706, acc 1\n",
      "2017-01-11T00:54:31.139241: step 6761, loss 0.100352, acc 0.96875\n",
      "2017-01-11T00:54:33.203689: step 6762, loss 0.0744534, acc 0.976562\n",
      "2017-01-11T00:54:35.275015: step 6763, loss 0.0161671, acc 1\n",
      "2017-01-11T00:54:38.053757: step 6764, loss 0.0240314, acc 0.984375\n",
      "2017-01-11T00:54:40.429263: step 6765, loss 0.0230054, acc 0.992188\n",
      "2017-01-11T00:54:42.464036: step 6766, loss 0.0105025, acc 0.992188\n",
      "2017-01-11T00:54:44.528831: step 6767, loss 0.0499593, acc 0.984375\n",
      "2017-01-11T00:54:46.569988: step 6768, loss 0.0135506, acc 0.992188\n",
      "2017-01-11T00:54:48.605983: step 6769, loss 0.0073083, acc 1\n",
      "2017-01-11T00:54:50.691793: step 6770, loss 0.00790605, acc 1\n",
      "2017-01-11T00:54:52.754860: step 6771, loss 0.0250402, acc 0.984375\n",
      "2017-01-11T00:54:54.799554: step 6772, loss 0.0517435, acc 0.984375\n",
      "2017-01-11T00:54:56.859350: step 6773, loss 0.0303987, acc 0.992188\n",
      "2017-01-11T00:54:58.928606: step 6774, loss 0.0386133, acc 0.984375\n",
      "2017-01-11T00:55:00.973005: step 6775, loss 0.0545049, acc 0.984375\n",
      "2017-01-11T00:55:03.335255: step 6776, loss 0.0690333, acc 0.984375\n",
      "2017-01-11T00:55:05.415966: step 6777, loss 0.0647894, acc 0.992188\n",
      "2017-01-11T00:55:07.479995: step 6778, loss 0.0460838, acc 0.976562\n",
      "2017-01-11T00:55:09.536554: step 6779, loss 0.0369397, acc 0.976562\n",
      "2017-01-11T00:55:11.555533: step 6780, loss 0.00945743, acc 1\n",
      "2017-01-11T00:55:13.578107: step 6781, loss 0.0609984, acc 0.96875\n",
      "2017-01-11T00:55:15.620444: step 6782, loss 0.0555067, acc 0.984375\n",
      "2017-01-11T00:55:17.682636: step 6783, loss 0.0456258, acc 0.984375\n",
      "2017-01-11T00:55:19.808449: step 6784, loss 0.001183, acc 1\n",
      "2017-01-11T00:55:21.908833: step 6785, loss 0.0241669, acc 0.984375\n",
      "2017-01-11T00:55:23.899554: step 6786, loss 0.0738677, acc 0.976562\n",
      "2017-01-11T00:55:25.848075: step 6787, loss 0.117309, acc 0.960938\n",
      "2017-01-11T00:55:27.893495: step 6788, loss 0.0817755, acc 0.976562\n",
      "2017-01-11T00:55:30.356670: step 6789, loss 0.000114184, acc 1\n",
      "2017-01-11T00:55:32.377448: step 6790, loss 0.00692356, acc 0.992188\n",
      "2017-01-11T00:55:34.737482: step 6791, loss 0.0436733, acc 0.984375\n",
      "2017-01-11T00:55:36.752462: step 6792, loss 0.0769034, acc 0.976562\n",
      "2017-01-11T00:55:38.780758: step 6793, loss 0.0179773, acc 1\n",
      "2017-01-11T00:55:40.812634: step 6794, loss 0.00133923, acc 1\n",
      "2017-01-11T00:55:42.843047: step 6795, loss 0.106026, acc 0.976562\n",
      "2017-01-11T00:55:44.854613: step 6796, loss 0.0459764, acc 0.984375\n",
      "2017-01-11T00:55:47.962878: step 6797, loss 0.0193376, acc 1\n",
      "2017-01-11T00:55:50.161615: step 6798, loss 0.0281735, acc 0.992188\n",
      "2017-01-11T00:55:52.251089: step 6799, loss 0.00969842, acc 0.992188\n",
      "2017-01-11T00:55:54.274505: step 6800, loss 0.0610458, acc 0.976562\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T00:56:19.912547: step 6800, loss 0.068325, acc 0.98132\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6800\n",
      "\n",
      "2017-01-11T00:56:24.493701: step 6801, loss 0.00187142, acc 1\n",
      "2017-01-11T00:56:26.509469: step 6802, loss 0.0704487, acc 0.976562\n",
      "2017-01-11T00:56:28.574385: step 6803, loss 0.101146, acc 0.976562\n",
      "2017-01-11T00:56:30.709342: step 6804, loss 0.0166919, acc 0.992188\n",
      "2017-01-11T00:56:32.722608: step 6805, loss 0.0518526, acc 0.976562\n",
      "2017-01-11T00:56:34.809148: step 6806, loss 0.0397186, acc 0.992188\n",
      "2017-01-11T00:56:36.880848: step 6807, loss 0.022509, acc 0.984375\n",
      "2017-01-11T00:56:39.248113: step 6808, loss 0.043908, acc 0.984375\n",
      "2017-01-11T00:56:41.216333: step 6809, loss 0.0309163, acc 0.984375\n",
      "2017-01-11T00:56:43.227283: step 6810, loss 0.162436, acc 0.945312\n",
      "2017-01-11T00:56:45.243122: step 6811, loss 0.0293179, acc 0.984375\n",
      "2017-01-11T00:56:47.296473: step 6812, loss 0.0635548, acc 0.984375\n",
      "2017-01-11T00:56:49.351377: step 6813, loss 0.0435972, acc 1\n",
      "2017-01-11T00:56:51.458943: step 6814, loss 0.052398, acc 0.992188\n",
      "2017-01-11T00:56:53.469359: step 6815, loss 0.0211577, acc 1\n",
      "2017-01-11T00:56:55.489709: step 6816, loss 0.0479131, acc 0.984375\n",
      "2017-01-11T00:56:57.501956: step 6817, loss 0.085855, acc 0.976562\n",
      "2017-01-11T00:56:59.524522: step 6818, loss 0.058497, acc 0.976562\n",
      "2017-01-11T00:57:01.570196: step 6819, loss 0.00839608, acc 1\n",
      "2017-01-11T00:57:03.633761: step 6820, loss 0.108537, acc 0.96875\n",
      "2017-01-11T00:57:05.719852: step 6821, loss 0.0409891, acc 0.992188\n",
      "2017-01-11T00:57:07.748213: step 6822, loss 0.0297515, acc 0.992188\n",
      "2017-01-11T00:57:09.872883: step 6823, loss 0.0327044, acc 0.992188\n",
      "2017-01-11T00:57:12.071110: step 6824, loss 0.060795, acc 0.984375\n",
      "2017-01-11T00:57:14.108864: step 6825, loss 0.0827024, acc 0.984375\n",
      "2017-01-11T00:57:16.143551: step 6826, loss 0.0356466, acc 0.992188\n",
      "2017-01-11T00:57:18.191197: step 6827, loss 0.00509167, acc 1\n",
      "2017-01-11T00:57:20.257506: step 6828, loss 0.0569631, acc 0.992188\n",
      "2017-01-11T00:57:22.437586: step 6829, loss 0.00646815, acc 1\n",
      "2017-01-11T00:57:24.479279: step 6830, loss 0.0335783, acc 0.992188\n",
      "2017-01-11T00:57:26.546292: step 6831, loss 0.0238863, acc 0.992188\n",
      "2017-01-11T00:57:28.577213: step 6832, loss 0.0291443, acc 0.992188\n",
      "2017-01-11T00:57:30.615055: step 6833, loss 0.129459, acc 0.96875\n",
      "2017-01-11T00:57:32.648053: step 6834, loss 0.0156607, acc 0.992188\n",
      "2017-01-11T00:57:34.692941: step 6835, loss 0.00163815, acc 1\n",
      "2017-01-11T00:57:36.713961: step 6836, loss 0.0580302, acc 0.96875\n",
      "2017-01-11T00:57:38.764632: step 6837, loss 0.0313258, acc 0.992188\n",
      "2017-01-11T00:57:40.825185: step 6838, loss 0.0412828, acc 0.984375\n",
      "2017-01-11T00:57:43.185150: step 6839, loss 0.0204451, acc 0.992188\n",
      "2017-01-11T00:57:45.208551: step 6840, loss 0.0394148, acc 0.984375\n",
      "2017-01-11T00:57:47.180508: step 6841, loss 0.0395864, acc 0.984375\n",
      "2017-01-11T00:57:49.234211: step 6842, loss 0.0662878, acc 0.976562\n",
      "2017-01-11T00:57:51.359852: step 6843, loss 0.0762945, acc 0.984375\n",
      "2017-01-11T00:57:53.416914: step 6844, loss 0.037244, acc 0.984375\n",
      "2017-01-11T00:57:55.432135: step 6845, loss 0.0276496, acc 0.984375\n",
      "2017-01-11T00:57:57.445855: step 6846, loss 0.0309759, acc 0.992188\n",
      "2017-01-11T00:57:59.481661: step 6847, loss 0.0568604, acc 0.976562\n",
      "2017-01-11T00:58:01.567272: step 6848, loss 0.084932, acc 0.960938\n",
      "2017-01-11T00:58:03.601990: step 6849, loss 0.123105, acc 0.976562\n",
      "2017-01-11T00:58:05.674014: step 6850, loss 0.0296613, acc 0.992188\n",
      "2017-01-11T00:58:07.710049: step 6851, loss 0.0159214, acc 0.992188\n",
      "2017-01-11T00:58:09.762048: step 6852, loss 0.0452851, acc 0.984375\n",
      "2017-01-11T00:58:11.794495: step 6853, loss 0.00129766, acc 1\n",
      "2017-01-11T00:58:13.954727: step 6854, loss 0.0577196, acc 0.984375\n",
      "2017-01-11T00:58:16.177990: step 6855, loss 0.0369176, acc 0.984375\n",
      "2017-01-11T00:58:18.202705: step 6856, loss 0.0168994, acc 1\n",
      "2017-01-11T00:58:20.274416: step 6857, loss 0.0557756, acc 0.992188\n",
      "2017-01-11T00:58:22.667038: step 6858, loss 0.0389817, acc 0.984375\n",
      "2017-01-11T00:58:24.702284: step 6859, loss 0.0725864, acc 0.976562\n",
      "2017-01-11T00:58:26.751264: step 6860, loss 0.032734, acc 0.984375\n",
      "2017-01-11T00:58:28.841929: step 6861, loss 0.0628597, acc 0.984375\n",
      "2017-01-11T00:58:30.965476: step 6862, loss 0.187508, acc 0.984375\n",
      "2017-01-11T00:58:33.014170: step 6863, loss 0.021288, acc 1\n",
      "2017-01-11T00:58:35.110508: step 6864, loss 0.0447128, acc 0.984375\n",
      "2017-01-11T00:58:37.277019: step 6865, loss 0.0506427, acc 0.992188\n",
      "2017-01-11T00:58:40.305910: step 6866, loss 0.0122918, acc 0.992188\n",
      "2017-01-11T00:58:42.351884: step 6867, loss 0.0316353, acc 0.992188\n",
      "2017-01-11T00:58:44.400202: step 6868, loss 0.0134643, acc 1\n",
      "2017-01-11T00:58:46.558999: step 6869, loss 0.024142, acc 0.992188\n",
      "2017-01-11T00:58:48.804061: step 6870, loss 0.0256136, acc 0.984375\n",
      "2017-01-11T00:58:50.918533: step 6871, loss 0.0375553, acc 0.992188\n",
      "2017-01-11T00:58:52.989799: step 6872, loss 0.00426151, acc 1\n",
      "2017-01-11T00:58:55.026438: step 6873, loss 0.0416771, acc 0.984375\n",
      "2017-01-11T00:58:57.074352: step 6874, loss 0.00182965, acc 1\n",
      "2017-01-11T00:58:59.104556: step 6875, loss 0.0533917, acc 0.984375\n",
      "2017-01-11T00:59:01.167709: step 6876, loss 0.0129211, acc 1\n",
      "2017-01-11T00:59:03.222547: step 6877, loss 0.0225283, acc 0.992188\n",
      "2017-01-11T00:59:05.294948: step 6878, loss 0.0242531, acc 0.984375\n",
      "2017-01-11T00:59:07.345504: step 6879, loss 0.0383223, acc 0.992188\n",
      "2017-01-11T00:59:09.377700: step 6880, loss 0.0872856, acc 0.96875\n",
      "2017-01-11T00:59:11.426526: step 6881, loss 0.0388145, acc 0.992188\n",
      "2017-01-11T00:59:13.455263: step 6882, loss 0.0300892, acc 0.984375\n",
      "2017-01-11T00:59:15.469288: step 6883, loss 0.0734214, acc 0.96875\n",
      "2017-01-11T00:59:17.515824: step 6884, loss 0.00301002, acc 1\n",
      "2017-01-11T00:59:19.911601: step 6885, loss 0.00892234, acc 1\n",
      "2017-01-11T00:59:22.032251: step 6886, loss 0.0459125, acc 0.992188\n",
      "2017-01-11T00:59:24.090017: step 6887, loss 0.0723288, acc 0.96875\n",
      "2017-01-11T00:59:26.131308: step 6888, loss 0.0798258, acc 0.976562\n",
      "2017-01-11T00:59:28.351853: step 6889, loss 0.000851202, acc 1\n",
      "2017-01-11T00:59:30.382607: step 6890, loss 0.0209801, acc 0.992188\n",
      "2017-01-11T00:59:32.406008: step 6891, loss 0.0887976, acc 0.96875\n",
      "2017-01-11T00:59:34.456067: step 6892, loss 0.160375, acc 0.96875\n",
      "2017-01-11T00:59:36.521484: step 6893, loss 0.0543199, acc 0.976562\n",
      "2017-01-11T00:59:38.562768: step 6894, loss 0.0233658, acc 1\n",
      "2017-01-11T00:59:40.617630: step 6895, loss 0.0265175, acc 0.992188\n",
      "2017-01-11T00:59:42.613929: step 6896, loss 0.0523514, acc 0.992188\n",
      "2017-01-11T00:59:45.449247: step 6897, loss 0.015068, acc 0.992188\n",
      "2017-01-11T00:59:47.690398: step 6898, loss 0.0151887, acc 1\n",
      "2017-01-11T00:59:49.774374: step 6899, loss 0.0398688, acc 0.984375\n",
      "2017-01-11T00:59:52.157911: step 6900, loss 0.0527133, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:00:18.196954: step 6900, loss 0.0692106, acc 0.98128\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-6900\n",
      "\n",
      "2017-01-11T01:00:23.077793: step 6901, loss 0.00493081, acc 1\n",
      "2017-01-11T01:00:25.386135: step 6902, loss 0.0420016, acc 0.976562\n",
      "2017-01-11T01:00:27.475377: step 6903, loss 0.0496542, acc 0.984375\n",
      "2017-01-11T01:00:29.873116: step 6904, loss 0.00515054, acc 1\n",
      "2017-01-11T01:00:32.174884: step 6905, loss 0.0290671, acc 0.984375\n",
      "2017-01-11T01:00:34.201544: step 6906, loss 0.0380574, acc 0.984375\n",
      "2017-01-11T01:00:36.280620: step 6907, loss 0.00368563, acc 1\n",
      "2017-01-11T01:00:38.301972: step 6908, loss 0.0727517, acc 0.976562\n",
      "2017-01-11T01:00:40.338671: step 6909, loss 0.000540032, acc 1\n",
      "2017-01-11T01:00:42.390675: step 6910, loss 0.0423126, acc 1\n",
      "2017-01-11T01:00:44.417014: step 6911, loss 0.047319, acc 0.976562\n",
      "2017-01-11T01:00:47.017452: step 6912, loss 0.0641028, acc 0.976562\n",
      "2017-01-11T01:00:49.550925: step 6913, loss 0.0491754, acc 0.984375\n",
      "2017-01-11T01:00:51.565006: step 6914, loss 0.000172889, acc 1\n",
      "2017-01-11T01:00:53.639071: step 6915, loss 0.01776, acc 0.992188\n",
      "2017-01-11T01:00:55.701014: step 6916, loss 0.00443173, acc 1\n",
      "2017-01-11T01:00:58.010595: step 6917, loss 0.00277191, acc 1\n",
      "2017-01-11T01:01:00.019647: step 6918, loss 0.0372701, acc 0.992188\n",
      "2017-01-11T01:01:02.068844: step 6919, loss 0.00913484, acc 1\n",
      "2017-01-11T01:01:04.156442: step 6920, loss 0.00319006, acc 1\n",
      "2017-01-11T01:01:06.226597: step 6921, loss 0.0418326, acc 0.992188\n",
      "2017-01-11T01:01:08.245048: step 6922, loss 0.0426343, acc 0.992188\n",
      "2017-01-11T01:01:10.287298: step 6923, loss 0.0141352, acc 0.992188\n",
      "2017-01-11T01:01:12.305866: step 6924, loss 0.0140607, acc 0.992188\n",
      "2017-01-11T01:01:14.347529: step 6925, loss 0.0186795, acc 0.992188\n",
      "2017-01-11T01:01:16.395057: step 6926, loss 0.0367749, acc 0.984375\n",
      "2017-01-11T01:01:18.437991: step 6927, loss 0.0582979, acc 0.976562\n",
      "2017-01-11T01:01:20.499054: step 6928, loss 0.0578567, acc 0.984375\n",
      "2017-01-11T01:01:22.634270: step 6929, loss 0.0556, acc 0.984375\n",
      "2017-01-11T01:01:24.640444: step 6930, loss 0.00036341, acc 1\n",
      "2017-01-11T01:01:26.636833: step 6931, loss 0.0128972, acc 1\n",
      "2017-01-11T01:01:29.013908: step 6932, loss 0.0668587, acc 0.976562\n",
      "2017-01-11T01:01:31.063491: step 6933, loss 0.112315, acc 0.976562\n",
      "2017-01-11T01:01:33.097247: step 6934, loss 0.0546621, acc 0.984375\n",
      "2017-01-11T01:01:35.118878: step 6935, loss 0.0255622, acc 0.992188\n",
      "2017-01-11T01:01:37.127020: step 6936, loss 0.0497789, acc 0.984375\n",
      "2017-01-11T01:01:39.122537: step 6937, loss 0.0771032, acc 0.976562\n",
      "2017-01-11T01:01:41.142633: step 6938, loss 0.0393022, acc 0.984375\n",
      "2017-01-11T01:01:43.181764: step 6939, loss 0.0364389, acc 0.992188\n",
      "2017-01-11T01:01:45.223081: step 6940, loss 0.0918451, acc 0.96875\n",
      "2017-01-11T01:01:47.295461: step 6941, loss 0.0331539, acc 0.992188\n",
      "2017-01-11T01:01:49.361081: step 6942, loss 0.0145026, acc 1\n",
      "2017-01-11T01:01:51.460648: step 6943, loss 0.0130857, acc 0.992188\n",
      "2017-01-11T01:01:53.541026: step 6944, loss 0.0519199, acc 0.992188\n",
      "2017-01-11T01:01:55.586661: step 6945, loss 0.0301263, acc 0.992188\n",
      "2017-01-11T01:01:57.654079: step 6946, loss 0.0547657, acc 0.984375\n",
      "2017-01-11T01:01:59.825455: step 6947, loss 0.0362742, acc 0.984375\n",
      "2017-01-11T01:02:02.060421: step 6948, loss 0.0012494, acc 1\n",
      "2017-01-11T01:02:04.117752: step 6949, loss 0.0219873, acc 0.992188\n",
      "2017-01-11T01:02:06.195149: step 6950, loss 0.0614499, acc 0.984375\n",
      "2017-01-11T01:02:08.221408: step 6951, loss 0.0390368, acc 0.992188\n",
      "2017-01-11T01:02:10.268127: step 6952, loss 0.0511558, acc 0.992188\n",
      "2017-01-11T01:02:12.291043: step 6953, loss 0.0400695, acc 0.984375\n",
      "2017-01-11T01:02:14.308241: step 6954, loss 0.00972788, acc 0.992188\n",
      "2017-01-11T01:02:16.323682: step 6955, loss 0.0197844, acc 0.992188\n",
      "2017-01-11T01:02:18.358346: step 6956, loss 0.0174641, acc 0.992188\n",
      "2017-01-11T01:02:20.385487: step 6957, loss 0.0914088, acc 0.96875\n",
      "2017-01-11T01:02:22.472101: step 6958, loss 0.0625681, acc 0.976562\n",
      "2017-01-11T01:02:24.630029: step 6959, loss 0.0358173, acc 0.984375\n",
      "2017-01-11T01:02:26.654241: step 6960, loss 0.0230884, acc 0.984375\n",
      "2017-01-11T01:02:28.693865: step 6961, loss 0.0505635, acc 0.992188\n",
      "2017-01-11T01:02:30.815560: step 6962, loss 0.0301301, acc 0.992188\n",
      "2017-01-11T01:02:33.166766: step 6963, loss 0.017112, acc 1\n",
      "2017-01-11T01:02:35.238670: step 6964, loss 0.0516559, acc 0.992188\n",
      "2017-01-11T01:02:37.232292: step 6965, loss 0.0411284, acc 0.992188\n",
      "2017-01-11T01:02:39.284572: step 6966, loss 0.0769771, acc 0.976562\n",
      "2017-01-11T01:02:41.329589: step 6967, loss 0.0737507, acc 0.976562\n",
      "2017-01-11T01:02:43.379620: step 6968, loss 0.0440789, acc 0.992188\n",
      "2017-01-11T01:02:45.449631: step 6969, loss 0.0590186, acc 0.984375\n",
      "2017-01-11T01:02:47.490388: step 6970, loss 0.00268673, acc 1\n",
      "2017-01-11T01:02:49.524724: step 6971, loss 0.00477429, acc 1\n",
      "2017-01-11T01:02:51.599227: step 6972, loss 0.048587, acc 0.992188\n",
      "2017-01-11T01:02:53.666376: step 6973, loss 0.00215959, acc 1\n",
      "2017-01-11T01:02:55.725274: step 6974, loss 0.0374951, acc 0.992188\n",
      "2017-01-11T01:02:57.766279: step 6975, loss 0.00476152, acc 1\n",
      "2017-01-11T01:02:59.829148: step 6976, loss 0.0152768, acc 0.992188\n",
      "2017-01-11T01:03:01.885063: step 6977, loss 7.18109e-05, acc 1\n",
      "2017-01-11T01:03:04.166746: step 6978, loss 0.00580498, acc 1\n",
      "2017-01-11T01:03:06.277528: step 6979, loss 0.013448, acc 1\n",
      "2017-01-11T01:03:08.358979: step 6980, loss 0.0530398, acc 0.984375\n",
      "2017-01-11T01:03:10.406764: step 6981, loss 0.0823634, acc 0.96875\n",
      "2017-01-11T01:03:12.447678: step 6982, loss 0.055658, acc 0.984375\n",
      "2017-01-11T01:03:14.495175: step 6983, loss 0.0344862, acc 0.992188\n",
      "2017-01-11T01:03:16.546628: step 6984, loss 0.0590107, acc 0.992188\n",
      "2017-01-11T01:03:18.564691: step 6985, loss 0.0269909, acc 1\n",
      "2017-01-11T01:03:20.663745: step 6986, loss 0.0524658, acc 0.984375\n",
      "2017-01-11T01:03:22.749975: step 6987, loss 0.0178048, acc 1\n",
      "2017-01-11T01:03:24.880859: step 6988, loss 0.00789407, acc 0.992188\n",
      "2017-01-11T01:03:26.932822: step 6989, loss 0.00365628, acc 1\n",
      "2017-01-11T01:03:29.163136: step 6990, loss 0.0486173, acc 0.976562\n",
      "2017-01-11T01:03:31.229560: step 6991, loss 0.0503028, acc 0.984375\n",
      "2017-01-11T01:03:33.253750: step 6992, loss 0.000213711, acc 1\n",
      "2017-01-11T01:03:35.386116: step 6993, loss 0.0298286, acc 1\n",
      "2017-01-11T01:03:37.589239: step 6994, loss 0.0204158, acc 1\n",
      "2017-01-11T01:03:39.637006: step 6995, loss 0.0331018, acc 0.992188\n",
      "2017-01-11T01:03:41.595802: step 6996, loss 0.0182212, acc 0.992188\n",
      "2017-01-11T01:03:44.044838: step 6997, loss 0.0105342, acc 1\n",
      "2017-01-11T01:03:46.743071: step 6998, loss 0.0385119, acc 0.984375\n",
      "2017-01-11T01:03:48.785502: step 6999, loss 0.0146049, acc 0.992188\n",
      "2017-01-11T01:03:50.825657: step 7000, loss 0.0284482, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:04:20.127092: step 7000, loss 0.068931, acc 0.98168\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7000\n",
      "\n",
      "2017-01-11T01:04:24.885549: step 7001, loss 0.0342361, acc 0.992188\n",
      "2017-01-11T01:04:26.915739: step 7002, loss 0.0458613, acc 0.984375\n",
      "2017-01-11T01:04:28.935531: step 7003, loss 0.046126, acc 0.984375\n",
      "2017-01-11T01:04:31.083892: step 7004, loss 0.0466064, acc 0.992188\n",
      "2017-01-11T01:04:33.317091: step 7005, loss 0.0784961, acc 0.976562\n",
      "2017-01-11T01:04:35.359711: step 7006, loss 0.0571382, acc 0.984375\n",
      "2017-01-11T01:04:37.422523: step 7007, loss 0.0187861, acc 0.992188\n",
      "2017-01-11T01:04:39.459398: step 7008, loss 0.0244886, acc 0.992188\n",
      "2017-01-11T01:04:41.496168: step 7009, loss 0.0983333, acc 0.960938\n",
      "2017-01-11T01:04:43.725956: step 7010, loss 0.0056749, acc 1\n",
      "2017-01-11T01:04:45.935523: step 7011, loss 0.0575924, acc 0.976562\n",
      "2017-01-11T01:04:47.969571: step 7012, loss 0.155335, acc 0.96875\n",
      "2017-01-11T01:04:51.126505: step 7013, loss 0.00171608, acc 1\n",
      "2017-01-11T01:04:53.266260: step 7014, loss 0.0217371, acc 0.992188\n",
      "2017-01-11T01:04:55.284808: step 7015, loss 0.116072, acc 0.96875\n",
      "2017-01-11T01:04:57.291022: step 7016, loss 0.0345176, acc 0.984375\n",
      "2017-01-11T01:04:59.326123: step 7017, loss 0.0303994, acc 0.984375\n",
      "2017-01-11T01:05:01.381981: step 7018, loss 0.0481899, acc 0.976562\n",
      "2017-01-11T01:05:03.438630: step 7019, loss 0.0550125, acc 0.984375\n",
      "2017-01-11T01:05:05.484396: step 7020, loss 0.0270197, acc 0.984375\n",
      "2017-01-11T01:05:07.516732: step 7021, loss 0.0028283, acc 1\n",
      "2017-01-11T01:05:09.554064: step 7022, loss 0.170202, acc 0.96875\n",
      "2017-01-11T01:05:11.604995: step 7023, loss 0.0212383, acc 1\n",
      "2017-01-11T01:05:13.637293: step 7024, loss 0.00958909, acc 0.992188\n",
      "2017-01-11T01:05:15.764060: step 7025, loss 0.00112772, acc 1\n",
      "2017-01-11T01:05:17.974303: step 7026, loss 0.0539207, acc 0.976562\n",
      "2017-01-11T01:05:20.035155: step 7027, loss 0.019477, acc 0.992188\n",
      "2017-01-11T01:05:22.113300: step 7028, loss 0.00879778, acc 1\n",
      "2017-01-11T01:05:24.140822: step 7029, loss 0.0152812, acc 0.992188\n",
      "2017-01-11T01:05:26.257846: step 7030, loss 0.0132917, acc 1\n",
      "2017-01-11T01:05:28.309028: step 7031, loss 0.0710985, acc 0.992188\n",
      "2017-01-11T01:05:30.292023: step 7032, loss 0.054052, acc 0.971154\n",
      "2017-01-11T01:05:32.427729: step 7033, loss 0.0142382, acc 1\n",
      "2017-01-11T01:05:34.757424: step 7034, loss 0.0589446, acc 0.976562\n",
      "2017-01-11T01:05:36.777802: step 7035, loss 0.0091474, acc 0.992188\n",
      "2017-01-11T01:05:38.841790: step 7036, loss 0.0162023, acc 0.992188\n",
      "2017-01-11T01:05:40.896057: step 7037, loss 0.0156591, acc 0.992188\n",
      "2017-01-11T01:05:42.961193: step 7038, loss 0.016705, acc 0.992188\n",
      "2017-01-11T01:05:44.987697: step 7039, loss 0.0184405, acc 0.992188\n",
      "2017-01-11T01:05:47.049117: step 7040, loss 0.0283516, acc 0.992188\n",
      "2017-01-11T01:05:49.951183: step 7041, loss 0.000263664, acc 1\n",
      "2017-01-11T01:05:52.611998: step 7042, loss 0.0539236, acc 0.984375\n",
      "2017-01-11T01:05:54.658515: step 7043, loss 0.00400469, acc 1\n",
      "2017-01-11T01:05:56.682782: step 7044, loss 0.0425923, acc 0.984375\n",
      "2017-01-11T01:05:58.695453: step 7045, loss 0.0415124, acc 0.984375\n",
      "2017-01-11T01:06:00.720956: step 7046, loss 0.00101403, acc 1\n",
      "2017-01-11T01:06:02.729040: step 7047, loss 0.0403315, acc 0.984375\n",
      "2017-01-11T01:06:04.791570: step 7048, loss 0.00760709, acc 0.992188\n",
      "2017-01-11T01:06:06.810350: step 7049, loss 0.0197488, acc 0.984375\n",
      "2017-01-11T01:06:08.850902: step 7050, loss 0.0262776, acc 0.992188\n",
      "2017-01-11T01:06:10.899426: step 7051, loss 0.0171068, acc 0.992188\n",
      "2017-01-11T01:06:12.908745: step 7052, loss 0.0582626, acc 0.976562\n",
      "2017-01-11T01:06:14.921542: step 7053, loss 0.0504677, acc 0.984375\n",
      "2017-01-11T01:06:16.965697: step 7054, loss 0.00265806, acc 1\n",
      "2017-01-11T01:06:19.050720: step 7055, loss 0.0393286, acc 0.984375\n",
      "2017-01-11T01:06:21.478236: step 7056, loss 0.0371699, acc 0.992188\n",
      "2017-01-11T01:06:23.517122: step 7057, loss 0.0929788, acc 0.96875\n",
      "2017-01-11T01:06:25.640868: step 7058, loss 0.0291559, acc 0.984375\n",
      "2017-01-11T01:06:27.683412: step 7059, loss 0.000584532, acc 1\n",
      "2017-01-11T01:06:29.694755: step 7060, loss 0.00865918, acc 1\n",
      "2017-01-11T01:06:31.847741: step 7061, loss 0.0150514, acc 1\n",
      "2017-01-11T01:06:33.882506: step 7062, loss 0.0475732, acc 0.984375\n",
      "2017-01-11T01:06:35.954435: step 7063, loss 0.0107573, acc 0.992188\n",
      "2017-01-11T01:06:37.976993: step 7064, loss 0.0375005, acc 0.992188\n",
      "2017-01-11T01:06:40.049352: step 7065, loss 0.00712375, acc 1\n",
      "2017-01-11T01:06:42.091041: step 7066, loss 0.0142172, acc 1\n",
      "2017-01-11T01:06:44.139045: step 7067, loss 0.027742, acc 1\n",
      "2017-01-11T01:06:46.202944: step 7068, loss 0.0013, acc 1\n",
      "2017-01-11T01:06:48.279497: step 7069, loss 0.0376402, acc 0.992188\n",
      "2017-01-11T01:06:50.316017: step 7070, loss 0.0316355, acc 0.992188\n",
      "2017-01-11T01:06:52.632868: step 7071, loss 0.0519701, acc 0.976562\n",
      "2017-01-11T01:06:54.751239: step 7072, loss 0.0260032, acc 0.984375\n",
      "2017-01-11T01:06:56.743992: step 7073, loss 0.0270671, acc 0.992188\n",
      "2017-01-11T01:06:58.792435: step 7074, loss 0.0831802, acc 0.96875\n",
      "2017-01-11T01:07:00.806457: step 7075, loss 0.0282527, acc 0.984375\n",
      "2017-01-11T01:07:02.829887: step 7076, loss 0.0613139, acc 0.992188\n",
      "2017-01-11T01:07:04.916060: step 7077, loss 0.0643928, acc 0.96875\n",
      "2017-01-11T01:07:06.963807: step 7078, loss 0.00463496, acc 1\n",
      "2017-01-11T01:07:08.992787: step 7079, loss 0.00321417, acc 1\n",
      "2017-01-11T01:07:10.988961: step 7080, loss 0.0529289, acc 0.984375\n",
      "2017-01-11T01:07:13.010161: step 7081, loss 0.0386718, acc 0.992188\n",
      "2017-01-11T01:07:15.046680: step 7082, loss 0.00313185, acc 1\n",
      "2017-01-11T01:07:17.088964: step 7083, loss 0.0309073, acc 0.984375\n",
      "2017-01-11T01:07:19.142577: step 7084, loss 0.012947, acc 0.992188\n",
      "2017-01-11T01:07:21.240556: step 7085, loss 0.00408174, acc 1\n",
      "2017-01-11T01:07:23.282352: step 7086, loss 0.00796811, acc 1\n",
      "2017-01-11T01:07:25.739988: step 7087, loss 0.0109131, acc 0.992188\n",
      "2017-01-11T01:07:27.757516: step 7088, loss 0.0252988, acc 0.992188\n",
      "2017-01-11T01:07:29.766369: step 7089, loss 0.00901699, acc 1\n",
      "2017-01-11T01:07:31.807569: step 7090, loss 0.0182999, acc 1\n",
      "2017-01-11T01:07:33.882010: step 7091, loss 0.0401973, acc 0.984375\n",
      "2017-01-11T01:07:35.978827: step 7092, loss 0.0736626, acc 0.976562\n",
      "2017-01-11T01:07:38.023197: step 7093, loss 0.0236028, acc 0.992188\n",
      "2017-01-11T01:07:40.062716: step 7094, loss 0.0327304, acc 0.984375\n",
      "2017-01-11T01:07:42.104364: step 7095, loss 0.0448759, acc 0.992188\n",
      "2017-01-11T01:07:44.154285: step 7096, loss 0.0610039, acc 0.992188\n",
      "2017-01-11T01:07:46.218251: step 7097, loss 0.0241331, acc 0.992188\n",
      "2017-01-11T01:07:48.253679: step 7098, loss 0.0323986, acc 0.992188\n",
      "2017-01-11T01:07:50.324240: step 7099, loss 0.0485158, acc 0.992188\n",
      "2017-01-11T01:07:52.384555: step 7100, loss 0.00237522, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:08:18.356863: step 7100, loss 0.0679342, acc 0.98204\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7100\n",
      "\n",
      "2017-01-11T01:08:22.937896: step 7101, loss 0.0451054, acc 0.992188\n",
      "2017-01-11T01:08:24.978327: step 7102, loss 0.0481179, acc 0.976562\n",
      "2017-01-11T01:08:27.044215: step 7103, loss 0.0285597, acc 0.992188\n",
      "2017-01-11T01:08:29.358518: step 7104, loss 0.00691254, acc 1\n",
      "2017-01-11T01:08:31.701271: step 7105, loss 0.0222972, acc 0.992188\n",
      "2017-01-11T01:08:33.929684: step 7106, loss 0.0178568, acc 1\n",
      "2017-01-11T01:08:36.031231: step 7107, loss 0.00113831, acc 1\n",
      "2017-01-11T01:08:38.065197: step 7108, loss 0.00840141, acc 0.992188\n",
      "2017-01-11T01:08:40.115744: step 7109, loss 0.0250321, acc 0.992188\n",
      "2017-01-11T01:08:42.125361: step 7110, loss 0.0305563, acc 0.992188\n",
      "2017-01-11T01:08:44.227371: step 7111, loss 0.0189936, acc 0.992188\n",
      "2017-01-11T01:08:46.272401: step 7112, loss 0.00724752, acc 1\n",
      "2017-01-11T01:08:48.341134: step 7113, loss 0.000647707, acc 1\n",
      "2017-01-11T01:08:51.462241: step 7114, loss 0.0301855, acc 0.992188\n",
      "2017-01-11T01:08:53.640513: step 7115, loss 0.0184346, acc 1\n",
      "2017-01-11T01:08:55.674910: step 7116, loss 0.0272168, acc 0.984375\n",
      "2017-01-11T01:08:57.708928: step 7117, loss 0.0376345, acc 0.984375\n",
      "2017-01-11T01:08:59.734477: step 7118, loss 0.00412487, acc 1\n",
      "2017-01-11T01:09:01.899847: step 7119, loss 0.0316822, acc 0.976562\n",
      "2017-01-11T01:09:04.157609: step 7120, loss 0.0375106, acc 0.992188\n",
      "2017-01-11T01:09:06.220128: step 7121, loss 0.0178301, acc 1\n",
      "2017-01-11T01:09:08.272863: step 7122, loss 0.0280275, acc 0.992188\n",
      "2017-01-11T01:09:10.290202: step 7123, loss 0.0393572, acc 0.984375\n",
      "2017-01-11T01:09:12.311322: step 7124, loss 0.026645, acc 0.992188\n",
      "2017-01-11T01:09:14.343047: step 7125, loss 0.0516972, acc 0.984375\n",
      "2017-01-11T01:09:16.349368: step 7126, loss 0.0555487, acc 0.984375\n",
      "2017-01-11T01:09:18.401802: step 7127, loss 0.0101809, acc 1\n",
      "2017-01-11T01:09:20.470322: step 7128, loss 0.0187689, acc 1\n",
      "2017-01-11T01:09:22.554633: step 7129, loss 0.00637972, acc 1\n",
      "2017-01-11T01:09:24.598388: step 7130, loss 0.0331589, acc 0.992188\n",
      "2017-01-11T01:09:26.718603: step 7131, loss 0.0442374, acc 0.984375\n",
      "2017-01-11T01:09:28.704028: step 7132, loss 0.0373893, acc 0.984375\n",
      "2017-01-11T01:09:30.780924: step 7133, loss 0.000350293, acc 1\n",
      "2017-01-11T01:09:32.808376: step 7134, loss 0.0307546, acc 0.992188\n",
      "2017-01-11T01:09:35.253176: step 7135, loss 0.0132444, acc 0.992188\n",
      "2017-01-11T01:09:37.299386: step 7136, loss 0.0465364, acc 0.976562\n",
      "2017-01-11T01:09:39.506408: step 7137, loss 0.00616575, acc 1\n",
      "2017-01-11T01:09:41.546193: step 7138, loss 0.0462384, acc 0.992188\n",
      "2017-01-11T01:09:43.585720: step 7139, loss 0.00681731, acc 0.992188\n",
      "2017-01-11T01:09:45.591805: step 7140, loss 0.00286331, acc 1\n",
      "2017-01-11T01:09:47.650636: step 7141, loss 0.0773879, acc 0.992188\n",
      "2017-01-11T01:09:49.768981: step 7142, loss 0.0133283, acc 1\n",
      "2017-01-11T01:09:51.847974: step 7143, loss 0.0273415, acc 0.992188\n",
      "2017-01-11T01:09:53.879122: step 7144, loss 0.0400974, acc 0.984375\n",
      "2017-01-11T01:09:57.040353: step 7145, loss 0.0416431, acc 0.984375\n",
      "2017-01-11T01:09:59.057559: step 7146, loss 0.0792101, acc 0.984375\n",
      "2017-01-11T01:10:01.109342: step 7147, loss 0.0313789, acc 0.992188\n",
      "2017-01-11T01:10:03.161417: step 7148, loss 0.0724923, acc 0.992188\n",
      "2017-01-11T01:10:05.279984: step 7149, loss 0.0865332, acc 0.976562\n",
      "2017-01-11T01:10:07.681900: step 7150, loss 0.00859644, acc 1\n",
      "2017-01-11T01:10:09.690720: step 7151, loss 0.0881799, acc 0.96875\n",
      "2017-01-11T01:10:11.713435: step 7152, loss 0.0805843, acc 0.96875\n",
      "2017-01-11T01:10:13.739956: step 7153, loss 0.0321056, acc 0.984375\n",
      "2017-01-11T01:10:15.766351: step 7154, loss 0.00802043, acc 1\n",
      "2017-01-11T01:10:17.847286: step 7155, loss 0.070983, acc 0.984375\n",
      "2017-01-11T01:10:19.932769: step 7156, loss 0.0125172, acc 1\n",
      "2017-01-11T01:10:21.975231: step 7157, loss 0.0465729, acc 0.984375\n",
      "2017-01-11T01:10:24.024319: step 7158, loss 0.0531864, acc 0.984375\n",
      "2017-01-11T01:10:26.046868: step 7159, loss 0.0196706, acc 0.992188\n",
      "2017-01-11T01:10:28.169486: step 7160, loss 0.0460109, acc 0.992188\n",
      "2017-01-11T01:10:30.319025: step 7161, loss 0.0145878, acc 1\n",
      "2017-01-11T01:10:32.369258: step 7162, loss 0.0180853, acc 0.992188\n",
      "2017-01-11T01:10:34.436433: step 7163, loss 0.048257, acc 0.984375\n",
      "2017-01-11T01:10:36.497073: step 7164, loss 0.0287284, acc 0.984375\n",
      "2017-01-11T01:10:38.954383: step 7165, loss 0.0244608, acc 1\n",
      "2017-01-11T01:10:41.266284: step 7166, loss 0.0130637, acc 1\n",
      "2017-01-11T01:10:43.310518: step 7167, loss 0.0229276, acc 0.992188\n",
      "2017-01-11T01:10:45.319355: step 7168, loss 0.0138452, acc 0.992188\n",
      "2017-01-11T01:10:47.351318: step 7169, loss 0.0330097, acc 0.984375\n",
      "2017-01-11T01:10:49.389178: step 7170, loss 0.0457746, acc 0.984375\n",
      "2017-01-11T01:10:51.511792: step 7171, loss 0.046219, acc 0.984375\n",
      "2017-01-11T01:10:53.526117: step 7172, loss 0.0076027, acc 1\n",
      "2017-01-11T01:10:56.033208: step 7173, loss 0.0060821, acc 1\n",
      "2017-01-11T01:10:58.659617: step 7174, loss 0.0138705, acc 0.992188\n",
      "2017-01-11T01:11:00.805502: step 7175, loss 0.000829505, acc 1\n",
      "2017-01-11T01:11:02.823754: step 7176, loss 0.0906596, acc 0.96875\n",
      "2017-01-11T01:11:04.895650: step 7177, loss 0.00391971, acc 1\n",
      "2017-01-11T01:11:06.941516: step 7178, loss 0.0186412, acc 0.992188\n",
      "2017-01-11T01:11:08.977924: step 7179, loss 0.00340763, acc 1\n",
      "2017-01-11T01:11:11.256935: step 7180, loss 0.0356635, acc 0.992188\n",
      "2017-01-11T01:11:13.332710: step 7181, loss 0.00881731, acc 1\n",
      "2017-01-11T01:11:15.352415: step 7182, loss 0.168441, acc 0.976562\n",
      "2017-01-11T01:11:17.396161: step 7183, loss 0.0189916, acc 0.992188\n",
      "2017-01-11T01:11:19.437734: step 7184, loss 0.0130007, acc 1\n",
      "2017-01-11T01:11:21.533873: step 7185, loss 0.0544012, acc 0.976562\n",
      "2017-01-11T01:11:23.560817: step 7186, loss 0.0037484, acc 1\n",
      "2017-01-11T01:11:25.601266: step 7187, loss 0.0286734, acc 0.984375\n",
      "2017-01-11T01:11:27.698344: step 7188, loss 0.00778683, acc 1\n",
      "2017-01-11T01:11:29.747562: step 7189, loss 0.0296065, acc 0.992188\n",
      "2017-01-11T01:11:31.783244: step 7190, loss 0.00753743, acc 1\n",
      "2017-01-11T01:11:33.786147: step 7191, loss 0.0459109, acc 0.984375\n",
      "2017-01-11T01:11:35.853358: step 7192, loss 0.0132148, acc 1\n",
      "2017-01-11T01:11:37.913310: step 7193, loss 0.0120851, acc 1\n",
      "2017-01-11T01:11:39.990219: step 7194, loss 0.0283488, acc 0.992188\n",
      "2017-01-11T01:11:42.003296: step 7195, loss 0.0449103, acc 0.992188\n",
      "2017-01-11T01:11:44.349543: step 7196, loss 0.00158657, acc 1\n",
      "2017-01-11T01:11:46.384415: step 7197, loss 0.0895932, acc 0.96875\n",
      "2017-01-11T01:11:48.417952: step 7198, loss 0.0554857, acc 0.976562\n",
      "2017-01-11T01:11:50.486112: step 7199, loss 0.0345548, acc 0.992188\n",
      "2017-01-11T01:11:52.549444: step 7200, loss 0.0386847, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:12:28.510729: step 7200, loss 0.0709857, acc 0.98192\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7200\n",
      "\n",
      "2017-01-11T01:12:33.229799: step 7201, loss 0.0276038, acc 0.984375\n",
      "2017-01-11T01:12:35.627542: step 7202, loss 0.0559952, acc 0.976562\n",
      "2017-01-11T01:12:38.002323: step 7203, loss 0.0157979, acc 0.992188\n",
      "2017-01-11T01:12:40.153611: step 7204, loss 0.012366, acc 1\n",
      "2017-01-11T01:12:42.391678: step 7205, loss 0.0562753, acc 0.984375\n",
      "2017-01-11T01:12:44.582866: step 7206, loss 0.0318043, acc 0.992188\n",
      "2017-01-11T01:12:46.700268: step 7207, loss 0.0508419, acc 0.984375\n",
      "2017-01-11T01:12:48.824085: step 7208, loss 0.0112463, acc 1\n",
      "2017-01-11T01:12:51.235325: step 7209, loss 0.0189203, acc 0.992188\n",
      "2017-01-11T01:12:53.524795: step 7210, loss 0.00763678, acc 0.992188\n",
      "2017-01-11T01:12:55.577670: step 7211, loss 0.0360571, acc 0.984375\n",
      "2017-01-11T01:12:57.628362: step 7212, loss 0.00598023, acc 1\n",
      "2017-01-11T01:12:59.658472: step 7213, loss 0.00914669, acc 1\n",
      "2017-01-11T01:13:01.727172: step 7214, loss 0.085702, acc 0.96875\n",
      "2017-01-11T01:13:03.771458: step 7215, loss 0.0367107, acc 0.984375\n",
      "2017-01-11T01:13:05.820654: step 7216, loss 0.0618321, acc 0.984375\n",
      "2017-01-11T01:13:07.870384: step 7217, loss 0.0360774, acc 0.992188\n",
      "2017-01-11T01:13:09.903203: step 7218, loss 0.107453, acc 0.976562\n",
      "2017-01-11T01:13:11.951078: step 7219, loss 0.014947, acc 1\n",
      "2017-01-11T01:13:13.977591: step 7220, loss 0.00575948, acc 1\n",
      "2017-01-11T01:13:15.954203: step 7221, loss 0.00139193, acc 1\n",
      "2017-01-11T01:13:17.970454: step 7222, loss 0.00948901, acc 1\n",
      "2017-01-11T01:13:20.037999: step 7223, loss 0.0220514, acc 0.992188\n",
      "2017-01-11T01:13:22.101049: step 7224, loss 0.0394054, acc 0.992188\n",
      "2017-01-11T01:13:24.436942: step 7225, loss 0.00260563, acc 1\n",
      "2017-01-11T01:13:26.485882: step 7226, loss 0.0601634, acc 0.984375\n",
      "2017-01-11T01:13:28.593097: step 7227, loss 0.0479935, acc 0.984375\n",
      "2017-01-11T01:13:30.605109: step 7228, loss 0.0181018, acc 0.992188\n",
      "2017-01-11T01:13:32.650930: step 7229, loss 0.00841121, acc 1\n",
      "2017-01-11T01:13:34.703508: step 7230, loss 0.0436551, acc 0.984375\n",
      "2017-01-11T01:13:36.668773: step 7231, loss 0.0169815, acc 0.992188\n",
      "2017-01-11T01:13:38.690317: step 7232, loss 0.0358019, acc 0.992188\n",
      "2017-01-11T01:13:40.915981: step 7233, loss 0.035474, acc 0.984375\n",
      "2017-01-11T01:13:42.948139: step 7234, loss 0.0147454, acc 0.992188\n",
      "2017-01-11T01:13:44.963326: step 7235, loss 0.0552004, acc 0.984375\n",
      "2017-01-11T01:13:47.069289: step 7236, loss 0.019406, acc 0.992188\n",
      "2017-01-11T01:13:49.081451: step 7237, loss 0.0428832, acc 0.984375\n",
      "2017-01-11T01:13:51.151697: step 7238, loss 0.00101811, acc 1\n",
      "2017-01-11T01:13:53.295532: step 7239, loss 0.00314359, acc 1\n",
      "2017-01-11T01:13:55.624822: step 7240, loss 0.0153612, acc 0.992188\n",
      "2017-01-11T01:13:58.880972: step 7241, loss 0.0813713, acc 0.976562\n",
      "2017-01-11T01:14:00.924308: step 7242, loss 0.00113614, acc 1\n",
      "2017-01-11T01:14:02.926053: step 7243, loss 0.0139906, acc 0.992188\n",
      "2017-01-11T01:14:05.021936: step 7244, loss 0.0292988, acc 1\n",
      "2017-01-11T01:14:07.134004: step 7245, loss 0.0351737, acc 1\n",
      "2017-01-11T01:14:09.182937: step 7246, loss 0.00459083, acc 1\n",
      "2017-01-11T01:14:11.215944: step 7247, loss 0.0387643, acc 0.984375\n",
      "2017-01-11T01:14:13.272305: step 7248, loss 0.023598, acc 0.992188\n",
      "2017-01-11T01:14:15.309833: step 7249, loss 0.0208055, acc 0.992188\n",
      "2017-01-11T01:14:17.346682: step 7250, loss 0.00224512, acc 1\n",
      "2017-01-11T01:14:19.393951: step 7251, loss 0.0114601, acc 0.992188\n",
      "2017-01-11T01:14:21.497606: step 7252, loss 0.0478866, acc 0.984375\n",
      "2017-01-11T01:14:23.561912: step 7253, loss 0.000689, acc 1\n",
      "2017-01-11T01:14:25.622876: step 7254, loss 0.088438, acc 0.984375\n",
      "2017-01-11T01:14:27.612267: step 7255, loss 0.00744216, acc 1\n",
      "2017-01-11T01:14:30.080375: step 7256, loss 0.0347025, acc 0.992188\n",
      "2017-01-11T01:14:32.214843: step 7257, loss 0.0539458, acc 0.984375\n",
      "2017-01-11T01:14:34.248474: step 7258, loss 0.0339187, acc 0.984375\n",
      "2017-01-11T01:14:36.306094: step 7259, loss 0.000934134, acc 1\n",
      "2017-01-11T01:14:38.355035: step 7260, loss 0.00186446, acc 1\n",
      "2017-01-11T01:14:40.373255: step 7261, loss 0.0117919, acc 1\n",
      "2017-01-11T01:14:42.415922: step 7262, loss 0.0168873, acc 0.992188\n",
      "2017-01-11T01:14:44.635595: step 7263, loss 0.000529124, acc 1\n",
      "2017-01-11T01:14:46.624471: step 7264, loss 0.0647488, acc 0.984375\n",
      "2017-01-11T01:14:48.674514: step 7265, loss 0.0359201, acc 0.984375\n",
      "2017-01-11T01:14:50.718197: step 7266, loss 0.0436452, acc 0.984375\n",
      "2017-01-11T01:14:52.806444: step 7267, loss 0.022735, acc 1\n",
      "2017-01-11T01:14:54.839288: step 7268, loss 0.00595039, acc 1\n",
      "2017-01-11T01:14:56.893620: step 7269, loss 0.0380707, acc 0.992188\n",
      "2017-01-11T01:14:58.929971: step 7270, loss 0.0100042, acc 0.992188\n",
      "2017-01-11T01:15:02.382477: step 7271, loss 0.0541469, acc 0.984375\n",
      "2017-01-11T01:15:04.542309: step 7272, loss 0.0347362, acc 0.984375\n",
      "2017-01-11T01:15:06.575301: step 7273, loss 0.00391504, acc 1\n",
      "2017-01-11T01:15:08.600370: step 7274, loss 0.0887798, acc 0.976562\n",
      "2017-01-11T01:15:10.656284: step 7275, loss 0.104752, acc 0.96875\n",
      "2017-01-11T01:15:12.698537: step 7276, loss 0.0113342, acc 1\n",
      "2017-01-11T01:15:14.766259: step 7277, loss 0.00256333, acc 1\n",
      "2017-01-11T01:15:16.793885: step 7278, loss 0.0395714, acc 0.984375\n",
      "2017-01-11T01:15:18.827663: step 7279, loss 0.0108606, acc 1\n",
      "2017-01-11T01:15:20.898941: step 7280, loss 0.0897806, acc 0.984375\n",
      "2017-01-11T01:15:22.958209: step 7281, loss 0.00145827, acc 1\n",
      "2017-01-11T01:15:24.985708: step 7282, loss 0.0544545, acc 0.984375\n",
      "2017-01-11T01:15:27.032145: step 7283, loss 0.0721191, acc 0.984375\n",
      "2017-01-11T01:15:29.159986: step 7284, loss 0.0154768, acc 0.992188\n",
      "2017-01-11T01:15:31.177875: step 7285, loss 0.000663776, acc 1\n",
      "2017-01-11T01:15:33.401771: step 7286, loss 0.0317654, acc 0.992188\n",
      "2017-01-11T01:15:35.590396: step 7287, loss 0.0170685, acc 0.992188\n",
      "2017-01-11T01:15:37.617949: step 7288, loss 0.0316488, acc 0.992188\n",
      "2017-01-11T01:15:39.640366: step 7289, loss 0.0891122, acc 0.96875\n",
      "2017-01-11T01:15:41.670320: step 7290, loss 0.0137317, acc 1\n",
      "2017-01-11T01:15:43.679738: step 7291, loss 0.0305294, acc 0.992188\n",
      "2017-01-11T01:15:45.988162: step 7292, loss 0.0347779, acc 0.992188\n",
      "2017-01-11T01:15:47.996209: step 7293, loss 0.00515016, acc 1\n",
      "2017-01-11T01:15:50.028230: step 7294, loss 0.0304209, acc 0.984375\n",
      "2017-01-11T01:15:52.090274: step 7295, loss 0.0354665, acc 0.992188\n",
      "2017-01-11T01:15:54.117345: step 7296, loss 0.0169061, acc 0.992188\n",
      "2017-01-11T01:15:56.149287: step 7297, loss 0.0106339, acc 1\n",
      "2017-01-11T01:15:58.206740: step 7298, loss 0.0113813, acc 1\n",
      "2017-01-11T01:16:00.486724: step 7299, loss 0.00179468, acc 1\n",
      "2017-01-11T01:16:03.307653: step 7300, loss 0.00776659, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:16:32.275107: step 7300, loss 0.0682658, acc 0.98248\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7300\n",
      "\n",
      "2017-01-11T01:16:36.719547: step 7301, loss 0.0881871, acc 0.984375\n",
      "2017-01-11T01:16:39.067622: step 7302, loss 0.00370012, acc 1\n",
      "2017-01-11T01:16:41.289019: step 7303, loss 0.0625073, acc 0.984375\n",
      "2017-01-11T01:16:43.332287: step 7304, loss 0.00707757, acc 1\n",
      "2017-01-11T01:16:45.370215: step 7305, loss 0.0017786, acc 1\n",
      "2017-01-11T01:16:47.395114: step 7306, loss 0.0246011, acc 0.992188\n",
      "2017-01-11T01:16:49.444841: step 7307, loss 0.000972688, acc 1\n",
      "2017-01-11T01:16:51.545366: step 7308, loss 0.052139, acc 0.992188\n",
      "2017-01-11T01:16:53.567735: step 7309, loss 0.043955, acc 0.992188\n",
      "2017-01-11T01:16:55.619557: step 7310, loss 0.00100531, acc 1\n",
      "2017-01-11T01:16:57.662521: step 7311, loss 0.0294968, acc 0.992188\n",
      "2017-01-11T01:16:59.702465: step 7312, loss 0.0329749, acc 0.984375\n",
      "2017-01-11T01:17:01.773473: step 7313, loss 0.0455183, acc 0.976562\n",
      "2017-01-11T01:17:03.843324: step 7314, loss 0.0368816, acc 0.984375\n",
      "2017-01-11T01:17:05.949520: step 7315, loss 0.0516547, acc 0.984375\n",
      "2017-01-11T01:17:08.012523: step 7316, loss 0.0195461, acc 0.992188\n",
      "2017-01-11T01:17:10.067730: step 7317, loss 0.0348458, acc 0.992188\n",
      "2017-01-11T01:17:12.405354: step 7318, loss 0.0697422, acc 0.984375\n",
      "2017-01-11T01:17:14.395072: step 7319, loss 0.00172495, acc 1\n",
      "2017-01-11T01:17:16.428540: step 7320, loss 0.024432, acc 0.984375\n",
      "2017-01-11T01:17:18.464262: step 7321, loss 0.00345825, acc 1\n",
      "2017-01-11T01:17:20.542741: step 7322, loss 0.000276025, acc 1\n",
      "2017-01-11T01:17:22.599743: step 7323, loss 0.00853345, acc 1\n",
      "2017-01-11T01:17:24.651104: step 7324, loss 0.00979922, acc 1\n",
      "2017-01-11T01:17:26.713130: step 7325, loss 0.0168989, acc 0.992188\n",
      "2017-01-11T01:17:28.721788: step 7326, loss 0.00199709, acc 1\n",
      "2017-01-11T01:17:30.849310: step 7327, loss 0.00486322, acc 1\n",
      "2017-01-11T01:17:32.865593: step 7328, loss 0.0134295, acc 0.992188\n",
      "2017-01-11T01:17:34.951159: step 7329, loss 0.0203399, acc 1\n",
      "2017-01-11T01:17:36.960888: step 7330, loss 0.0297193, acc 0.992188\n",
      "2017-01-11T01:17:39.001434: step 7331, loss 0.0450376, acc 0.984375\n",
      "2017-01-11T01:17:41.036964: step 7332, loss 0.0390325, acc 0.984375\n",
      "2017-01-11T01:17:43.205911: step 7333, loss 0.0103558, acc 0.992188\n",
      "2017-01-11T01:17:45.418558: step 7334, loss 0.00716782, acc 1\n",
      "2017-01-11T01:17:47.448214: step 7335, loss 0.0066057, acc 1\n",
      "2017-01-11T01:17:49.553464: step 7336, loss 0.00605815, acc 0.992188\n",
      "2017-01-11T01:17:51.636251: step 7337, loss 0.0158702, acc 0.992188\n",
      "2017-01-11T01:17:53.676145: step 7338, loss 0.0088549, acc 1\n",
      "2017-01-11T01:17:55.743938: step 7339, loss 0.0626344, acc 0.984375\n",
      "2017-01-11T01:17:57.795936: step 7340, loss 0.0209398, acc 0.992188\n",
      "2017-01-11T01:17:59.836665: step 7341, loss 0.0853369, acc 0.976562\n",
      "2017-01-11T01:18:01.878324: step 7342, loss 0.0246168, acc 0.992188\n",
      "2017-01-11T01:18:03.907552: step 7343, loss 0.0209118, acc 1\n",
      "2017-01-11T01:18:06.027433: step 7344, loss 0.0182082, acc 0.992188\n",
      "2017-01-11T01:18:08.060649: step 7345, loss 0.0117275, acc 0.992188\n",
      "2017-01-11T01:18:10.085840: step 7346, loss 0.0399099, acc 0.992188\n",
      "2017-01-11T01:18:12.102917: step 7347, loss 0.0190684, acc 0.992188\n",
      "2017-01-11T01:18:14.140730: step 7348, loss 0.0341883, acc 0.992188\n",
      "2017-01-11T01:18:16.482368: step 7349, loss 0.0192452, acc 0.992188\n",
      "2017-01-11T01:18:18.467000: step 7350, loss 0.00547772, acc 1\n",
      "2017-01-11T01:18:20.525192: step 7351, loss 0.0286028, acc 0.992188\n",
      "2017-01-11T01:18:22.585478: step 7352, loss 0.027821, acc 0.992188\n",
      "2017-01-11T01:18:24.623300: step 7353, loss 0.00945955, acc 0.992188\n",
      "2017-01-11T01:18:26.633364: step 7354, loss 0.0197073, acc 0.992188\n",
      "2017-01-11T01:18:28.631178: step 7355, loss 0.0395261, acc 0.976562\n",
      "2017-01-11T01:18:30.837493: step 7356, loss 0.0946337, acc 0.984375\n",
      "2017-01-11T01:18:32.883556: step 7357, loss 0.00099975, acc 1\n",
      "2017-01-11T01:18:34.943283: step 7358, loss 0.00842961, acc 1\n",
      "2017-01-11T01:18:36.963127: step 7359, loss 0.00816769, acc 1\n",
      "2017-01-11T01:18:38.984548: step 7360, loss 0.0113132, acc 0.992188\n",
      "2017-01-11T01:18:40.994630: step 7361, loss 0.0124395, acc 0.992188\n",
      "2017-01-11T01:18:43.017058: step 7362, loss 0.0111499, acc 1\n",
      "2017-01-11T01:18:45.077379: step 7363, loss 0.0325699, acc 0.992188\n",
      "2017-01-11T01:18:47.471907: step 7364, loss 0.00214253, acc 1\n",
      "2017-01-11T01:18:49.690275: step 7365, loss 0.0199721, acc 0.992188\n",
      "2017-01-11T01:18:51.756313: step 7366, loss 0.0164506, acc 0.992188\n",
      "2017-01-11T01:18:53.764728: step 7367, loss 0.000682363, acc 1\n",
      "2017-01-11T01:18:55.839165: step 7368, loss 0.0217308, acc 0.992188\n",
      "2017-01-11T01:18:57.893535: step 7369, loss 0.0808061, acc 0.984375\n",
      "2017-01-11T01:18:59.900218: step 7370, loss 0.0298759, acc 0.992188\n",
      "2017-01-11T01:19:02.487547: step 7371, loss 0.162666, acc 0.992188\n",
      "2017-01-11T01:19:05.069809: step 7372, loss 0.0296457, acc 0.992188\n",
      "2017-01-11T01:19:07.105850: step 7373, loss 0.00279504, acc 1\n",
      "2017-01-11T01:19:09.124672: step 7374, loss 0.00321231, acc 1\n",
      "2017-01-11T01:19:11.119535: step 7375, loss 0.0561802, acc 0.976562\n",
      "2017-01-11T01:19:13.199519: step 7376, loss 0.0316899, acc 0.992188\n",
      "2017-01-11T01:19:15.218108: step 7377, loss 0.00430309, acc 1\n",
      "2017-01-11T01:19:17.277100: step 7378, loss 0.010069, acc 1\n",
      "2017-01-11T01:19:19.469588: step 7379, loss 0.00882804, acc 1\n",
      "2017-01-11T01:19:21.704541: step 7380, loss 0.0129257, acc 1\n",
      "2017-01-11T01:19:23.726335: step 7381, loss 0.0674633, acc 0.984375\n",
      "2017-01-11T01:19:25.774990: step 7382, loss 0.0322273, acc 0.984375\n",
      "2017-01-11T01:19:27.824823: step 7383, loss 0.0547007, acc 0.984375\n",
      "2017-01-11T01:19:29.890858: step 7384, loss 0.00921741, acc 1\n",
      "2017-01-11T01:19:32.046497: step 7385, loss 0.0164735, acc 0.992188\n",
      "2017-01-11T01:19:34.078774: step 7386, loss 0.0613317, acc 0.984375\n",
      "2017-01-11T01:19:36.152658: step 7387, loss 0.0253183, acc 0.992188\n",
      "2017-01-11T01:19:38.211370: step 7388, loss 0.0087212, acc 1\n",
      "2017-01-11T01:19:40.209897: step 7389, loss 0.0160834, acc 0.992188\n",
      "2017-01-11T01:19:42.207251: step 7390, loss 0.000300407, acc 1\n",
      "2017-01-11T01:19:44.226274: step 7391, loss 0.017894, acc 0.992188\n",
      "2017-01-11T01:19:46.248675: step 7392, loss 0.0182535, acc 0.992188\n",
      "2017-01-11T01:19:48.305622: step 7393, loss 3.98136e-05, acc 1\n",
      "2017-01-11T01:19:50.534831: step 7394, loss 0.0841269, acc 0.976562\n",
      "2017-01-11T01:19:52.939649: step 7395, loss 0.00818388, acc 1\n",
      "2017-01-11T01:19:55.002540: step 7396, loss 0.00733772, acc 0.992188\n",
      "2017-01-11T01:19:56.968275: step 7397, loss 0.030786, acc 0.992188\n",
      "2017-01-11T01:19:59.013942: step 7398, loss 0.00225202, acc 1\n",
      "2017-01-11T01:20:01.071818: step 7399, loss 0.0289945, acc 0.992188\n",
      "2017-01-11T01:20:03.138954: step 7400, loss 0.00134464, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:20:31.685158: step 7400, loss 0.0678118, acc 0.98236\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7400\n",
      "\n",
      "2017-01-11T01:20:36.399323: step 7401, loss 0.0696909, acc 0.984375\n",
      "2017-01-11T01:20:38.424173: step 7402, loss 0.00612098, acc 1\n",
      "2017-01-11T01:20:40.442201: step 7403, loss 0.0216112, acc 0.992188\n",
      "2017-01-11T01:20:42.473653: step 7404, loss 0.0174834, acc 0.992188\n",
      "2017-01-11T01:20:44.506754: step 7405, loss 0.0143122, acc 1\n",
      "2017-01-11T01:20:46.791857: step 7406, loss 0.00528162, acc 1\n",
      "2017-01-11T01:20:48.783698: step 7407, loss 0.0406078, acc 0.992188\n",
      "2017-01-11T01:20:51.147577: step 7408, loss 0.0830284, acc 0.96875\n",
      "2017-01-11T01:20:53.231165: step 7409, loss 0.0228091, acc 0.992188\n",
      "2017-01-11T01:20:55.272605: step 7410, loss 0.000413177, acc 1\n",
      "2017-01-11T01:20:57.506827: step 7411, loss 0.00757731, acc 1\n",
      "2017-01-11T01:20:59.610451: step 7412, loss 0.0656624, acc 0.976562\n",
      "2017-01-11T01:21:01.668966: step 7413, loss 0.028762, acc 0.992188\n",
      "2017-01-11T01:21:03.750426: step 7414, loss 0.0281195, acc 0.992188\n",
      "2017-01-11T01:21:06.119901: step 7415, loss 0.022704, acc 0.992188\n",
      "2017-01-11T01:21:08.980755: step 7416, loss 0.0377002, acc 0.992188\n",
      "2017-01-11T01:21:11.003257: step 7417, loss 0.0693712, acc 0.976562\n",
      "2017-01-11T01:21:13.069997: step 7418, loss 0.030715, acc 0.992188\n",
      "2017-01-11T01:21:15.099410: step 7419, loss 0.02967, acc 0.992188\n",
      "2017-01-11T01:21:17.147280: step 7420, loss 0.0354901, acc 0.984375\n",
      "2017-01-11T01:21:19.164349: step 7421, loss 0.109083, acc 0.976562\n",
      "2017-01-11T01:21:21.284240: step 7422, loss 0.0178606, acc 1\n",
      "2017-01-11T01:21:23.289671: step 7423, loss 0.0292322, acc 1\n",
      "2017-01-11T01:21:25.332270: step 7424, loss 0.00969002, acc 0.992188\n",
      "2017-01-11T01:21:27.369653: step 7425, loss 0.0341035, acc 0.992188\n",
      "2017-01-11T01:21:29.581830: step 7426, loss 0.0725852, acc 0.976562\n",
      "2017-01-11T01:21:31.791781: step 7427, loss 0.0731122, acc 0.976562\n",
      "2017-01-11T01:21:33.861834: step 7428, loss 0.0465783, acc 0.984375\n",
      "2017-01-11T01:21:35.928589: step 7429, loss 0.0015899, acc 1\n",
      "2017-01-11T01:21:37.958504: step 7430, loss 0.0452328, acc 0.984375\n",
      "2017-01-11T01:21:39.966563: step 7431, loss 0.0625034, acc 0.984375\n",
      "2017-01-11T01:21:42.033307: step 7432, loss 0.00268534, acc 1\n",
      "2017-01-11T01:21:44.075583: step 7433, loss 0.00803282, acc 1\n",
      "2017-01-11T01:21:46.113230: step 7434, loss 0.000611902, acc 1\n",
      "2017-01-11T01:21:48.154727: step 7435, loss 0.0242084, acc 0.992188\n",
      "2017-01-11T01:21:50.253745: step 7436, loss 0.00987349, acc 1\n",
      "2017-01-11T01:21:52.303303: step 7437, loss 0.0285882, acc 0.984375\n",
      "2017-01-11T01:21:54.345100: step 7438, loss 0.00469997, acc 1\n",
      "2017-01-11T01:21:56.415496: step 7439, loss 0.0607053, acc 0.984375\n",
      "2017-01-11T01:21:58.445831: step 7440, loss 0.0453079, acc 0.976562\n",
      "2017-01-11T01:22:00.530086: step 7441, loss 0.0277727, acc 0.992188\n",
      "2017-01-11T01:22:02.855089: step 7442, loss 0.0102335, acc 0.992188\n",
      "2017-01-11T01:22:04.935394: step 7443, loss 0.00829582, acc 1\n",
      "2017-01-11T01:22:07.003068: step 7444, loss 0.000394641, acc 1\n",
      "2017-01-11T01:22:09.041420: step 7445, loss 0.051932, acc 0.992188\n",
      "2017-01-11T01:22:11.063575: step 7446, loss 0.0218016, acc 0.992188\n",
      "2017-01-11T01:22:13.088572: step 7447, loss 0.00289566, acc 1\n",
      "2017-01-11T01:22:15.127076: step 7448, loss 0.0195974, acc 1\n",
      "2017-01-11T01:22:17.208049: step 7449, loss 0.0352319, acc 0.984375\n",
      "2017-01-11T01:22:19.273499: step 7450, loss 0.000593162, acc 1\n",
      "2017-01-11T01:22:21.395576: step 7451, loss 0.000217752, acc 1\n",
      "2017-01-11T01:22:23.464757: step 7452, loss 0.109492, acc 0.992188\n",
      "2017-01-11T01:22:25.507450: step 7453, loss 0.052528, acc 0.984375\n",
      "2017-01-11T01:22:27.567965: step 7454, loss 0.0441315, acc 0.992188\n",
      "2017-01-11T01:22:29.608992: step 7455, loss 0.00354297, acc 1\n",
      "2017-01-11T01:22:31.813166: step 7456, loss 0.00223509, acc 1\n",
      "2017-01-11T01:22:34.181563: step 7457, loss 0.0138211, acc 1\n",
      "2017-01-11T01:22:36.210403: step 7458, loss 0.000320868, acc 1\n",
      "2017-01-11T01:22:38.233285: step 7459, loss 0.0248392, acc 0.992188\n",
      "2017-01-11T01:22:40.290638: step 7460, loss 0.000849825, acc 1\n",
      "2017-01-11T01:22:42.300451: step 7461, loss 0.0352755, acc 0.992188\n",
      "2017-01-11T01:22:44.336359: step 7462, loss 0.01095, acc 1\n",
      "2017-01-11T01:22:46.400694: step 7463, loss 0.0547772, acc 0.984375\n",
      "2017-01-11T01:22:48.439888: step 7464, loss 0.0132836, acc 0.992188\n",
      "2017-01-11T01:22:50.493111: step 7465, loss 0.00501113, acc 1\n",
      "2017-01-11T01:22:52.565004: step 7466, loss 0.0465488, acc 0.992188\n",
      "2017-01-11T01:22:54.646346: step 7467, loss 0.0245238, acc 0.992188\n",
      "2017-01-11T01:22:56.680061: step 7468, loss 0.0144025, acc 1\n",
      "2017-01-11T01:22:58.725145: step 7469, loss 0.0197099, acc 0.992188\n",
      "2017-01-11T01:23:00.761367: step 7470, loss 0.000962176, acc 1\n",
      "2017-01-11T01:23:02.804634: step 7471, loss 0.0162827, acc 0.992188\n",
      "2017-01-11T01:23:04.932529: step 7472, loss 0.0132356, acc 1\n",
      "2017-01-11T01:23:07.168639: step 7473, loss 0.0209981, acc 1\n",
      "2017-01-11T01:23:09.221282: step 7474, loss 0.0201847, acc 0.992188\n",
      "2017-01-11T01:23:11.275997: step 7475, loss 0.0544409, acc 0.976562\n",
      "2017-01-11T01:23:13.314535: step 7476, loss 0.0301002, acc 0.992188\n",
      "2017-01-11T01:23:15.341700: step 7477, loss 0.00805393, acc 1\n",
      "2017-01-11T01:23:17.404309: step 7478, loss 0.0182943, acc 0.992188\n",
      "2017-01-11T01:23:19.458756: step 7479, loss 0.0263763, acc 0.992188\n",
      "2017-01-11T01:23:21.548768: step 7480, loss 0.0434215, acc 0.992188\n",
      "2017-01-11T01:23:23.579765: step 7481, loss 0.0513743, acc 0.976562\n",
      "2017-01-11T01:23:25.615072: step 7482, loss 0.0333667, acc 0.992188\n",
      "2017-01-11T01:23:27.643455: step 7483, loss 0.0372378, acc 0.992188\n",
      "2017-01-11T01:23:29.665117: step 7484, loss 0.00709949, acc 0.992188\n",
      "2017-01-11T01:23:31.718581: step 7485, loss 0.00177855, acc 1\n",
      "2017-01-11T01:23:33.889718: step 7486, loss 0.0270715, acc 0.992188\n",
      "2017-01-11T01:23:35.951962: step 7487, loss 0.0128282, acc 1\n",
      "2017-01-11T01:23:38.326463: step 7488, loss 0.00479587, acc 1\n",
      "2017-01-11T01:23:40.343510: step 7489, loss 0.0616073, acc 0.984375\n",
      "2017-01-11T01:23:42.410931: step 7490, loss 0.0471566, acc 0.992188\n",
      "2017-01-11T01:23:44.452648: step 7491, loss 0.0458647, acc 0.984375\n",
      "2017-01-11T01:23:46.432126: step 7492, loss 0.0140455, acc 0.992188\n",
      "2017-01-11T01:23:48.488538: step 7493, loss 0.0175733, acc 0.992188\n",
      "2017-01-11T01:23:50.564957: step 7494, loss 0.0156268, acc 0.992188\n",
      "2017-01-11T01:23:52.841476: step 7495, loss 0.0638389, acc 0.984375\n",
      "2017-01-11T01:23:54.910504: step 7496, loss 0.068026, acc 0.976562\n",
      "2017-01-11T01:23:56.974583: step 7497, loss 0.00911743, acc 1\n",
      "2017-01-11T01:23:58.989123: step 7498, loss 0.0047954, acc 1\n",
      "2017-01-11T01:24:01.065256: step 7499, loss 0.0149636, acc 0.992188\n",
      "2017-01-11T01:24:03.137332: step 7500, loss 0.00382996, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:24:33.987946: step 7500, loss 0.0716502, acc 0.98164\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7500\n",
      "\n",
      "2017-01-11T01:24:38.523581: step 7501, loss 0.0122914, acc 1\n",
      "2017-01-11T01:24:40.579189: step 7502, loss 0.00782461, acc 1\n",
      "2017-01-11T01:24:42.846291: step 7503, loss 0.0545862, acc 0.976562\n",
      "2017-01-11T01:24:45.107399: step 7504, loss 0.0553233, acc 0.984375\n",
      "2017-01-11T01:24:47.147278: step 7505, loss 0.0712764, acc 0.976562\n",
      "2017-01-11T01:24:49.213737: step 7506, loss 0.0226218, acc 0.992188\n",
      "2017-01-11T01:24:51.336919: step 7507, loss 0.00314694, acc 1\n",
      "2017-01-11T01:24:53.378030: step 7508, loss 0.0373077, acc 0.984375\n",
      "2017-01-11T01:24:55.502733: step 7509, loss 0.0335773, acc 0.992188\n",
      "2017-01-11T01:24:57.640111: step 7510, loss 0.0328647, acc 0.992188\n",
      "2017-01-11T01:24:59.683727: step 7511, loss 0.00272758, acc 1\n",
      "2017-01-11T01:25:01.763143: step 7512, loss 0.0151949, acc 1\n",
      "2017-01-11T01:25:03.824019: step 7513, loss 0.0244988, acc 0.992188\n",
      "2017-01-11T01:25:05.856501: step 7514, loss 0.0480342, acc 0.976562\n",
      "2017-01-11T01:25:07.930887: step 7515, loss 0.00875525, acc 0.992188\n",
      "2017-01-11T01:25:09.961011: step 7516, loss 0.0233805, acc 1\n",
      "2017-01-11T01:25:12.858708: step 7517, loss 0.10155, acc 0.976562\n",
      "2017-01-11T01:25:15.077322: step 7518, loss 0.00803004, acc 1\n",
      "2017-01-11T01:25:17.449540: step 7519, loss 0.0406544, acc 0.992188\n",
      "2017-01-11T01:25:19.459903: step 7520, loss 0.00417089, acc 1\n",
      "2017-01-11T01:25:21.577911: step 7521, loss 0.0179901, acc 0.992188\n",
      "2017-01-11T01:25:23.617996: step 7522, loss 0.0529376, acc 0.984375\n",
      "2017-01-11T01:25:25.670242: step 7523, loss 0.0119271, acc 0.992188\n",
      "2017-01-11T01:25:27.678100: step 7524, loss 0.0566353, acc 0.976562\n",
      "2017-01-11T01:25:29.695848: step 7525, loss 0.0150866, acc 1\n",
      "2017-01-11T01:25:31.721587: step 7526, loss 0.0189661, acc 0.992188\n",
      "2017-01-11T01:25:33.836864: step 7527, loss 0.0416651, acc 0.992188\n",
      "2017-01-11T01:25:35.875238: step 7528, loss 0.00178965, acc 1\n",
      "2017-01-11T01:25:37.923585: step 7529, loss 0.049766, acc 0.976562\n",
      "2017-01-11T01:25:39.965124: step 7530, loss 0.05671, acc 0.984375\n",
      "2017-01-11T01:25:41.997119: step 7531, loss 0.0247764, acc 0.992188\n",
      "2017-01-11T01:25:44.067534: step 7532, loss 0.00359445, acc 1\n",
      "2017-01-11T01:25:46.112822: step 7533, loss 0.00450164, acc 1\n",
      "2017-01-11T01:25:48.389218: step 7534, loss 0.016204, acc 0.984375\n",
      "2017-01-11T01:25:50.524526: step 7535, loss 0.0367121, acc 0.992188\n",
      "2017-01-11T01:25:52.619675: step 7536, loss 0.00397124, acc 1\n",
      "2017-01-11T01:25:54.700435: step 7537, loss 0.0444935, acc 0.984375\n",
      "2017-01-11T01:25:57.117946: step 7538, loss 0.00576856, acc 1\n",
      "2017-01-11T01:25:59.126131: step 7539, loss 0.0386363, acc 0.984375\n",
      "2017-01-11T01:26:01.169789: step 7540, loss 0.00221798, acc 1\n",
      "2017-01-11T01:26:03.182253: step 7541, loss 0.0340399, acc 0.984375\n",
      "2017-01-11T01:26:05.293118: step 7542, loss 0.0144892, acc 0.992188\n",
      "2017-01-11T01:26:07.336577: step 7543, loss 0.000183208, acc 1\n",
      "2017-01-11T01:26:09.357226: step 7544, loss 0.00458511, acc 1\n",
      "2017-01-11T01:26:11.419825: step 7545, loss 0.0175284, acc 0.992188\n",
      "2017-01-11T01:26:14.371818: step 7546, loss 0.0273611, acc 0.992188\n",
      "2017-01-11T01:26:16.668747: step 7547, loss 0.0314225, acc 0.984375\n",
      "2017-01-11T01:26:18.725136: step 7548, loss 0.0830823, acc 0.992188\n",
      "2017-01-11T01:26:21.151089: step 7549, loss 0.0144204, acc 1\n",
      "2017-01-11T01:26:23.230704: step 7550, loss 0.00431987, acc 1\n",
      "2017-01-11T01:26:25.256442: step 7551, loss 0.00841426, acc 1\n",
      "2017-01-11T01:26:27.296562: step 7552, loss 0.0525191, acc 0.992188\n",
      "2017-01-11T01:26:29.304669: step 7553, loss 0.00514282, acc 1\n",
      "2017-01-11T01:26:31.474339: step 7554, loss 0.0434309, acc 0.992188\n",
      "2017-01-11T01:26:33.598927: step 7555, loss 0.0202642, acc 0.992188\n",
      "2017-01-11T01:26:35.674550: step 7556, loss 0.0841285, acc 0.96875\n",
      "2017-01-11T01:26:37.688182: step 7557, loss 0.00606861, acc 1\n",
      "2017-01-11T01:26:39.732646: step 7558, loss 0.0033733, acc 1\n",
      "2017-01-11T01:26:41.800039: step 7559, loss 0.0228033, acc 0.992188\n",
      "2017-01-11T01:26:43.851630: step 7560, loss 0.10195, acc 0.976562\n",
      "2017-01-11T01:26:45.866501: step 7561, loss 0.0859909, acc 0.992188\n",
      "2017-01-11T01:26:47.879718: step 7562, loss 0.0932939, acc 0.984375\n",
      "2017-01-11T01:26:49.980189: step 7563, loss 0.0236327, acc 0.992188\n",
      "2017-01-11T01:26:52.378154: step 7564, loss 0.0139507, acc 0.992188\n",
      "2017-01-11T01:26:54.405287: step 7565, loss 0.0154347, acc 0.992188\n",
      "2017-01-11T01:26:56.431644: step 7566, loss 0.0441072, acc 0.984375\n",
      "2017-01-11T01:26:58.473956: step 7567, loss 0.013093, acc 0.992188\n",
      "2017-01-11T01:27:00.536487: step 7568, loss 0.00635825, acc 1\n",
      "2017-01-11T01:27:02.557922: step 7569, loss 0.04158, acc 0.984375\n",
      "2017-01-11T01:27:04.643030: step 7570, loss 0.00647974, acc 1\n",
      "2017-01-11T01:27:06.696484: step 7571, loss 0.0176991, acc 0.992188\n",
      "2017-01-11T01:27:08.729136: step 7572, loss 0.0332601, acc 0.984375\n",
      "2017-01-11T01:27:10.796470: step 7573, loss 0.00911248, acc 1\n",
      "2017-01-11T01:27:12.785352: step 7574, loss 0.0181447, acc 0.992188\n",
      "2017-01-11T01:27:14.829565: step 7575, loss 0.00365161, acc 1\n",
      "2017-01-11T01:27:16.881692: step 7576, loss 0.00916679, acc 1\n",
      "2017-01-11T01:27:18.907422: step 7577, loss 0.0877985, acc 0.976562\n",
      "2017-01-11T01:27:20.964072: step 7578, loss 0.0267367, acc 0.992188\n",
      "2017-01-11T01:27:23.074716: step 7579, loss 0.039001, acc 0.992188\n",
      "2017-01-11T01:27:25.388445: step 7580, loss 0.0298722, acc 0.992188\n",
      "2017-01-11T01:27:27.388543: step 7581, loss 0.0107797, acc 1\n",
      "2017-01-11T01:27:29.400389: step 7582, loss 0.014483, acc 0.992188\n",
      "2017-01-11T01:27:31.434576: step 7583, loss 0.000248749, acc 1\n",
      "2017-01-11T01:27:33.491453: step 7584, loss 0.0120426, acc 0.992188\n",
      "2017-01-11T01:27:35.680045: step 7585, loss 0.0384202, acc 0.992188\n",
      "2017-01-11T01:27:37.736674: step 7586, loss 0.0334082, acc 0.984375\n",
      "2017-01-11T01:27:39.784774: step 7587, loss 0.0581268, acc 0.976562\n",
      "2017-01-11T01:27:41.826060: step 7588, loss 0.0264983, acc 0.984375\n",
      "2017-01-11T01:27:43.857811: step 7589, loss 0.0315295, acc 0.992188\n",
      "2017-01-11T01:27:45.867545: step 7590, loss 0.0174262, acc 1\n",
      "2017-01-11T01:27:47.934221: step 7591, loss 0.00469621, acc 1\n",
      "2017-01-11T01:27:49.991762: step 7592, loss 0.0259425, acc 1\n",
      "2017-01-11T01:27:52.044288: step 7593, loss 0.0248946, acc 0.992188\n",
      "2017-01-11T01:27:54.054520: step 7594, loss 0.090665, acc 0.960938\n",
      "2017-01-11T01:27:56.360603: step 7595, loss 0.0579114, acc 0.984375\n",
      "2017-01-11T01:27:58.433779: step 7596, loss 0.0648991, acc 0.984375\n",
      "2017-01-11T01:28:00.500637: step 7597, loss 0.00238498, acc 1\n",
      "2017-01-11T01:28:02.523478: step 7598, loss 0.0133692, acc 0.992188\n",
      "2017-01-11T01:28:04.603579: step 7599, loss 0.0169083, acc 0.992188\n",
      "2017-01-11T01:28:06.653358: step 7600, loss 0.00221673, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:28:32.791024: step 7600, loss 0.0691014, acc 0.98268\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7600\n",
      "\n",
      "2017-01-11T01:28:37.307727: step 7601, loss 0.0427029, acc 0.992188\n",
      "2017-01-11T01:28:39.345597: step 7602, loss 0.0215004, acc 0.992188\n",
      "2017-01-11T01:28:41.391247: step 7603, loss 0.0223471, acc 0.992188\n",
      "2017-01-11T01:28:43.434466: step 7604, loss 0.00741207, acc 0.992188\n",
      "2017-01-11T01:28:45.468214: step 7605, loss 0.0264805, acc 0.992188\n",
      "2017-01-11T01:28:47.522606: step 7606, loss 0.0254356, acc 0.992188\n",
      "2017-01-11T01:28:49.537076: step 7607, loss 0.041766, acc 0.976562\n",
      "2017-01-11T01:28:51.631496: step 7608, loss 0.0387259, acc 0.992188\n",
      "2017-01-11T01:28:53.642350: step 7609, loss 0.0196246, acc 0.992188\n",
      "2017-01-11T01:28:55.675318: step 7610, loss 0.00209942, acc 1\n",
      "2017-01-11T01:28:57.891519: step 7611, loss 0.00440548, acc 1\n",
      "2017-01-11T01:28:59.910886: step 7612, loss 0.0366353, acc 0.992188\n",
      "2017-01-11T01:29:02.247758: step 7613, loss 0.0479317, acc 0.984375\n",
      "2017-01-11T01:29:04.321381: step 7614, loss 0.0500545, acc 0.984375\n",
      "2017-01-11T01:29:06.393562: step 7615, loss 0.0480619, acc 0.984375\n",
      "2017-01-11T01:29:08.450228: step 7616, loss 0.0182688, acc 0.992188\n",
      "2017-01-11T01:29:10.508206: step 7617, loss 0.0704205, acc 0.976562\n",
      "2017-01-11T01:29:12.562007: step 7618, loss 0.000882456, acc 1\n",
      "2017-01-11T01:29:15.622885: step 7619, loss 0.0253185, acc 0.992188\n",
      "2017-01-11T01:29:17.749639: step 7620, loss 0.0304587, acc 0.984375\n",
      "2017-01-11T01:29:20.121246: step 7621, loss 0.0297384, acc 0.992188\n",
      "2017-01-11T01:29:22.531824: step 7622, loss 0.0211293, acc 1\n",
      "2017-01-11T01:29:24.729567: step 7623, loss 0.0363194, acc 0.992188\n",
      "2017-01-11T01:29:27.011465: step 7624, loss 0.0602082, acc 0.984375\n",
      "2017-01-11T01:29:29.099603: step 7625, loss 0.0078423, acc 1\n",
      "2017-01-11T01:29:31.178506: step 7626, loss 4.12899e-05, acc 1\n",
      "2017-01-11T01:29:33.305571: step 7627, loss 0.0214889, acc 1\n",
      "2017-01-11T01:29:35.700010: step 7628, loss 0.00647278, acc 1\n",
      "2017-01-11T01:29:37.715099: step 7629, loss 0.0559243, acc 0.976562\n",
      "2017-01-11T01:29:39.719192: step 7630, loss 0.0254744, acc 0.984375\n",
      "2017-01-11T01:29:41.755956: step 7631, loss 0.0434832, acc 0.984375\n",
      "2017-01-11T01:29:43.762782: step 7632, loss 0.0562079, acc 0.984375\n",
      "2017-01-11T01:29:45.795174: step 7633, loss 0.0166061, acc 0.992188\n",
      "2017-01-11T01:29:47.834267: step 7634, loss 0.0609615, acc 0.96875\n",
      "2017-01-11T01:29:49.908048: step 7635, loss 0.0150399, acc 1\n",
      "2017-01-11T01:29:51.929552: step 7636, loss 0.0180607, acc 1\n",
      "2017-01-11T01:29:53.949285: step 7637, loss 0.0464769, acc 0.992188\n",
      "2017-01-11T01:29:55.996550: step 7638, loss 0.000563134, acc 1\n",
      "2017-01-11T01:29:58.027760: step 7639, loss 0.0380063, acc 0.984375\n",
      "2017-01-11T01:30:00.066943: step 7640, loss 0.0164853, acc 0.992188\n",
      "2017-01-11T01:30:02.301484: step 7641, loss 0.0132147, acc 0.992188\n",
      "2017-01-11T01:30:04.370888: step 7642, loss 2.7147e-05, acc 1\n",
      "2017-01-11T01:30:06.745945: step 7643, loss 0.00719628, acc 1\n",
      "2017-01-11T01:30:08.766749: step 7644, loss 0.00353372, acc 1\n",
      "2017-01-11T01:30:10.755633: step 7645, loss 0.0263977, acc 1\n",
      "2017-01-11T01:30:12.865311: step 7646, loss 0.0339359, acc 0.984375\n",
      "2017-01-11T01:30:14.960012: step 7647, loss 0.00413023, acc 1\n",
      "2017-01-11T01:30:17.456948: step 7648, loss 0.0502379, acc 0.984375\n",
      "2017-01-11T01:30:20.201987: step 7649, loss 0.0242598, acc 0.992188\n",
      "2017-01-11T01:30:22.278718: step 7650, loss 0.0644388, acc 0.984375\n",
      "2017-01-11T01:30:24.321383: step 7651, loss 0.0510962, acc 0.984375\n",
      "2017-01-11T01:30:26.358674: step 7652, loss 0.0174263, acc 0.992188\n",
      "2017-01-11T01:30:28.422173: step 7653, loss 0.107911, acc 0.992188\n",
      "2017-01-11T01:30:30.527077: step 7654, loss 0.0018226, acc 1\n",
      "2017-01-11T01:30:32.568207: step 7655, loss 0.00239074, acc 1\n",
      "2017-01-11T01:30:34.645525: step 7656, loss 0.0167142, acc 0.992188\n",
      "2017-01-11T01:30:36.840253: step 7657, loss 0.0357294, acc 0.984375\n",
      "2017-01-11T01:30:39.178781: step 7658, loss 0.0263571, acc 0.992188\n",
      "2017-01-11T01:30:41.211760: step 7659, loss 0.00250548, acc 1\n",
      "2017-01-11T01:30:43.239936: step 7660, loss 0.0069327, acc 1\n",
      "2017-01-11T01:30:45.287688: step 7661, loss 0.0475761, acc 0.984375\n",
      "2017-01-11T01:30:47.273454: step 7662, loss 0.00106742, acc 1\n",
      "2017-01-11T01:30:49.313847: step 7663, loss 0.00172217, acc 1\n",
      "2017-01-11T01:30:51.401017: step 7664, loss 0.0211584, acc 0.992188\n",
      "2017-01-11T01:30:53.408811: step 7665, loss 0.0222992, acc 0.992188\n",
      "2017-01-11T01:30:55.432344: step 7666, loss 0.0399228, acc 0.984375\n",
      "2017-01-11T01:30:57.499940: step 7667, loss 0.0679516, acc 0.984375\n",
      "2017-01-11T01:30:59.521290: step 7668, loss 0.0592878, acc 0.976562\n",
      "2017-01-11T01:31:01.799319: step 7669, loss 0.0138122, acc 0.992188\n",
      "2017-01-11T01:31:03.866251: step 7670, loss 0.00814054, acc 1\n",
      "2017-01-11T01:31:05.911724: step 7671, loss 0.0195197, acc 0.992188\n",
      "2017-01-11T01:31:07.951667: step 7672, loss 0.0254083, acc 0.984375\n",
      "2017-01-11T01:31:10.272523: step 7673, loss 0.0394332, acc 0.992188\n",
      "2017-01-11T01:31:12.290557: step 7674, loss 0.0158636, acc 1\n",
      "2017-01-11T01:31:14.352495: step 7675, loss 0.0777293, acc 0.984375\n",
      "2017-01-11T01:31:16.399864: step 7676, loss 0.076253, acc 0.976562\n",
      "2017-01-11T01:31:19.344558: step 7677, loss 0.017994, acc 0.992188\n",
      "2017-01-11T01:31:21.634393: step 7678, loss 0.0179083, acc 0.992188\n",
      "2017-01-11T01:31:23.648653: step 7679, loss 0.00263337, acc 1\n",
      "2017-01-11T01:31:25.649832: step 7680, loss 0.0296762, acc 0.992188\n",
      "2017-01-11T01:31:27.692341: step 7681, loss 0.0552723, acc 0.976562\n",
      "2017-01-11T01:31:29.694566: step 7682, loss 0.0119394, acc 1\n",
      "2017-01-11T01:31:31.695019: step 7683, loss 0.0187366, acc 1\n",
      "2017-01-11T01:31:33.756214: step 7684, loss 0.00340654, acc 1\n",
      "2017-01-11T01:31:35.810654: step 7685, loss 0.0122687, acc 0.992188\n",
      "2017-01-11T01:31:37.971128: step 7686, loss 0.0030106, acc 1\n",
      "2017-01-11T01:31:39.986404: step 7687, loss 0.00959948, acc 1\n",
      "2017-01-11T01:31:42.347849: step 7688, loss 0.0340089, acc 1\n",
      "2017-01-11T01:31:44.364249: step 7689, loss 0.0583455, acc 0.984375\n",
      "2017-01-11T01:31:46.430358: step 7690, loss 0.048547, acc 0.984375\n",
      "2017-01-11T01:31:48.470039: step 7691, loss 0.00322136, acc 1\n",
      "2017-01-11T01:31:50.551582: step 7692, loss 0.0587407, acc 0.984375\n",
      "2017-01-11T01:31:52.591668: step 7693, loss 0.0162416, acc 0.992188\n",
      "2017-01-11T01:31:54.632080: step 7694, loss 0.0790509, acc 0.976562\n",
      "2017-01-11T01:31:56.687416: step 7695, loss 0.0135193, acc 0.992188\n",
      "2017-01-11T01:31:58.674173: step 7696, loss 0.0485206, acc 0.976562\n",
      "2017-01-11T01:32:00.711553: step 7697, loss 4.72838e-05, acc 1\n",
      "2017-01-11T01:32:02.746405: step 7698, loss 0.0207737, acc 0.992188\n",
      "2017-01-11T01:32:04.807488: step 7699, loss 0.0330196, acc 0.992188\n",
      "2017-01-11T01:32:06.841202: step 7700, loss 0.0286355, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:32:36.829692: step 7700, loss 0.0683491, acc 0.98232\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7700\n",
      "\n",
      "2017-01-11T01:32:41.395709: step 7701, loss 0.00173317, acc 1\n",
      "2017-01-11T01:32:43.467958: step 7702, loss 0.0044937, acc 1\n",
      "2017-01-11T01:32:45.692961: step 7703, loss 0.0156059, acc 0.992188\n",
      "2017-01-11T01:32:48.037044: step 7704, loss 0.0263389, acc 0.992188\n",
      "2017-01-11T01:32:50.151101: step 7705, loss 0.0527032, acc 0.96875\n",
      "2017-01-11T01:32:52.290813: step 7706, loss 0.00526844, acc 1\n",
      "2017-01-11T01:32:54.400249: step 7707, loss 0.0658952, acc 0.984375\n",
      "2017-01-11T01:32:56.528936: step 7708, loss 0.0632495, acc 0.984375\n",
      "2017-01-11T01:32:58.543310: step 7709, loss 0.0331754, acc 0.992188\n",
      "2017-01-11T01:33:00.590097: step 7710, loss 0.0505841, acc 0.984375\n",
      "2017-01-11T01:33:02.617339: step 7711, loss 0.0117366, acc 1\n",
      "2017-01-11T01:33:04.716663: step 7712, loss 0.0419443, acc 0.984375\n",
      "2017-01-11T01:33:06.719225: step 7713, loss 0.0132781, acc 1\n",
      "2017-01-11T01:33:08.747921: step 7714, loss 0.154789, acc 0.96875\n",
      "2017-01-11T01:33:10.757614: step 7715, loss 0.00780581, acc 1\n",
      "2017-01-11T01:33:12.791703: step 7716, loss 0.0792915, acc 0.984375\n",
      "2017-01-11T01:33:14.836844: step 7717, loss 0.023579, acc 0.992188\n",
      "2017-01-11T01:33:16.890507: step 7718, loss 0.0290156, acc 0.984375\n",
      "2017-01-11T01:33:19.259491: step 7719, loss 0.0055617, acc 1\n",
      "2017-01-11T01:33:21.340509: step 7720, loss 0.0135606, acc 0.992188\n",
      "2017-01-11T01:33:23.390451: step 7721, loss 0.00472765, acc 1\n",
      "2017-01-11T01:33:25.444887: step 7722, loss 0.0432253, acc 0.984375\n",
      "2017-01-11T01:33:27.452141: step 7723, loss 0.010164, acc 1\n",
      "2017-01-11T01:33:29.471285: step 7724, loss 0.00746436, acc 0.992188\n",
      "2017-01-11T01:33:31.494366: step 7725, loss 0.000545777, acc 1\n",
      "2017-01-11T01:33:33.559930: step 7726, loss 0.0424233, acc 0.992188\n",
      "2017-01-11T01:33:35.615713: step 7727, loss 0.00267919, acc 1\n",
      "2017-01-11T01:33:37.741010: step 7728, loss 0.0523077, acc 0.984375\n",
      "2017-01-11T01:33:39.779510: step 7729, loss 0.0175659, acc 0.992188\n",
      "2017-01-11T01:33:41.819694: step 7730, loss 0.0117948, acc 1\n",
      "2017-01-11T01:33:43.852953: step 7731, loss 0.00390147, acc 1\n",
      "2017-01-11T01:33:45.872876: step 7732, loss 0.131573, acc 0.984375\n",
      "2017-01-11T01:33:47.907400: step 7733, loss 0.00403212, acc 1\n",
      "2017-01-11T01:33:50.154459: step 7734, loss 0.0677006, acc 0.984375\n",
      "2017-01-11T01:33:52.354157: step 7735, loss 0.0521808, acc 0.992188\n",
      "2017-01-11T01:33:54.398973: step 7736, loss 0.0323481, acc 0.992188\n",
      "2017-01-11T01:33:56.467670: step 7737, loss 0.0331544, acc 0.992188\n",
      "2017-01-11T01:33:58.528103: step 7738, loss 0.000993599, acc 1\n",
      "2017-01-11T01:34:00.563015: step 7739, loss 0.0810939, acc 0.976562\n",
      "2017-01-11T01:34:02.590024: step 7740, loss 0.0601286, acc 0.976562\n",
      "2017-01-11T01:34:04.623367: step 7741, loss 0.0551607, acc 0.984375\n",
      "2017-01-11T01:34:06.649376: step 7742, loss 0.0260633, acc 0.992188\n",
      "2017-01-11T01:34:08.692212: step 7743, loss 0.0155654, acc 0.992188\n",
      "2017-01-11T01:34:10.738764: step 7744, loss 0.0299634, acc 0.992188\n",
      "2017-01-11T01:34:12.754251: step 7745, loss 0.0548875, acc 0.976562\n",
      "2017-01-11T01:34:14.823249: step 7746, loss 0.0179134, acc 0.992188\n",
      "2017-01-11T01:34:16.896648: step 7747, loss 0.0582488, acc 0.976562\n",
      "2017-01-11T01:34:18.955985: step 7748, loss 0.0236833, acc 0.992188\n",
      "2017-01-11T01:34:21.086599: step 7749, loss 0.0484001, acc 0.984375\n",
      "2017-01-11T01:34:23.448861: step 7750, loss 0.0345051, acc 0.984375\n",
      "2017-01-11T01:34:25.466621: step 7751, loss 0.0264389, acc 0.992188\n",
      "2017-01-11T01:34:27.516208: step 7752, loss 0.0328464, acc 0.992188\n",
      "2017-01-11T01:34:29.546021: step 7753, loss 0.0225866, acc 0.992188\n",
      "2017-01-11T01:34:31.678030: step 7754, loss 4.325e-05, acc 1\n",
      "2017-01-11T01:34:33.693886: step 7755, loss 0.0118269, acc 1\n",
      "2017-01-11T01:34:35.749118: step 7756, loss 0.0389816, acc 0.984375\n",
      "2017-01-11T01:34:37.856529: step 7757, loss 0.00735822, acc 1\n",
      "2017-01-11T01:34:39.845081: step 7758, loss 0.0144967, acc 0.992188\n",
      "2017-01-11T01:34:41.881419: step 7759, loss 0.0539916, acc 0.976562\n",
      "2017-01-11T01:34:43.912288: step 7760, loss 0.0051282, acc 1\n",
      "2017-01-11T01:34:45.960014: step 7761, loss 0.0418136, acc 0.992188\n",
      "2017-01-11T01:34:47.975645: step 7762, loss 0.044594, acc 0.984375\n",
      "2017-01-11T01:34:50.046881: step 7763, loss 0.00605517, acc 1\n",
      "2017-01-11T01:34:52.106440: step 7764, loss 0.0544274, acc 0.984375\n",
      "2017-01-11T01:34:54.279104: step 7765, loss 0.0247022, acc 0.992188\n",
      "2017-01-11T01:34:56.474334: step 7766, loss 0.0366496, acc 0.984375\n",
      "2017-01-11T01:34:58.489068: step 7767, loss 0.0125034, acc 1\n",
      "2017-01-11T01:35:00.541574: step 7768, loss 0.0344324, acc 0.984375\n",
      "2017-01-11T01:35:02.603302: step 7769, loss 0.111044, acc 0.984375\n",
      "2017-01-11T01:35:04.882099: step 7770, loss 0.0274329, acc 0.992188\n",
      "2017-01-11T01:35:06.934777: step 7771, loss 0.0212609, acc 0.992188\n",
      "2017-01-11T01:35:08.934164: step 7772, loss 0.012172, acc 0.992188\n",
      "2017-01-11T01:35:10.977477: step 7773, loss 0.013996, acc 0.992188\n",
      "2017-01-11T01:35:13.005570: step 7774, loss 0.000771862, acc 1\n",
      "2017-01-11T01:35:15.063245: step 7775, loss 0.0283288, acc 0.992188\n",
      "2017-01-11T01:35:17.176553: step 7776, loss 0.00670705, acc 0.992188\n",
      "2017-01-11T01:35:19.229327: step 7777, loss 0.0339273, acc 0.992188\n",
      "2017-01-11T01:35:21.308378: step 7778, loss 0.0941356, acc 0.976562\n",
      "2017-01-11T01:35:23.333122: step 7779, loss 0.0402223, acc 0.992188\n",
      "2017-01-11T01:35:26.524440: step 7780, loss 0.0150132, acc 0.992188\n",
      "2017-01-11T01:35:28.888657: step 7781, loss 0.00265602, acc 1\n",
      "2017-01-11T01:35:30.943983: step 7782, loss 0.0918782, acc 0.976562\n",
      "2017-01-11T01:35:32.969026: step 7783, loss 0.0365349, acc 0.992188\n",
      "2017-01-11T01:35:35.064488: step 7784, loss 0.00838554, acc 1\n",
      "2017-01-11T01:35:37.115729: step 7785, loss 0.059263, acc 0.976562\n",
      "2017-01-11T01:35:39.227275: step 7786, loss 0.0460803, acc 0.984375\n",
      "2017-01-11T01:35:41.284811: step 7787, loss 0.0507077, acc 0.976562\n",
      "2017-01-11T01:35:43.324089: step 7788, loss 0.0181685, acc 0.984375\n",
      "2017-01-11T01:35:45.357003: step 7789, loss 0.000528875, acc 1\n",
      "2017-01-11T01:35:47.396318: step 7790, loss 0.068544, acc 0.96875\n",
      "2017-01-11T01:35:49.477350: step 7791, loss 0.00232345, acc 1\n",
      "2017-01-11T01:35:51.508015: step 7792, loss 0.0585806, acc 0.984375\n",
      "2017-01-11T01:35:53.569885: step 7793, loss 0.0141684, acc 0.992188\n",
      "2017-01-11T01:35:55.599619: step 7794, loss 0.0455088, acc 0.984375\n",
      "2017-01-11T01:35:57.642057: step 7795, loss 0.00124551, acc 1\n",
      "2017-01-11T01:36:00.000372: step 7796, loss 0.013801, acc 0.992188\n",
      "2017-01-11T01:36:02.025841: step 7797, loss 0.0523448, acc 0.984375\n",
      "2017-01-11T01:36:04.084557: step 7798, loss 0.0378288, acc 0.984375\n",
      "2017-01-11T01:36:06.151381: step 7799, loss 0.0436069, acc 0.992188\n",
      "2017-01-11T01:36:08.152816: step 7800, loss 0.00222473, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:36:46.007133: step 7800, loss 0.0662952, acc 0.98304\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7800\n",
      "\n",
      "2017-01-11T01:36:50.899882: step 7801, loss 0.0295669, acc 0.992188\n",
      "2017-01-11T01:36:52.967529: step 7802, loss 0.0388984, acc 0.984375\n",
      "2017-01-11T01:36:55.010043: step 7803, loss 0.0022676, acc 1\n",
      "2017-01-11T01:36:57.107657: step 7804, loss 0.0295345, acc 0.992188\n",
      "2017-01-11T01:36:59.162790: step 7805, loss 0.0458156, acc 0.976562\n",
      "2017-01-11T01:37:01.316083: step 7806, loss 0.0619325, acc 0.976562\n",
      "2017-01-11T01:37:03.445586: step 7807, loss 0.0261789, acc 0.992188\n",
      "2017-01-11T01:37:05.537603: step 7808, loss 0.0269072, acc 0.984375\n",
      "2017-01-11T01:37:07.565053: step 7809, loss 0.0291146, acc 0.992188\n",
      "2017-01-11T01:37:09.631222: step 7810, loss 0.0121259, acc 1\n",
      "2017-01-11T01:37:12.019895: step 7811, loss 0.0311189, acc 0.992188\n",
      "2017-01-11T01:37:14.050989: step 7812, loss 0.0267957, acc 0.992188\n",
      "2017-01-11T01:37:16.104525: step 7813, loss 0.0280042, acc 0.992188\n",
      "2017-01-11T01:37:18.166589: step 7814, loss 0.0173277, acc 0.992188\n",
      "2017-01-11T01:37:20.245028: step 7815, loss 0.0127678, acc 1\n",
      "2017-01-11T01:37:22.335356: step 7816, loss 0.00133848, acc 1\n",
      "2017-01-11T01:37:24.323893: step 7817, loss 0.0372798, acc 0.992188\n",
      "2017-01-11T01:37:26.331683: step 7818, loss 0.0111406, acc 1\n",
      "2017-01-11T01:37:28.363679: step 7819, loss 0.0200305, acc 0.992188\n",
      "2017-01-11T01:37:30.391097: step 7820, loss 0.0256579, acc 0.992188\n",
      "2017-01-11T01:37:32.431144: step 7821, loss 0.0659345, acc 0.984375\n",
      "2017-01-11T01:37:34.500320: step 7822, loss 0.0153496, acc 1\n",
      "2017-01-11T01:37:36.558643: step 7823, loss 0.0368796, acc 0.992188\n",
      "2017-01-11T01:37:38.606121: step 7824, loss 0.000683981, acc 1\n",
      "2017-01-11T01:37:40.744562: step 7825, loss 0.022439, acc 0.992188\n",
      "2017-01-11T01:37:42.923226: step 7826, loss 0.0459884, acc 0.984375\n",
      "2017-01-11T01:37:45.087569: step 7827, loss 0.0318402, acc 0.984375\n",
      "2017-01-11T01:37:47.077564: step 7828, loss 0.000100347, acc 1\n",
      "2017-01-11T01:37:49.165150: step 7829, loss 0.0179741, acc 0.992188\n",
      "2017-01-11T01:37:51.279114: step 7830, loss 0.0229042, acc 0.992188\n",
      "2017-01-11T01:37:53.300272: step 7831, loss 0.0258264, acc 0.992188\n",
      "2017-01-11T01:37:55.365180: step 7832, loss 0.0776836, acc 0.976562\n",
      "2017-01-11T01:37:57.400201: step 7833, loss 0.0298403, acc 0.992188\n",
      "2017-01-11T01:37:59.451277: step 7834, loss 0.00400362, acc 1\n",
      "2017-01-11T01:38:01.494347: step 7835, loss 0.0267387, acc 0.992188\n",
      "2017-01-11T01:38:03.624883: step 7836, loss 0.0455917, acc 0.984375\n",
      "2017-01-11T01:38:05.688466: step 7837, loss 0.049267, acc 0.984375\n",
      "2017-01-11T01:38:07.724940: step 7838, loss 0.0265962, acc 0.992188\n",
      "2017-01-11T01:38:09.782136: step 7839, loss 0.0716714, acc 0.976562\n",
      "2017-01-11T01:38:11.818001: step 7840, loss 0.00383195, acc 1\n",
      "2017-01-11T01:38:13.851745: step 7841, loss 0.00831241, acc 1\n",
      "2017-01-11T01:38:16.181944: step 7842, loss 0.000191416, acc 1\n",
      "2017-01-11T01:38:18.234788: step 7843, loss 0.114347, acc 0.992188\n",
      "2017-01-11T01:38:20.322028: step 7844, loss 0.00121505, acc 1\n",
      "2017-01-11T01:38:22.415437: step 7845, loss 0.0351098, acc 0.984375\n",
      "2017-01-11T01:38:24.468461: step 7846, loss 0.00665881, acc 0.992188\n",
      "2017-01-11T01:38:26.501943: step 7847, loss 0.00506925, acc 1\n",
      "2017-01-11T01:38:28.492649: step 7848, loss 0.0166567, acc 0.992188\n",
      "2017-01-11T01:38:30.624671: step 7849, loss 0.0504625, acc 0.984375\n",
      "2017-01-11T01:38:32.654071: step 7850, loss 0.0302639, acc 0.992188\n",
      "2017-01-11T01:38:34.720463: step 7851, loss 0.0671368, acc 0.976562\n",
      "2017-01-11T01:38:36.732308: step 7852, loss 0.181471, acc 0.992188\n",
      "2017-01-11T01:38:38.783096: step 7853, loss 0.0318258, acc 0.984375\n",
      "2017-01-11T01:38:40.931484: step 7854, loss 0.0289585, acc 0.992188\n",
      "2017-01-11T01:38:43.280947: step 7855, loss 0.000856601, acc 1\n",
      "2017-01-11T01:38:45.303522: step 7856, loss 0.0540787, acc 0.984375\n",
      "2017-01-11T01:38:47.538716: step 7857, loss 0.0224485, acc 1\n",
      "2017-01-11T01:38:49.713172: step 7858, loss 0.045125, acc 0.992188\n",
      "2017-01-11T01:38:51.826540: step 7859, loss 0.00304026, acc 1\n",
      "2017-01-11T01:38:53.870008: step 7860, loss 0.0595632, acc 0.976562\n",
      "2017-01-11T01:38:55.901527: step 7861, loss 0.0280217, acc 0.984375\n",
      "2017-01-11T01:38:57.899277: step 7862, loss 0.00135218, acc 1\n",
      "2017-01-11T01:38:59.970606: step 7863, loss 0.00688651, acc 1\n",
      "2017-01-11T01:39:02.044367: step 7864, loss 0.00199465, acc 1\n",
      "2017-01-11T01:39:04.134768: step 7865, loss 0.0797496, acc 0.96875\n",
      "2017-01-11T01:39:06.301064: step 7866, loss 0.00332407, acc 1\n",
      "2017-01-11T01:39:08.409244: step 7867, loss 0.147283, acc 0.945312\n",
      "2017-01-11T01:39:10.469436: step 7868, loss 0.0176721, acc 0.992188\n",
      "2017-01-11T01:39:12.504060: step 7869, loss 0.0499294, acc 0.976562\n",
      "2017-01-11T01:39:14.553409: step 7870, loss 0.0167694, acc 1\n",
      "2017-01-11T01:39:16.581772: step 7871, loss 0.00972348, acc 1\n",
      "2017-01-11T01:39:18.630191: step 7872, loss 0.0217826, acc 0.992188\n",
      "2017-01-11T01:39:20.994457: step 7873, loss 0.0693688, acc 0.976562\n",
      "2017-01-11T01:39:23.087202: step 7874, loss 0.0168623, acc 1\n",
      "2017-01-11T01:39:25.113802: step 7875, loss 0.00394433, acc 1\n",
      "2017-01-11T01:39:27.145516: step 7876, loss 0.0133715, acc 0.992188\n",
      "2017-01-11T01:39:29.153431: step 7877, loss 0.0300261, acc 0.984375\n",
      "2017-01-11T01:39:31.170466: step 7878, loss 0.0327088, acc 0.976562\n",
      "2017-01-11T01:39:33.233768: step 7879, loss 0.0583874, acc 0.984375\n",
      "2017-01-11T01:39:35.281680: step 7880, loss 0.0277676, acc 0.984375\n",
      "2017-01-11T01:39:37.286915: step 7881, loss 0.00806089, acc 1\n",
      "2017-01-11T01:39:39.342060: step 7882, loss 0.0907775, acc 0.976562\n",
      "2017-01-11T01:39:41.483172: step 7883, loss 0.0422053, acc 0.984375\n",
      "2017-01-11T01:39:43.538768: step 7884, loss 0.0345482, acc 0.984375\n",
      "2017-01-11T01:39:45.578305: step 7885, loss 0.0281535, acc 0.992188\n",
      "2017-01-11T01:39:47.606240: step 7886, loss 0.0121622, acc 1\n",
      "2017-01-11T01:39:49.658355: step 7887, loss 0.0230555, acc 1\n",
      "2017-01-11T01:39:51.941882: step 7888, loss 0.0628473, acc 0.984375\n",
      "2017-01-11T01:39:54.011658: step 7889, loss 0.0279084, acc 0.984375\n",
      "2017-01-11T01:39:56.044376: step 7890, loss 0.000193075, acc 1\n",
      "2017-01-11T01:39:58.074966: step 7891, loss 0.029704, acc 0.992188\n",
      "2017-01-11T01:40:00.093501: step 7892, loss 0.0600461, acc 0.976562\n",
      "2017-01-11T01:40:02.126542: step 7893, loss 0.0065934, acc 1\n",
      "2017-01-11T01:40:04.145095: step 7894, loss 0.0121531, acc 0.992188\n",
      "2017-01-11T01:40:06.223437: step 7895, loss 0.0264693, acc 0.992188\n",
      "2017-01-11T01:40:08.241688: step 7896, loss 0.0300288, acc 0.992188\n",
      "2017-01-11T01:40:10.281448: step 7897, loss 0.0280102, acc 0.992188\n",
      "2017-01-11T01:40:12.325596: step 7898, loss 0.0110579, acc 1\n",
      "2017-01-11T01:40:14.501796: step 7899, loss 0.0141449, acc 0.992188\n",
      "2017-01-11T01:40:16.619617: step 7900, loss 0.0279541, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:40:44.980673: step 7900, loss 0.0697997, acc 0.98272\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-7900\n",
      "\n",
      "2017-01-11T01:40:50.566931: step 7901, loss 0.0159021, acc 0.992188\n",
      "2017-01-11T01:40:52.652169: step 7902, loss 0.0375222, acc 0.984375\n",
      "2017-01-11T01:40:54.686661: step 7903, loss 0.0230891, acc 0.992188\n",
      "2017-01-11T01:40:57.115339: step 7904, loss 0.0211223, acc 0.992188\n",
      "2017-01-11T01:40:59.267928: step 7905, loss 0.0175493, acc 1\n",
      "2017-01-11T01:41:01.291834: step 7906, loss 0.00790186, acc 1\n",
      "2017-01-11T01:41:03.316946: step 7907, loss 0.0364018, acc 0.984375\n",
      "2017-01-11T01:41:05.438901: step 7908, loss 0.00925689, acc 0.992188\n",
      "2017-01-11T01:41:07.492845: step 7909, loss 0.00135452, acc 1\n",
      "2017-01-11T01:41:09.527017: step 7910, loss 0.0547518, acc 0.984375\n",
      "2017-01-11T01:41:11.563536: step 7911, loss 0.00606508, acc 1\n",
      "2017-01-11T01:41:13.608516: step 7912, loss 0.0233334, acc 0.992188\n",
      "2017-01-11T01:41:15.651260: step 7913, loss 0.0162187, acc 0.992188\n",
      "2017-01-11T01:41:17.689072: step 7914, loss 0.0104268, acc 1\n",
      "2017-01-11T01:41:19.753063: step 7915, loss 0.0453338, acc 0.984375\n",
      "2017-01-11T01:41:22.462996: step 7916, loss 0.000804613, acc 1\n",
      "2017-01-11T01:41:24.509290: step 7917, loss 0.101572, acc 0.976562\n",
      "2017-01-11T01:41:26.525571: step 7918, loss 0.00074598, acc 1\n",
      "2017-01-11T01:41:28.622254: step 7919, loss 0.0017755, acc 1\n",
      "2017-01-11T01:41:30.911270: step 7920, loss 0.264438, acc 0.984375\n",
      "2017-01-11T01:41:32.998086: step 7921, loss 0.000155508, acc 1\n",
      "2017-01-11T01:41:35.088710: step 7922, loss 0.00768734, acc 0.992188\n",
      "2017-01-11T01:41:37.892027: step 7923, loss 0.0688147, acc 0.96875\n",
      "2017-01-11T01:41:40.919590: step 7924, loss 0.030995, acc 0.992188\n",
      "2017-01-11T01:41:43.490622: step 7925, loss 0.0431009, acc 0.984375\n",
      "2017-01-11T01:41:45.555228: step 7926, loss 0.00304212, acc 1\n",
      "2017-01-11T01:41:47.563570: step 7927, loss 0.0194172, acc 0.992188\n",
      "2017-01-11T01:41:49.620580: step 7928, loss 0.0193156, acc 1\n",
      "2017-01-11T01:41:51.712151: step 7929, loss 0.000570275, acc 1\n",
      "2017-01-11T01:41:53.747747: step 7930, loss 0.0270979, acc 0.992188\n",
      "2017-01-11T01:41:55.806408: step 7931, loss 0.0131696, acc 0.992188\n",
      "2017-01-11T01:41:57.829273: step 7932, loss 0.00498041, acc 1\n",
      "2017-01-11T01:41:59.863940: step 7933, loss 0.0484567, acc 0.984375\n",
      "2017-01-11T01:42:02.261413: step 7934, loss 0.000977557, acc 1\n",
      "2017-01-11T01:42:04.300232: step 7935, loss 0.00145091, acc 1\n",
      "2017-01-11T01:42:06.350464: step 7936, loss 0.02482, acc 0.992188\n",
      "2017-01-11T01:42:08.379600: step 7937, loss 0.0400088, acc 0.984375\n",
      "2017-01-11T01:42:10.435904: step 7938, loss 0.0267986, acc 0.992188\n",
      "2017-01-11T01:42:12.494519: step 7939, loss 0.0675222, acc 0.992188\n",
      "2017-01-11T01:42:14.539047: step 7940, loss 0.00772157, acc 1\n",
      "2017-01-11T01:42:16.561190: step 7941, loss 0.00317215, acc 1\n",
      "2017-01-11T01:42:18.584489: step 7942, loss 0.01768, acc 0.992188\n",
      "2017-01-11T01:42:20.660297: step 7943, loss 0.00692006, acc 0.992188\n",
      "2017-01-11T01:42:22.760912: step 7944, loss 0.00318315, acc 1\n",
      "2017-01-11T01:42:24.808373: step 7945, loss 0.0438583, acc 0.984375\n",
      "2017-01-11T01:42:26.827890: step 7946, loss 0.0157098, acc 0.984375\n",
      "2017-01-11T01:42:28.826015: step 7947, loss 0.044215, acc 0.984375\n",
      "2017-01-11T01:42:30.949231: step 7948, loss 0.0269908, acc 0.984375\n",
      "2017-01-11T01:42:33.205059: step 7949, loss 0.0292841, acc 0.992188\n",
      "2017-01-11T01:42:35.346112: step 7950, loss 0.00711195, acc 1\n",
      "2017-01-11T01:42:37.398792: step 7951, loss 0.0554442, acc 0.976562\n",
      "2017-01-11T01:42:39.449557: step 7952, loss 0.00867081, acc 1\n",
      "2017-01-11T01:42:41.491584: step 7953, loss 0.0162828, acc 0.992188\n",
      "2017-01-11T01:42:43.654245: step 7954, loss 0.0623246, acc 0.976562\n",
      "2017-01-11T01:42:45.702870: step 7955, loss 0.095339, acc 0.976562\n",
      "2017-01-11T01:42:47.728833: step 7956, loss 0.0017587, acc 1\n",
      "2017-01-11T01:42:49.724039: step 7957, loss 0.0506524, acc 0.976562\n",
      "2017-01-11T01:42:51.819613: step 7958, loss 0.0638969, acc 0.976562\n",
      "2017-01-11T01:42:53.867765: step 7959, loss 0.0175765, acc 0.992188\n",
      "2017-01-11T01:42:55.887632: step 7960, loss 0.017372, acc 0.992188\n",
      "2017-01-11T01:42:57.914631: step 7961, loss 0.0256833, acc 0.992188\n",
      "2017-01-11T01:42:59.996415: step 7962, loss 0.00916035, acc 1\n",
      "2017-01-11T01:43:02.040974: step 7963, loss 0.000803121, acc 1\n",
      "2017-01-11T01:43:04.111837: step 7964, loss 0.0210488, acc 1\n",
      "2017-01-11T01:43:06.484601: step 7965, loss 0.0203409, acc 1\n",
      "2017-01-11T01:43:08.513424: step 7966, loss 0.0514697, acc 0.984375\n",
      "2017-01-11T01:43:10.565870: step 7967, loss 0.061294, acc 0.984375\n",
      "2017-01-11T01:43:12.571376: step 7968, loss 0.000463801, acc 1\n",
      "2017-01-11T01:43:14.627634: step 7969, loss 0.064785, acc 0.96875\n",
      "2017-01-11T01:43:16.690162: step 7970, loss 0.0448833, acc 0.984375\n",
      "2017-01-11T01:43:18.684444: step 7971, loss 0.047059, acc 0.984375\n",
      "2017-01-11T01:43:20.740277: step 7972, loss 0.0419007, acc 0.984375\n",
      "2017-01-11T01:43:22.835736: step 7973, loss 0.00270728, acc 1\n",
      "2017-01-11T01:43:24.876214: step 7974, loss 0.0580865, acc 0.976562\n",
      "2017-01-11T01:43:26.896426: step 7975, loss 0.0505975, acc 0.984375\n",
      "2017-01-11T01:43:28.966153: step 7976, loss 0.0371262, acc 0.984375\n",
      "2017-01-11T01:43:30.976872: step 7977, loss 0.0301911, acc 0.992188\n",
      "2017-01-11T01:43:33.016061: step 7978, loss 0.0325165, acc 0.992188\n",
      "2017-01-11T01:43:35.093015: step 7979, loss 0.0585788, acc 0.976562\n",
      "2017-01-11T01:43:37.278484: step 7980, loss 0.0091129, acc 1\n",
      "2017-01-11T01:43:39.446515: step 7981, loss 0.00729263, acc 1\n",
      "2017-01-11T01:43:41.463453: step 7982, loss 0.0275762, acc 0.992188\n",
      "2017-01-11T01:43:43.611538: step 7983, loss 0.00717366, acc 0.992188\n",
      "2017-01-11T01:43:45.645601: step 7984, loss 0.0305441, acc 0.992188\n",
      "2017-01-11T01:43:47.687144: step 7985, loss 0.0322847, acc 0.992188\n",
      "2017-01-11T01:43:49.719218: step 7986, loss 0.000693207, acc 1\n",
      "2017-01-11T01:43:51.774484: step 7987, loss 0.0450228, acc 0.984375\n",
      "2017-01-11T01:43:53.821796: step 7988, loss 0.0408613, acc 0.992188\n",
      "2017-01-11T01:43:55.862237: step 7989, loss 0.0124825, acc 0.992188\n",
      "2017-01-11T01:48:53.806113: step 8102, loss 0.00356846, acc 1\n",
      "2017-01-11T01:48:55.851090: step 8103, loss 0.0394088, acc 0.992188\n",
      "2017-01-11T01:48:57.836045: step 8104, loss 0.000186916, acc 1\n",
      "2017-01-11T01:48:59.885652: step 8105, loss 0.033534, acc 0.992188\n",
      "2017-01-11T01:49:02.251781: step 8106, loss 0.0729192, acc 0.976562\n",
      "2017-01-11T01:49:04.471933: step 8107, loss 0.0191199, acc 0.992188\n",
      "2017-01-11T01:49:06.502330: step 8108, loss 0.0393923, acc 0.992188\n",
      "2017-01-11T01:49:08.526246: step 8109, loss 0.0430013, acc 1\n",
      "2017-01-11T01:49:10.563314: step 8110, loss 0.023104, acc 0.992188\n",
      "2017-01-11T01:49:12.594509: step 8111, loss 0.000875736, acc 1\n",
      "2017-01-11T01:49:14.606343: step 8112, loss 0.0178685, acc 0.992188\n",
      "2017-01-11T01:49:16.643802: step 8113, loss 0.0205364, acc 0.992188\n",
      "2017-01-11T01:49:18.682309: step 8114, loss 0.000288569, acc 1\n",
      "2017-01-11T01:49:20.738065: step 8115, loss 0.00319937, acc 1\n",
      "2017-01-11T01:49:22.816537: step 8116, loss 0.0347005, acc 0.984375\n",
      "2017-01-11T01:49:24.882666: step 8117, loss 0.0412635, acc 0.984375\n",
      "2017-01-11T01:49:26.905004: step 8118, loss 0.0181041, acc 0.992188\n",
      "2017-01-11T01:49:28.940413: step 8119, loss 0.0098389, acc 1\n",
      "2017-01-11T01:49:30.958607: step 8120, loss 0.0119361, acc 0.992188\n",
      "2017-01-11T01:49:32.999908: step 8121, loss 0.0369102, acc 0.992188\n",
      "2017-01-11T01:49:35.419921: step 8122, loss 0.000641184, acc 1\n",
      "2017-01-11T01:49:37.487447: step 8123, loss 0.0326862, acc 0.984375\n",
      "2017-01-11T01:49:39.792473: step 8124, loss 0.0104942, acc 0.992188\n",
      "2017-01-11T01:49:41.823348: step 8125, loss 0.0225366, acc 0.992188\n",
      "2017-01-11T01:49:43.852493: step 8126, loss 0.123648, acc 0.992188\n",
      "2017-01-11T01:49:45.964696: step 8127, loss 0.0295267, acc 0.992188\n",
      "2017-01-11T01:49:47.971849: step 8128, loss 0.0330059, acc 0.984375\n",
      "2017-01-11T01:49:50.023367: step 8129, loss 4.59151e-05, acc 1\n",
      "2017-01-11T01:49:52.100476: step 8130, loss 0.000402326, acc 1\n",
      "2017-01-11T01:49:54.158224: step 8131, loss 0.0336266, acc 0.984375\n",
      "2017-01-11T01:49:56.196915: step 8132, loss 0.0208116, acc 1\n",
      "2017-01-11T01:49:58.230765: step 8133, loss 0.00054055, acc 1\n",
      "2017-01-11T01:50:00.251500: step 8134, loss 0.0385826, acc 0.992188\n",
      "2017-01-11T01:50:02.295131: step 8135, loss 0.0391161, acc 0.984375\n",
      "2017-01-11T01:50:04.352436: step 8136, loss 0.017588, acc 0.992188\n",
      "2017-01-11T01:50:06.638006: step 8137, loss 0.0387105, acc 0.984375\n",
      "2017-01-11T01:50:08.757041: step 8138, loss 0.00341113, acc 1\n",
      "2017-01-11T01:50:10.806613: step 8139, loss 0.00379761, acc 1\n",
      "2017-01-11T01:50:12.840655: step 8140, loss 0.0162026, acc 0.992188\n",
      "2017-01-11T01:50:14.864586: step 8141, loss 0.0115543, acc 1\n",
      "2017-01-11T01:50:16.837552: step 8142, loss 0.000114939, acc 1\n",
      "2017-01-11T01:50:18.890501: step 8143, loss 0.00976675, acc 1\n",
      "2017-01-11T01:50:21.011981: step 8144, loss 0.021927, acc 0.992188\n",
      "2017-01-11T01:50:23.134040: step 8145, loss 0.0039837, acc 1\n",
      "2017-01-11T01:50:25.177216: step 8146, loss 0.0470145, acc 0.992188\n",
      "2017-01-11T01:50:27.425096: step 8147, loss 0.0250366, acc 0.992188\n",
      "2017-01-11T01:50:29.484673: step 8148, loss 0.00762222, acc 1\n",
      "2017-01-11T01:50:31.612101: step 8149, loss 0.00119401, acc 1\n",
      "2017-01-11T01:50:33.630919: step 8150, loss 0.0403622, acc 0.984375\n",
      "2017-01-11T01:50:35.692763: step 8151, loss 0.103722, acc 0.976562\n",
      "2017-01-11T01:50:37.773009: step 8152, loss 0.0864739, acc 0.984375\n",
      "2017-01-11T01:50:40.120845: step 8153, loss 0.000748074, acc 1\n",
      "2017-01-11T01:50:42.560200: step 8154, loss 0.0909757, acc 0.96875\n",
      "2017-01-11T01:50:45.203553: step 8155, loss 0.00845938, acc 1\n",
      "2017-01-11T01:50:47.352004: step 8156, loss 0.0499175, acc 0.984375\n",
      "2017-01-11T01:50:49.376732: step 8157, loss 0.00383427, acc 1\n",
      "2017-01-11T01:50:51.482751: step 8158, loss 0.0380339, acc 0.992188\n",
      "2017-01-11T01:50:53.563482: step 8159, loss 0.0292612, acc 0.992188\n",
      "2017-01-11T01:50:55.587739: step 8160, loss 0.0345562, acc 1\n",
      "2017-01-11T01:50:57.614671: step 8161, loss 0.00170373, acc 1\n",
      "2017-01-11T01:50:59.665156: step 8162, loss 0.0127169, acc 0.992188\n",
      "2017-01-11T01:51:01.735829: step 8163, loss 0.000832986, acc 1\n",
      "2017-01-11T01:51:03.795729: step 8164, loss 0.0889787, acc 0.976562\n",
      "2017-01-11T01:51:05.918831: step 8165, loss 0.00285107, acc 1\n",
      "2017-01-11T01:51:07.955326: step 8166, loss 0.0293738, acc 0.992188\n",
      "2017-01-11T01:51:09.992649: step 8167, loss 0.0137791, acc 1\n",
      "2017-01-11T01:51:12.340392: step 8168, loss 0.020057, acc 0.992188\n",
      "2017-01-11T01:51:14.375901: step 8169, loss 0.0205262, acc 1\n",
      "2017-01-11T01:51:16.382751: step 8170, loss 0.0159833, acc 0.992188\n",
      "2017-01-11T01:51:18.383783: step 8171, loss 0.0484731, acc 0.992188\n",
      "2017-01-11T01:51:20.458197: step 8172, loss 0.020944, acc 0.992188\n",
      "2017-01-11T01:51:22.527145: step 8173, loss 0.0279514, acc 0.992188\n",
      "2017-01-11T01:51:24.549223: step 8174, loss 0.0137418, acc 0.992188\n",
      "2017-01-11T01:51:26.872894: step 8175, loss 0.0685756, acc 0.960938\n",
      "2017-01-11T01:51:28.896756: step 8176, loss 0.119683, acc 0.984375\n",
      "2017-01-11T01:51:30.916576: step 8177, loss 0.0295693, acc 0.984375\n",
      "2017-01-11T01:51:32.952530: step 8178, loss 0.035057, acc 0.984375\n",
      "2017-01-11T01:51:35.010984: step 8179, loss 0.000492289, acc 1\n",
      "2017-01-11T01:51:37.047860: step 8180, loss 0.0666266, acc 0.976562\n",
      "2017-01-11T01:51:39.096652: step 8181, loss 0.0214128, acc 0.992188\n",
      "2017-01-11T01:51:41.098980: step 8182, loss 0.00560536, acc 1\n",
      "2017-01-11T01:51:44.363300: step 8183, loss 0.0451081, acc 0.984375\n",
      "2017-01-11T01:51:46.652241: step 8184, loss 0.0179725, acc 0.992188\n",
      "2017-01-11T01:51:48.668237: step 8185, loss 0.00212105, acc 1\n",
      "2017-01-11T01:51:50.700899: step 8186, loss 0.000134497, acc 1\n",
      "2017-01-11T01:51:52.798312: step 8187, loss 0.0482489, acc 0.992188\n",
      "2017-01-11T01:51:54.858019: step 8188, loss 0.00219542, acc 1\n",
      "2017-01-11T01:51:56.891308: step 8189, loss 0.02202, acc 0.992188\n",
      "2017-01-11T01:51:58.944538: step 8190, loss 0.022534, acc 0.992188\n",
      "2017-01-11T01:52:00.967392: step 8191, loss 0.0907226, acc 0.984375\n",
      "2017-01-11T01:52:03.021346: step 8192, loss 0.0114935, acc 1\n",
      "2017-01-11T01:52:05.106385: step 8193, loss 0.0107048, acc 0.992188\n",
      "2017-01-11T01:52:07.113360: step 8194, loss 0.010849, acc 1\n",
      "2017-01-11T01:52:09.133502: step 8195, loss 0.00073686, acc 1\n",
      "2017-01-11T01:52:11.178096: step 8196, loss 0.00768564, acc 1\n",
      "2017-01-11T01:52:13.213971: step 8197, loss 0.0344219, acc 0.992188\n",
      "2017-01-11T01:52:15.485138: step 8198, loss 0.0287829, acc 0.992188\n",
      "2017-01-11T01:52:17.585649: step 8199, loss 0.0387874, acc 0.992188\n",
      "2017-01-11T01:52:19.684787: step 8200, loss 0.0344583, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:52:48.218580: step 8200, loss 0.0691421, acc 0.9824\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8200\n",
      "\n",
      "2017-01-11T01:52:52.967228: step 8201, loss 0.0317333, acc 0.984375\n",
      "2017-01-11T01:52:55.098454: step 8202, loss 0.00243092, acc 1\n",
      "2017-01-11T01:52:57.187838: step 8203, loss 0.0209527, acc 0.992188\n",
      "2017-01-11T01:52:59.236413: step 8204, loss 0.0469115, acc 0.992188\n",
      "2017-01-11T01:53:01.242192: step 8205, loss 0.0445611, acc 0.984375\n",
      "2017-01-11T01:53:03.259244: step 8206, loss 0.00979011, acc 1\n",
      "2017-01-11T01:53:05.341906: step 8207, loss 0.00124993, acc 1\n",
      "2017-01-11T01:53:07.415518: step 8208, loss 0.0130287, acc 0.992188\n",
      "2017-01-11T01:53:09.483685: step 8209, loss 0.0541465, acc 0.984375\n",
      "2017-01-11T01:53:11.533450: step 8210, loss 0.0190215, acc 0.992188\n",
      "2017-01-11T01:53:13.568043: step 8211, loss 0.0861844, acc 0.96875\n",
      "2017-01-11T01:53:15.624871: step 8212, loss 0.0135786, acc 0.992188\n",
      "2017-01-11T01:53:17.701286: step 8213, loss 0.0125788, acc 0.992188\n",
      "2017-01-11T01:53:19.781896: step 8214, loss 0.000381621, acc 1\n",
      "2017-01-11T01:53:21.821761: step 8215, loss 0.000916287, acc 1\n",
      "2017-01-11T01:53:24.160675: step 8216, loss 0.0119076, acc 0.992188\n",
      "2017-01-11T01:53:26.191529: step 8217, loss 0.000224348, acc 1\n",
      "2017-01-11T01:53:28.216181: step 8218, loss 0.0187668, acc 0.992188\n",
      "2017-01-11T01:53:30.234117: step 8219, loss 0.000792382, acc 1\n",
      "2017-01-11T01:53:32.260876: step 8220, loss 0.000264905, acc 1\n",
      "2017-01-11T01:53:34.291826: step 8221, loss 0.0132221, acc 0.992188\n",
      "2017-01-11T01:53:36.350338: step 8222, loss 0.0439851, acc 0.992188\n",
      "2017-01-11T01:53:38.398984: step 8223, loss 0.0194392, acc 0.984375\n",
      "2017-01-11T01:53:40.447109: step 8224, loss 0.00846012, acc 1\n",
      "2017-01-11T01:53:42.504393: step 8225, loss 0.0115942, acc 0.992188\n",
      "2017-01-11T01:53:44.546920: step 8226, loss 0.0381783, acc 0.992188\n",
      "2017-01-11T01:53:46.586884: step 8227, loss 0.0132001, acc 0.992188\n",
      "2017-01-11T01:53:48.726063: step 8228, loss 0.0202482, acc 0.984375\n",
      "2017-01-11T01:53:50.768163: step 8229, loss 0.0184729, acc 1\n",
      "2017-01-11T01:53:52.856770: step 8230, loss 0.0293426, acc 0.992188\n",
      "2017-01-11T01:53:55.172399: step 8231, loss 0.000491177, acc 1\n",
      "2017-01-11T01:53:57.289010: step 8232, loss 0.0662118, acc 0.984375\n",
      "2017-01-11T01:53:59.336217: step 8233, loss 0.0409776, acc 0.992188\n",
      "2017-01-11T01:54:01.398567: step 8234, loss 0.0403946, acc 0.984375\n",
      "2017-01-11T01:54:03.452314: step 8235, loss 0.0103166, acc 1\n",
      "2017-01-11T01:54:05.536981: step 8236, loss 0.0397903, acc 0.992188\n",
      "2017-01-11T01:54:07.554827: step 8237, loss 0.0201382, acc 1\n",
      "2017-01-11T01:54:09.617249: step 8238, loss 0.0405558, acc 0.984375\n",
      "2017-01-11T01:54:11.670435: step 8239, loss 0.00812558, acc 0.992188\n",
      "2017-01-11T01:54:13.714818: step 8240, loss 0.00199827, acc 1\n",
      "2017-01-11T01:54:15.768662: step 8241, loss 0.0359364, acc 0.992188\n",
      "2017-01-11T01:54:17.802326: step 8242, loss 0.0747897, acc 0.960938\n",
      "2017-01-11T01:54:19.897834: step 8243, loss 0.0202918, acc 0.992188\n",
      "2017-01-11T01:54:22.005849: step 8244, loss 0.0305762, acc 0.992188\n",
      "2017-01-11T01:54:24.030874: step 8245, loss 0.00641058, acc 1\n",
      "2017-01-11T01:54:26.069652: step 8246, loss 0.000223377, acc 1\n",
      "2017-01-11T01:54:28.452127: step 8247, loss 0.0247991, acc 0.992188\n",
      "2017-01-11T01:54:30.553849: step 8248, loss 0.0315665, acc 0.992188\n",
      "2017-01-11T01:54:32.594652: step 8249, loss 0.0392203, acc 0.976562\n",
      "2017-01-11T01:54:34.680352: step 8250, loss 0.0768022, acc 0.992188\n",
      "2017-01-11T01:54:36.697493: step 8251, loss 0.0199688, acc 0.992188\n",
      "2017-01-11T01:54:38.694632: step 8252, loss 0.0308162, acc 0.984375\n",
      "2017-01-11T01:54:40.726750: step 8253, loss 0.228642, acc 0.96875\n",
      "2017-01-11T01:54:42.773800: step 8254, loss 0.0305841, acc 0.984375\n",
      "2017-01-11T01:54:44.802352: step 8255, loss 0.0117064, acc 1\n",
      "2017-01-11T01:54:46.822171: step 8256, loss 0.0370173, acc 0.992188\n",
      "2017-01-11T01:54:48.955733: step 8257, loss 0.0028488, acc 1\n",
      "2017-01-11T01:54:51.036488: step 8258, loss 0.027069, acc 0.992188\n",
      "2017-01-11T01:54:53.117178: step 8259, loss 0.0680019, acc 0.984375\n",
      "2017-01-11T01:54:55.139641: step 8260, loss 0.0528305, acc 0.976562\n",
      "2017-01-11T01:54:57.165519: step 8261, loss 0.0810131, acc 0.976562\n",
      "2017-01-11T01:54:59.479045: step 8262, loss 0.0138715, acc 1\n",
      "2017-01-11T01:55:01.522986: step 8263, loss 0.0276967, acc 0.984375\n",
      "2017-01-11T01:55:03.586626: step 8264, loss 0.0187453, acc 0.992188\n",
      "2017-01-11T01:55:05.672974: step 8265, loss 0.0363121, acc 0.984375\n",
      "2017-01-11T01:55:07.729559: step 8266, loss 0.017588, acc 0.992188\n",
      "2017-01-11T01:55:09.795163: step 8267, loss 0.0449115, acc 0.992188\n",
      "2017-01-11T01:55:11.815586: step 8268, loss 0.0185356, acc 0.992188\n",
      "2017-01-11T01:55:13.877066: step 8269, loss 0.119503, acc 0.976562\n",
      "2017-01-11T01:55:15.918369: step 8270, loss 0.0207062, acc 0.992188\n",
      "2017-01-11T01:55:17.968963: step 8271, loss 0.0115818, acc 0.992188\n",
      "2017-01-11T01:55:20.022412: step 8272, loss 0.0996721, acc 0.984375\n",
      "2017-01-11T01:55:22.104157: step 8273, loss 0.0259591, acc 0.992188\n",
      "2017-01-11T01:55:24.136789: step 8274, loss 0.0231972, acc 0.992188\n",
      "2017-01-11T01:55:26.192775: step 8275, loss 0.000864846, acc 1\n",
      "2017-01-11T01:55:28.328536: step 8276, loss 0.083714, acc 0.984375\n",
      "2017-01-11T01:55:30.519971: step 8277, loss 0.0187942, acc 0.992188\n",
      "2017-01-11T01:55:32.757115: step 8278, loss 0.0133931, acc 0.992188\n",
      "2017-01-11T01:55:34.823609: step 8279, loss 0.0241058, acc 0.992188\n",
      "2017-01-11T01:55:36.844612: step 8280, loss 0.0292024, acc 0.992188\n",
      "2017-01-11T01:55:38.861809: step 8281, loss 0.0295021, acc 0.984375\n",
      "2017-01-11T01:55:40.911161: step 8282, loss 0.00420281, acc 1\n",
      "2017-01-11T01:55:42.939273: step 8283, loss 0.0180444, acc 1\n",
      "2017-01-11T01:55:45.914551: step 8284, loss 0.0265246, acc 0.992188\n",
      "2017-01-11T01:55:48.078722: step 8285, loss 0.00336564, acc 1\n",
      "2017-01-11T01:55:50.236744: step 8286, loss 0.00157974, acc 1\n",
      "2017-01-11T01:55:52.323710: step 8287, loss 0.0350737, acc 0.984375\n",
      "2017-01-11T01:55:54.394395: step 8288, loss 0.00833023, acc 1\n",
      "2017-01-11T01:55:56.421447: step 8289, loss 0.081608, acc 0.984375\n",
      "2017-01-11T01:55:58.456158: step 8290, loss 0.00306738, acc 1\n",
      "2017-01-11T01:56:00.501544: step 8291, loss 0.00563814, acc 1\n",
      "2017-01-11T01:56:02.645456: step 8292, loss 0.0509663, acc 0.984375\n",
      "2017-01-11T01:56:04.950390: step 8293, loss 0.0108344, acc 0.992188\n",
      "2017-01-11T01:56:06.971045: step 8294, loss 0.00710865, acc 1\n",
      "2017-01-11T01:56:09.000825: step 8295, loss 0.0320346, acc 0.992188\n",
      "2017-01-11T01:56:11.026970: step 8296, loss 0.0144388, acc 0.992188\n",
      "2017-01-11T01:56:13.080577: step 8297, loss 0.000444587, acc 1\n",
      "2017-01-11T01:56:15.102971: step 8298, loss 0.00368585, acc 1\n",
      "2017-01-11T01:56:17.157890: step 8299, loss 0.000712671, acc 1\n",
      "2017-01-11T01:56:19.209776: step 8300, loss 0.000192259, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T01:56:56.935455: step 8300, loss 0.069308, acc 0.98312\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8300\n",
      "\n",
      "2017-01-11T01:57:01.450859: step 8301, loss 0.0500818, acc 0.984375\n",
      "2017-01-11T01:57:03.465727: step 8302, loss 0.0108289, acc 0.992188\n",
      "2017-01-11T01:57:05.557084: step 8303, loss 0.0183702, acc 0.992188\n",
      "2017-01-11T01:57:07.562659: step 8304, loss 0.000267213, acc 1\n",
      "2017-01-11T01:57:09.695486: step 8305, loss 0.0452701, acc 0.992188\n",
      "2017-01-11T01:57:12.174617: step 8306, loss 0.0220933, acc 0.992188\n",
      "2017-01-11T01:57:14.305564: step 8307, loss 0.0414989, acc 0.984375\n",
      "2017-01-11T01:57:16.379335: step 8308, loss 0.00615183, acc 1\n",
      "2017-01-11T01:57:18.400379: step 8309, loss 0.0532822, acc 0.976562\n",
      "2017-01-11T01:57:20.460840: step 8310, loss 0.000807509, acc 1\n",
      "2017-01-11T01:57:22.562933: step 8311, loss 0.0313297, acc 0.992188\n",
      "2017-01-11T01:57:24.629729: step 8312, loss 0.0151065, acc 0.992188\n",
      "2017-01-11T01:57:26.727118: step 8313, loss 0.000381556, acc 1\n",
      "2017-01-11T01:57:28.794675: step 8314, loss 0.0284937, acc 0.992188\n",
      "2017-01-11T01:57:30.824687: step 8315, loss 0.0508259, acc 0.984375\n",
      "2017-01-11T01:57:33.058410: step 8316, loss 0.071229, acc 0.976562\n",
      "2017-01-11T01:57:35.154579: step 8317, loss 0.0747282, acc 0.960938\n",
      "2017-01-11T01:57:37.216529: step 8318, loss 0.0285836, acc 0.992188\n",
      "2017-01-11T01:57:39.245551: step 8319, loss 0.0124063, acc 1\n",
      "2017-01-11T01:57:41.291991: step 8320, loss 0.0321645, acc 0.984375\n",
      "2017-01-11T01:57:43.678740: step 8321, loss 0.00136612, acc 1\n",
      "2017-01-11T01:57:45.698053: step 8322, loss 0.0209926, acc 0.992188\n",
      "2017-01-11T01:57:48.056563: step 8323, loss 0.0324307, acc 0.984375\n",
      "2017-01-11T01:57:50.975286: step 8324, loss 0.00564586, acc 1\n",
      "2017-01-11T01:57:53.019250: step 8325, loss 0.0536663, acc 0.96875\n",
      "2017-01-11T01:57:55.043067: step 8326, loss 0.0170782, acc 1\n",
      "2017-01-11T01:57:57.067496: step 8327, loss 0.019996, acc 0.992188\n",
      "2017-01-11T01:57:59.117704: step 8328, loss 0.0243243, acc 0.992188\n",
      "2017-01-11T01:58:01.186930: step 8329, loss 0.0919463, acc 0.992188\n",
      "2017-01-11T01:58:03.225196: step 8330, loss 0.0460623, acc 0.984375\n",
      "2017-01-11T01:58:05.307578: step 8331, loss 0.0544649, acc 0.984375\n",
      "2017-01-11T01:58:07.354825: step 8332, loss 0.0522414, acc 0.976562\n",
      "2017-01-11T01:58:09.419703: step 8333, loss 0.0316516, acc 0.992188\n",
      "2017-01-11T01:58:11.468597: step 8334, loss 0.0602331, acc 0.976562\n",
      "2017-01-11T01:58:13.478986: step 8335, loss 0.000121505, acc 1\n",
      "2017-01-11T01:58:15.809449: step 8336, loss 0.0254708, acc 0.992188\n",
      "2017-01-11T01:58:17.885964: step 8337, loss 0.036801, acc 0.992188\n",
      "2017-01-11T01:58:19.977911: step 8338, loss 0.0154005, acc 1\n",
      "2017-01-11T01:58:22.013167: step 8339, loss 0.0249938, acc 0.992188\n",
      "2017-01-11T01:58:24.045342: step 8340, loss 0.0275855, acc 0.992188\n",
      "2017-01-11T01:58:26.095410: step 8341, loss 0.0206543, acc 0.992188\n",
      "2017-01-11T01:58:28.136324: step 8342, loss 0.0643394, acc 0.984375\n",
      "2017-01-11T01:58:30.232570: step 8343, loss 0.0285912, acc 0.992188\n",
      "2017-01-11T01:58:32.305311: step 8344, loss 0.0407737, acc 0.984375\n",
      "2017-01-11T01:58:34.332635: step 8345, loss 0.0094877, acc 0.992188\n",
      "2017-01-11T01:58:36.398925: step 8346, loss 0.00878007, acc 1\n",
      "2017-01-11T01:58:38.436070: step 8347, loss 0.0146577, acc 0.992188\n",
      "2017-01-11T01:58:40.457680: step 8348, loss 0.000103922, acc 1\n",
      "2017-01-11T01:58:42.505646: step 8349, loss 0.0446123, acc 0.984375\n",
      "2017-01-11T01:58:44.532080: step 8350, loss 0.000891451, acc 1\n",
      "2017-01-11T01:58:46.929065: step 8351, loss 4.96862e-05, acc 1\n",
      "2017-01-11T01:58:48.963804: step 8352, loss 0.0736754, acc 0.96875\n",
      "2017-01-11T01:58:51.192616: step 8353, loss 0.00386544, acc 1\n",
      "2017-01-11T01:58:53.224853: step 8354, loss 0.0453639, acc 0.976562\n",
      "2017-01-11T01:58:55.304437: step 8355, loss 0.0594204, acc 0.976562\n",
      "2017-01-11T01:58:57.368199: step 8356, loss 0.0254705, acc 0.992188\n",
      "2017-01-11T01:58:59.431100: step 8357, loss 0.0244835, acc 0.992188\n",
      "2017-01-11T01:59:01.453061: step 8358, loss 0.0303445, acc 0.992188\n",
      "2017-01-11T01:59:03.510912: step 8359, loss 0.0247433, acc 0.992188\n",
      "2017-01-11T01:59:05.524272: step 8360, loss 0.00317925, acc 1\n",
      "2017-01-11T01:59:07.559634: step 8361, loss 0.0186087, acc 1\n",
      "2017-01-11T01:59:09.592630: step 8362, loss 0.0115524, acc 1\n",
      "2017-01-11T01:59:11.627771: step 8363, loss 0.0335049, acc 0.984375\n",
      "2017-01-11T01:59:13.642647: step 8364, loss 0.00802675, acc 1\n",
      "2017-01-11T01:59:15.690562: step 8365, loss 0.0280642, acc 0.992188\n",
      "2017-01-11T01:59:17.857857: step 8366, loss 0.0256194, acc 0.992188\n",
      "2017-01-11T01:59:20.088304: step 8367, loss 0.0376257, acc 0.984375\n",
      "2017-01-11T01:59:22.150248: step 8368, loss 0.0185549, acc 0.984375\n",
      "2017-01-11T01:59:24.163884: step 8369, loss 0.043482, acc 0.984375\n",
      "2017-01-11T01:59:26.189059: step 8370, loss 0.0479793, acc 0.984375\n",
      "2017-01-11T01:59:28.209830: step 8371, loss 0.00100218, acc 1\n",
      "2017-01-11T01:59:30.265050: step 8372, loss 0.0194554, acc 0.992188\n",
      "2017-01-11T01:59:32.306601: step 8373, loss 0.0539336, acc 0.992188\n",
      "2017-01-11T01:59:34.340926: step 8374, loss 0.0090578, acc 1\n",
      "2017-01-11T01:59:36.398640: step 8375, loss 0.00253445, acc 1\n",
      "2017-01-11T01:59:38.428566: step 8376, loss 0.0355302, acc 0.984375\n",
      "2017-01-11T01:59:40.461603: step 8377, loss 0.0279862, acc 0.984375\n",
      "2017-01-11T01:59:42.482294: step 8378, loss 0.0510025, acc 0.984375\n",
      "2017-01-11T01:59:44.528885: step 8379, loss 0.0117605, acc 0.992188\n",
      "2017-01-11T01:59:46.590380: step 8380, loss 0.0224009, acc 0.992188\n",
      "2017-01-11T01:59:48.623339: step 8381, loss 0.0264812, acc 0.984375\n",
      "2017-01-11T01:59:51.122599: step 8382, loss 0.0830798, acc 0.984375\n",
      "2017-01-11T01:59:53.173975: step 8383, loss 0.000755598, acc 1\n",
      "2017-01-11T01:59:55.204013: step 8384, loss 0.0082621, acc 1\n",
      "2017-01-11T01:59:57.244022: step 8385, loss 0.0161685, acc 1\n",
      "2017-01-11T01:59:59.273480: step 8386, loss 0.000542956, acc 1\n",
      "2017-01-11T02:00:01.314908: step 8387, loss 0.0306255, acc 0.992188\n",
      "2017-01-11T02:00:03.336563: step 8388, loss 0.0329825, acc 0.984375\n",
      "2017-01-11T02:00:05.391284: step 8389, loss 0.0203551, acc 0.992188\n",
      "2017-01-11T02:00:07.440931: step 8390, loss 0.0273686, acc 0.992188\n",
      "2017-01-11T02:00:09.483655: step 8391, loss 2.50111e-05, acc 1\n",
      "2017-01-11T02:00:11.506584: step 8392, loss 0.0227336, acc 0.984375\n",
      "2017-01-11T02:00:13.524431: step 8393, loss 0.00561759, acc 1\n",
      "2017-01-11T02:00:15.547259: step 8394, loss 0.000272236, acc 1\n",
      "2017-01-11T02:00:17.584198: step 8395, loss 0.000234193, acc 1\n",
      "2017-01-11T02:00:19.666025: step 8396, loss 0.00997889, acc 0.992188\n",
      "2017-01-11T02:00:21.868075: step 8397, loss 0.0380786, acc 0.984375\n",
      "2017-01-11T02:00:24.090492: step 8398, loss 0.0165673, acc 1\n",
      "2017-01-11T02:00:26.154196: step 8399, loss 0.0172937, acc 1\n",
      "2017-01-11T02:00:28.349108: step 8400, loss 0.0296704, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:00:55.434465: step 8400, loss 0.0695594, acc 0.98232\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8400\n",
      "\n",
      "2017-01-11T02:01:00.276409: step 8401, loss 0.0153164, acc 0.992188\n",
      "2017-01-11T02:01:02.338418: step 8402, loss 0.0483519, acc 0.976562\n",
      "2017-01-11T02:01:04.452938: step 8403, loss 0.0206149, acc 0.992188\n",
      "2017-01-11T02:01:06.470677: step 8404, loss 0.00120305, acc 1\n",
      "2017-01-11T02:01:08.534411: step 8405, loss 0.0390144, acc 0.992188\n",
      "2017-01-11T02:01:10.587606: step 8406, loss 0.0264566, acc 0.984375\n",
      "2017-01-11T02:01:12.636978: step 8407, loss 0.0542208, acc 0.976562\n",
      "2017-01-11T02:01:14.649343: step 8408, loss 0.00927452, acc 1\n",
      "2017-01-11T02:01:16.718651: step 8409, loss 0.0421313, acc 0.992188\n",
      "2017-01-11T02:01:18.759586: step 8410, loss 0.067429, acc 0.992188\n",
      "2017-01-11T02:01:20.785793: step 8411, loss 0.0148965, acc 0.992188\n",
      "2017-01-11T02:01:22.792232: step 8412, loss 0.0399909, acc 0.984375\n",
      "2017-01-11T02:01:24.820979: step 8413, loss 0.0320215, acc 0.992188\n",
      "2017-01-11T02:01:26.855761: step 8414, loss 0.00897417, acc 0.992188\n",
      "2017-01-11T02:01:28.979619: step 8415, loss 0.0341737, acc 0.976562\n",
      "2017-01-11T02:01:31.261897: step 8416, loss 0.00289423, acc 1\n",
      "2017-01-11T02:01:33.317275: step 8417, loss 0.0729877, acc 0.992188\n",
      "2017-01-11T02:01:35.572331: step 8418, loss 0.0144279, acc 0.992188\n",
      "2017-01-11T02:01:37.601544: step 8419, loss 0.0337404, acc 0.984375\n",
      "2017-01-11T02:01:39.623766: step 8420, loss 0.0409919, acc 0.984375\n",
      "2017-01-11T02:01:41.672369: step 8421, loss 0.047199, acc 0.984375\n",
      "2017-01-11T02:01:43.706838: step 8422, loss 0.0249564, acc 0.992188\n",
      "2017-01-11T02:01:45.791173: step 8423, loss 3.38902e-05, acc 1\n",
      "2017-01-11T02:01:47.825936: step 8424, loss 0.0564746, acc 0.976562\n",
      "2017-01-11T02:01:49.880941: step 8425, loss 0.0445736, acc 0.976562\n",
      "2017-01-11T02:01:53.130203: step 8426, loss 0.0349864, acc 0.984375\n",
      "2017-01-11T02:01:55.301218: step 8427, loss 0.0125809, acc 0.992188\n",
      "2017-01-11T02:01:57.363469: step 8428, loss 0.0548932, acc 0.984375\n",
      "2017-01-11T02:01:59.411267: step 8429, loss 0.0463448, acc 0.984375\n",
      "2017-01-11T02:02:01.656119: step 8430, loss 0.0427324, acc 0.984375\n",
      "2017-01-11T02:02:03.881851: step 8431, loss 0.0533037, acc 0.984375\n",
      "2017-01-11T02:02:05.983664: step 8432, loss 0.026033, acc 0.992188\n",
      "2017-01-11T02:02:08.017821: step 8433, loss 0.0139591, acc 1\n",
      "2017-01-11T02:02:10.071176: step 8434, loss 0.00946499, acc 0.992188\n",
      "2017-01-11T02:02:12.105498: step 8435, loss 1.82357e-05, acc 1\n",
      "2017-01-11T02:02:14.109135: step 8436, loss 0.0433583, acc 0.992188\n",
      "2017-01-11T02:02:16.166069: step 8437, loss 0.00124424, acc 1\n",
      "2017-01-11T02:02:18.168698: step 8438, loss 0.00147448, acc 1\n",
      "2017-01-11T02:02:20.209618: step 8439, loss 0.0268454, acc 0.992188\n",
      "2017-01-11T02:02:22.294351: step 8440, loss 0.0090695, acc 1\n",
      "2017-01-11T02:02:24.333003: step 8441, loss 0.0609466, acc 0.976562\n",
      "2017-01-11T02:02:26.357804: step 8442, loss 0.000608439, acc 1\n",
      "2017-01-11T02:02:28.394451: step 8443, loss 0.0176579, acc 0.992188\n",
      "2017-01-11T02:02:30.511747: step 8444, loss 0.0145284, acc 1\n",
      "2017-01-11T02:02:32.561992: step 8445, loss 0.0579751, acc 0.976562\n",
      "2017-01-11T02:02:34.940311: step 8446, loss 0.0113174, acc 0.992188\n",
      "2017-01-11T02:02:36.931519: step 8447, loss 0.022376, acc 0.992188\n",
      "2017-01-11T02:02:39.146012: step 8448, loss 0.0443387, acc 0.992188\n",
      "2017-01-11T02:02:41.203628: step 8449, loss 0.0321462, acc 0.992188\n",
      "2017-01-11T02:02:43.233779: step 8450, loss 0.0305452, acc 0.984375\n",
      "2017-01-11T02:02:45.247722: step 8451, loss 0.00161457, acc 1\n",
      "2017-01-11T02:02:47.291132: step 8452, loss 0.0084328, acc 0.992188\n",
      "2017-01-11T02:02:49.351009: step 8453, loss 0.0642588, acc 0.984375\n",
      "2017-01-11T02:02:51.491230: step 8454, loss 0.00408813, acc 1\n",
      "2017-01-11T02:02:53.869448: step 8455, loss 0.0628879, acc 0.984375\n",
      "2017-01-11T02:02:56.787591: step 8456, loss 0.00200114, acc 1\n",
      "2017-01-11T02:02:58.826249: step 8457, loss 0.0210033, acc 0.992188\n",
      "2017-01-11T02:03:00.860366: step 8458, loss 0.0155056, acc 1\n",
      "2017-01-11T02:03:02.934422: step 8459, loss 0.0515954, acc 0.984375\n",
      "2017-01-11T02:03:04.998089: step 8460, loss 0.00141129, acc 1\n",
      "2017-01-11T02:03:07.376017: step 8461, loss 0.0193857, acc 0.984375\n",
      "2017-01-11T02:03:09.424733: step 8462, loss 0.0034705, acc 1\n",
      "2017-01-11T02:03:11.463467: step 8463, loss 0.0209688, acc 1\n",
      "2017-01-11T02:03:13.500137: step 8464, loss 0.00335956, acc 1\n",
      "2017-01-11T02:03:15.552809: step 8465, loss 0.0246993, acc 0.992188\n",
      "2017-01-11T02:03:17.638295: step 8466, loss 0.0424208, acc 0.984375\n",
      "2017-01-11T02:03:19.702717: step 8467, loss 0.033879, acc 0.992188\n",
      "2017-01-11T02:03:21.786210: step 8468, loss 0.0185671, acc 0.992188\n",
      "2017-01-11T02:03:23.816055: step 8469, loss 0.0118458, acc 0.992188\n",
      "2017-01-11T02:03:25.848682: step 8470, loss 0.0133527, acc 0.992188\n",
      "2017-01-11T02:03:27.864931: step 8471, loss 0.0388658, acc 0.992188\n",
      "2017-01-11T02:03:29.895021: step 8472, loss 0.0226018, acc 0.992188\n",
      "2017-01-11T02:03:31.933088: step 8473, loss 0.0173413, acc 0.992188\n",
      "2017-01-11T02:03:34.003783: step 8474, loss 0.023343, acc 0.992188\n",
      "2017-01-11T02:03:36.074484: step 8475, loss 0.0341967, acc 0.992188\n",
      "2017-01-11T02:03:38.451961: step 8476, loss 0.0903743, acc 0.984375\n",
      "2017-01-11T02:03:40.465562: step 8477, loss 0.00335394, acc 1\n",
      "2017-01-11T02:03:42.531455: step 8478, loss 0.00520011, acc 1\n",
      "2017-01-11T02:03:44.587459: step 8479, loss 0.202071, acc 0.96875\n",
      "2017-01-11T02:03:46.637763: step 8480, loss 0.0419088, acc 0.984375\n",
      "2017-01-11T02:03:48.649504: step 8481, loss 0.00505119, acc 1\n",
      "2017-01-11T02:03:50.697279: step 8482, loss 0.00011696, acc 1\n",
      "2017-01-11T02:03:52.801393: step 8483, loss 0.0152881, acc 0.992188\n",
      "2017-01-11T02:03:55.082680: step 8484, loss 0.104451, acc 0.992188\n",
      "2017-01-11T02:03:57.338558: step 8485, loss 0.0593732, acc 0.984375\n",
      "2017-01-11T02:03:59.675511: step 8486, loss 0.00808448, acc 1\n",
      "2017-01-11T02:04:01.736234: step 8487, loss 0.0103099, acc 1\n",
      "2017-01-11T02:04:04.099862: step 8488, loss 0.00407801, acc 1\n",
      "2017-01-11T02:04:06.196380: step 8489, loss 0.0114909, acc 0.992188\n",
      "2017-01-11T02:04:08.258885: step 8490, loss 0.0399602, acc 0.984375\n",
      "2017-01-11T02:04:10.652151: step 8491, loss 0.0651147, acc 0.984375\n",
      "2017-01-11T02:04:12.686242: step 8492, loss 0.0216365, acc 0.992188\n",
      "2017-01-11T02:04:14.792148: step 8493, loss 0.00922201, acc 1\n",
      "2017-01-11T02:04:16.864743: step 8494, loss 0.0236731, acc 0.992188\n",
      "2017-01-11T02:04:18.898706: step 8495, loss 0.0496868, acc 0.984375\n",
      "2017-01-11T02:04:20.980083: step 8496, loss 0.0251157, acc 0.992188\n",
      "2017-01-11T02:04:23.092259: step 8497, loss 0.0540913, acc 0.992188\n",
      "2017-01-11T02:04:25.105331: step 8498, loss 0.0283511, acc 0.984375\n",
      "2017-01-11T02:04:27.161803: step 8499, loss 0.0165592, acc 0.992188\n",
      "2017-01-11T02:04:29.187006: step 8500, loss 0.0229737, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:04:58.286776: step 8500, loss 0.0706082, acc 0.98264\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8500\n",
      "\n",
      "2017-01-11T02:05:02.947479: step 8501, loss 0.000473929, acc 1\n",
      "2017-01-11T02:05:04.972975: step 8502, loss 0.0792445, acc 0.976562\n",
      "2017-01-11T02:05:06.984340: step 8503, loss 0.0206577, acc 0.992188\n",
      "2017-01-11T02:05:09.035691: step 8504, loss 0.0360181, acc 0.984375\n",
      "2017-01-11T02:05:11.072183: step 8505, loss 0.0807791, acc 0.984375\n",
      "2017-01-11T02:05:13.183199: step 8506, loss 0.0260233, acc 0.992188\n",
      "2017-01-11T02:05:15.467048: step 8507, loss 0.0086769, acc 1\n",
      "2017-01-11T02:05:17.499164: step 8508, loss 0.0507609, acc 0.992188\n",
      "2017-01-11T02:05:19.580998: step 8509, loss 0.00724474, acc 1\n",
      "2017-01-11T02:05:21.626612: step 8510, loss 0.0118249, acc 1\n",
      "2017-01-11T02:05:23.665150: step 8511, loss 0.0339371, acc 0.984375\n",
      "2017-01-11T02:05:25.710176: step 8512, loss 0.010872, acc 1\n",
      "2017-01-11T02:05:27.749956: step 8513, loss 0.0254462, acc 0.992188\n",
      "2017-01-11T02:05:30.115402: step 8514, loss 0.0125261, acc 1\n",
      "2017-01-11T02:05:32.126272: step 8515, loss 0.021302, acc 0.992188\n",
      "2017-01-11T02:05:34.169109: step 8516, loss 0.0219455, acc 0.992188\n",
      "2017-01-11T02:05:36.240277: step 8517, loss 0.0247372, acc 1\n",
      "2017-01-11T02:05:38.283904: step 8518, loss 0.0251279, acc 0.992188\n",
      "2017-01-11T02:05:40.297122: step 8519, loss 0.000578962, acc 1\n",
      "2017-01-11T02:05:42.316637: step 8520, loss 0.0479294, acc 0.976562\n",
      "2017-01-11T02:05:44.321516: step 8521, loss 0.0609682, acc 0.984375\n",
      "2017-01-11T02:05:47.734057: step 8522, loss 0.0467092, acc 0.984375\n",
      "2017-01-11T02:05:49.814547: step 8523, loss 0.0124134, acc 0.992188\n",
      "2017-01-11T02:05:51.902962: step 8524, loss 0.0492712, acc 0.984375\n",
      "2017-01-11T02:05:54.033574: step 8525, loss 0.00678431, acc 0.992188\n",
      "2017-01-11T02:05:56.041173: step 8526, loss 0.00451811, acc 1\n",
      "2017-01-11T02:05:58.083945: step 8527, loss 0.0155706, acc 1\n",
      "2017-01-11T02:06:00.129518: step 8528, loss 0.0820306, acc 0.976562\n",
      "2017-01-11T02:06:02.169341: step 8529, loss 0.0432342, acc 0.984375\n",
      "2017-01-11T02:06:04.216843: step 8530, loss 0.0653772, acc 0.984375\n",
      "2017-01-11T02:06:06.304639: step 8531, loss 0.0357752, acc 0.992188\n",
      "2017-01-11T02:06:08.353965: step 8532, loss 0.0104711, acc 0.992188\n",
      "2017-01-11T02:06:10.381492: step 8533, loss 0.0155585, acc 0.992188\n",
      "2017-01-11T02:06:12.423313: step 8534, loss 0.0206813, acc 0.992188\n",
      "2017-01-11T02:06:14.431441: step 8535, loss 0.0253842, acc 0.992188\n",
      "2017-01-11T02:06:16.473197: step 8536, loss 7.7551e-05, acc 1\n",
      "2017-01-11T02:06:18.806147: step 8537, loss 0.020194, acc 1\n",
      "2017-01-11T02:06:20.862597: step 8538, loss 0.0358125, acc 0.984375\n",
      "2017-01-11T02:06:22.952864: step 8539, loss 0.0629154, acc 0.976562\n",
      "2017-01-11T02:06:24.986295: step 8540, loss 0.108378, acc 0.976562\n",
      "2017-01-11T02:06:26.993879: step 8541, loss 0.0225571, acc 0.992188\n",
      "2017-01-11T02:06:29.043201: step 8542, loss 0.00188414, acc 1\n",
      "2017-01-11T02:06:31.236581: step 8543, loss 0.00489299, acc 1\n",
      "2017-01-11T02:06:33.284800: step 8544, loss 0.00697767, acc 1\n",
      "2017-01-11T02:06:35.340019: step 8545, loss 0.000321532, acc 1\n",
      "2017-01-11T02:06:37.357114: step 8546, loss 0.00129165, acc 1\n",
      "2017-01-11T02:06:39.400287: step 8547, loss 0.0399286, acc 0.984375\n",
      "2017-01-11T02:06:41.635334: step 8548, loss 0.00592698, acc 1\n",
      "2017-01-11T02:06:43.692892: step 8549, loss 0.00952614, acc 1\n",
      "2017-01-11T02:06:45.688417: step 8550, loss 0.000134452, acc 1\n",
      "2017-01-11T02:06:47.733568: step 8551, loss 0.0276221, acc 0.992188\n",
      "2017-01-11T02:06:49.901331: step 8552, loss 0.0275305, acc 0.984375\n",
      "2017-01-11T02:06:52.221803: step 8553, loss 0.00381568, acc 1\n",
      "2017-01-11T02:06:54.372798: step 8554, loss 0.0140792, acc 0.992188\n",
      "2017-01-11T02:06:57.032208: step 8555, loss 0.026123, acc 1\n",
      "2017-01-11T02:06:59.530944: step 8556, loss 0.0352397, acc 0.984375\n",
      "2017-01-11T02:07:01.557180: step 8557, loss 0.000676459, acc 1\n",
      "2017-01-11T02:07:03.614997: step 8558, loss 0.0101924, acc 0.992188\n",
      "2017-01-11T02:07:05.681502: step 8559, loss 0.0355275, acc 0.992188\n",
      "2017-01-11T02:07:07.768391: step 8560, loss 0.00416085, acc 1\n",
      "2017-01-11T02:07:09.797297: step 8561, loss 0.0357582, acc 0.992188\n",
      "2017-01-11T02:07:11.789945: step 8562, loss 0.0656263, acc 0.984375\n",
      "2017-01-11T02:07:13.815947: step 8563, loss 0.0215895, acc 1\n",
      "2017-01-11T02:07:15.865159: step 8564, loss 0.0375924, acc 0.984375\n",
      "2017-01-11T02:07:17.918487: step 8565, loss 0.0174363, acc 0.992188\n",
      "2017-01-11T02:07:19.986722: step 8566, loss 0.00971595, acc 1\n",
      "2017-01-11T02:11:44.363701: step 8677, loss 0.0334616, acc 0.992188\n",
      "2017-01-11T02:11:46.596024: step 8678, loss 0.0302606, acc 0.992188\n",
      "2017-01-11T02:11:48.608312: step 8679, loss 0.0259113, acc 0.984375\n",
      "2017-01-11T02:11:50.675865: step 8680, loss 0.00471578, acc 1\n",
      "2017-01-11T02:11:52.750372: step 8681, loss 0.0049704, acc 1\n",
      "2017-01-11T02:11:54.747247: step 8682, loss 0.0282751, acc 0.984375\n",
      "2017-01-11T02:11:56.908165: step 8683, loss 0.0469625, acc 0.984375\n",
      "2017-01-11T02:11:58.954920: step 8684, loss 0.000812369, acc 1\n",
      "2017-01-11T02:12:01.026701: step 8685, loss 0.0343337, acc 0.992188\n",
      "2017-01-11T02:12:03.941427: step 8686, loss 0.0181822, acc 0.992188\n",
      "2017-01-11T02:12:06.214819: step 8687, loss 0.264303, acc 0.976562\n",
      "2017-01-11T02:12:08.273875: step 8688, loss 0.00210749, acc 1\n",
      "2017-01-11T02:12:10.312876: step 8689, loss 0.0136955, acc 0.992188\n",
      "2017-01-11T02:12:12.383178: step 8690, loss 0.0131493, acc 0.992188\n",
      "2017-01-11T02:12:14.725199: step 8691, loss 0.0601949, acc 0.976562\n",
      "2017-01-11T02:12:16.748404: step 8692, loss 0.0668262, acc 0.976562\n",
      "2017-01-11T02:12:18.798441: step 8693, loss 0.111629, acc 0.96875\n",
      "2017-01-11T02:12:20.870433: step 8694, loss 0.045444, acc 0.984375\n",
      "2017-01-11T02:12:22.938122: step 8695, loss 0.0672802, acc 0.976562\n",
      "2017-01-11T02:12:24.980469: step 8696, loss 0.0105309, acc 0.992188\n",
      "2017-01-11T02:12:26.965849: step 8697, loss 0.0393222, acc 0.984375\n",
      "2017-01-11T02:12:29.043758: step 8698, loss 0.011556, acc 1\n",
      "2017-01-11T02:12:31.180604: step 8699, loss 0.0561697, acc 0.976562\n",
      "2017-01-11T02:12:33.221128: step 8700, loss 0.0140713, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:13:12.076885: step 8700, loss 0.0685088, acc 0.98284\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8700\n",
      "\n",
      "2017-01-11T02:13:17.209500: step 8701, loss 0.00542509, acc 1\n",
      "2017-01-11T02:13:19.534164: step 8702, loss 0.00645499, acc 1\n",
      "2017-01-11T02:13:21.877347: step 8703, loss 0.0119502, acc 0.992188\n",
      "2017-01-11T02:13:24.002717: step 8704, loss 0.0937873, acc 0.976562\n",
      "2017-01-11T02:13:26.118006: step 8705, loss 0.000762406, acc 1\n",
      "2017-01-11T02:13:28.288995: step 8706, loss 0.000102007, acc 1\n",
      "2017-01-11T02:13:30.357063: step 8707, loss 0.0399515, acc 0.992188\n",
      "2017-01-11T02:13:32.394771: step 8708, loss 0.0183922, acc 0.992188\n",
      "2017-01-11T02:13:34.443751: step 8709, loss 0.0143623, acc 0.992188\n",
      "2017-01-11T02:13:36.501865: step 8710, loss 0.0144506, acc 0.992188\n",
      "2017-01-11T02:13:38.562761: step 8711, loss 0.000262482, acc 1\n",
      "2017-01-11T02:13:40.601914: step 8712, loss 0.0017517, acc 1\n",
      "2017-01-11T02:13:42.645834: step 8713, loss 0.00234209, acc 1\n",
      "2017-01-11T02:13:44.616493: step 8714, loss 0.0197493, acc 0.992188\n",
      "2017-01-11T02:13:46.666711: step 8715, loss 0.0412065, acc 0.992188\n",
      "2017-01-11T02:13:48.683541: step 8716, loss 0.0390557, acc 0.992188\n",
      "2017-01-11T02:13:50.737591: step 8717, loss 0.0183805, acc 1\n",
      "2017-01-11T02:13:53.128403: step 8718, loss 0.016312, acc 0.992188\n",
      "2017-01-11T02:13:55.153329: step 8719, loss 0.0213035, acc 0.992188\n",
      "2017-01-11T02:13:57.155870: step 8720, loss 0.00468867, acc 1\n",
      "2017-01-11T02:13:59.270434: step 8721, loss 0.0336234, acc 0.992188\n",
      "2017-01-11T02:14:01.352948: step 8722, loss 0.00191375, acc 1\n",
      "2017-01-11T02:14:03.381220: step 8723, loss 1.17169e-05, acc 1\n",
      "2017-01-11T02:14:05.450806: step 8724, loss 0.0629276, acc 0.976562\n",
      "2017-01-11T02:14:07.500997: step 8725, loss 0.0354648, acc 0.992188\n",
      "2017-01-11T02:14:09.530293: step 8726, loss 0.0489308, acc 0.984375\n",
      "2017-01-11T02:14:11.566753: step 8727, loss 0.0158479, acc 0.992188\n",
      "2017-01-11T02:14:13.600050: step 8728, loss 0.0149839, acc 1\n",
      "2017-01-11T02:14:15.649857: step 8729, loss 0.000227729, acc 1\n",
      "2017-01-11T02:14:17.686961: step 8730, loss 0.0173666, acc 0.992188\n",
      "2017-01-11T02:14:19.769227: step 8731, loss 0.000125621, acc 1\n",
      "2017-01-11T02:14:21.865263: step 8732, loss 0.0393769, acc 0.984375\n",
      "2017-01-11T02:14:24.097547: step 8733, loss 0.0407562, acc 0.984375\n",
      "2017-01-11T02:14:26.212281: step 8734, loss 0.0143552, acc 0.992188\n",
      "2017-01-11T02:14:28.242429: step 8735, loss 0.0504839, acc 0.984375\n",
      "2017-01-11T02:14:30.349994: step 8736, loss 0.00033285, acc 1\n",
      "2017-01-11T02:14:32.445649: step 8737, loss 0.0351892, acc 0.992188\n",
      "2017-01-11T02:14:34.445896: step 8738, loss 0.00534689, acc 1\n",
      "2017-01-11T02:14:36.500108: step 8739, loss 0.00156, acc 1\n",
      "2017-01-11T02:14:38.520707: step 8740, loss 0.0178882, acc 0.992188\n",
      "2017-01-11T02:14:40.555373: step 8741, loss 0.00230389, acc 1\n",
      "2017-01-11T02:14:42.596757: step 8742, loss 0.0298292, acc 0.984375\n",
      "2017-01-11T02:14:44.646583: step 8743, loss 0.0280469, acc 0.992188\n",
      "2017-01-11T02:14:46.698159: step 8744, loss 0.0251938, acc 0.992188\n",
      "2017-01-11T02:14:48.740491: step 8745, loss 0.0249792, acc 0.992188\n",
      "2017-01-11T02:14:50.799765: step 8746, loss 0.0635425, acc 0.984375\n",
      "2017-01-11T02:14:52.887669: step 8747, loss 0.00070999, acc 1\n",
      "2017-01-11T02:14:54.887059: step 8748, loss 0.0381989, acc 0.984375\n",
      "2017-01-11T02:14:57.231567: step 8749, loss 0.00583107, acc 0.992188\n",
      "2017-01-11T02:14:59.351295: step 8750, loss 0.015893, acc 1\n",
      "2017-01-11T02:15:01.394855: step 8751, loss 0.0540025, acc 0.992188\n",
      "2017-01-11T02:15:03.441760: step 8752, loss 0.0170469, acc 0.992188\n",
      "2017-01-11T02:15:05.533991: step 8753, loss 0.000631281, acc 1\n",
      "2017-01-11T02:15:07.581934: step 8754, loss 0.0110584, acc 1\n",
      "2017-01-11T02:15:09.624031: step 8755, loss 0.0387714, acc 0.992188\n",
      "2017-01-11T02:15:11.672394: step 8756, loss 0.000166461, acc 1\n",
      "2017-01-11T02:15:13.723705: step 8757, loss 0.198592, acc 0.976562\n",
      "2017-01-11T02:15:15.746477: step 8758, loss 0.0255831, acc 0.992188\n",
      "2017-01-11T02:15:17.791386: step 8759, loss 0.031084, acc 0.984375\n",
      "2017-01-11T02:15:19.850862: step 8760, loss 0.000285133, acc 1\n",
      "2017-01-11T02:15:21.914071: step 8761, loss 0.00264106, acc 1\n",
      "2017-01-11T02:15:23.909756: step 8762, loss 0.000433903, acc 1\n",
      "2017-01-11T02:15:25.941980: step 8763, loss 0.00408561, acc 1\n",
      "2017-01-11T02:15:28.373174: step 8764, loss 0.023727, acc 0.992188\n",
      "2017-01-11T02:15:30.554245: step 8765, loss 0.0476642, acc 0.984375\n",
      "2017-01-11T02:15:32.605412: step 8766, loss 0.0194388, acc 1\n",
      "2017-01-11T02:15:34.685957: step 8767, loss 0.0263657, acc 0.992188\n",
      "2017-01-11T02:15:36.712924: step 8768, loss 0.0261769, acc 0.992188\n",
      "2017-01-11T02:15:38.747751: step 8769, loss 0.0554383, acc 0.976562\n",
      "2017-01-11T02:15:40.745443: step 8770, loss 0.0109484, acc 1\n",
      "2017-01-11T02:15:42.778348: step 8771, loss 0.0332232, acc 0.992188\n",
      "2017-01-11T02:15:45.664184: step 8772, loss 0.0363928, acc 0.976562\n",
      "2017-01-11T02:15:47.961427: step 8773, loss 0.0343878, acc 0.992188\n",
      "2017-01-11T02:15:50.025157: step 8774, loss 0.00250285, acc 1\n",
      "2017-01-11T02:15:52.095719: step 8775, loss 0.0478423, acc 0.992188\n",
      "2017-01-11T02:15:54.116731: step 8776, loss 0.00466873, acc 1\n",
      "2017-01-11T02:15:56.173592: step 8777, loss 0.0342055, acc 0.984375\n",
      "2017-01-11T02:15:58.195496: step 8778, loss 0.0283613, acc 0.992188\n",
      "2017-01-11T02:16:00.672977: step 8779, loss 0.0235219, acc 0.992188\n",
      "2017-01-11T02:16:02.678279: step 8780, loss 0.000325119, acc 1\n",
      "2017-01-11T02:16:04.739357: step 8781, loss 0.00299655, acc 1\n",
      "2017-01-11T02:16:06.770170: step 8782, loss 0.0259619, acc 0.992188\n",
      "2017-01-11T02:16:08.818017: step 8783, loss 0.0126678, acc 0.992188\n",
      "2017-01-11T02:16:10.880848: step 8784, loss 0.00102102, acc 1\n",
      "2017-01-11T02:16:12.910796: step 8785, loss 0.0344044, acc 0.992188\n",
      "2017-01-11T02:16:14.987921: step 8786, loss 0.0387107, acc 0.984375\n",
      "2017-01-11T02:16:17.020732: step 8787, loss 0.0394935, acc 0.992188\n",
      "2017-01-11T02:16:19.078381: step 8788, loss 0.0300616, acc 0.992188\n",
      "2017-01-11T02:16:21.185397: step 8789, loss 0.0811671, acc 0.984375\n",
      "2017-01-11T02:16:23.188469: step 8790, loss 0.00494935, acc 1\n",
      "2017-01-11T02:16:25.284733: step 8791, loss 0.0355933, acc 0.992188\n",
      "2017-01-11T02:16:27.299208: step 8792, loss 0.0060026, acc 1\n",
      "2017-01-11T02:16:29.322027: step 8793, loss 0.00617045, acc 1\n",
      "2017-01-11T02:16:31.631514: step 8794, loss 0.046802, acc 0.984375\n",
      "2017-01-11T02:16:33.773019: step 8795, loss 0.00316595, acc 1\n",
      "2017-01-11T02:16:35.844756: step 8796, loss 0.000339462, acc 1\n",
      "2017-01-11T02:16:37.876159: step 8797, loss 0.0439486, acc 0.976562\n",
      "2017-01-11T02:16:39.903078: step 8798, loss 0.0244855, acc 0.992188\n",
      "2017-01-11T02:16:41.971484: step 8799, loss 0.000165978, acc 1\n",
      "2017-01-11T02:16:44.021577: step 8800, loss 0.0466293, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:17:12.886505: step 8800, loss 0.0692387, acc 0.98312\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8800\n",
      "\n",
      "2017-01-11T02:17:17.382319: step 8801, loss 0.105844, acc 0.984375\n",
      "2017-01-11T02:17:19.410560: step 8802, loss 0.0662109, acc 0.96875\n",
      "2017-01-11T02:17:21.511557: step 8803, loss 0.0144431, acc 1\n",
      "2017-01-11T02:17:23.527562: step 8804, loss 0.0106211, acc 0.992188\n",
      "2017-01-11T02:17:25.596626: step 8805, loss 0.0135484, acc 1\n",
      "2017-01-11T02:17:27.659090: step 8806, loss 0.0488516, acc 0.992188\n",
      "2017-01-11T02:17:29.699371: step 8807, loss 0.114042, acc 0.976562\n",
      "2017-01-11T02:17:31.706851: step 8808, loss 0.0784336, acc 0.96875\n",
      "2017-01-11T02:17:33.744247: step 8809, loss 0.0337605, acc 0.984375\n",
      "2017-01-11T02:17:35.818691: step 8810, loss 0.0117646, acc 1\n",
      "2017-01-11T02:17:38.134472: step 8811, loss 0.02911, acc 0.992188\n",
      "2017-01-11T02:17:40.182828: step 8812, loss 0.0187444, acc 0.992188\n",
      "2017-01-11T02:17:42.218445: step 8813, loss 0.0311028, acc 0.992188\n",
      "2017-01-11T02:17:44.213497: step 8814, loss 0.019987, acc 1\n",
      "2017-01-11T02:17:46.252048: step 8815, loss 0.00308066, acc 1\n",
      "2017-01-11T02:17:48.306152: step 8816, loss 0.0212136, acc 0.992188\n",
      "2017-01-11T02:17:50.365521: step 8817, loss 0.003957, acc 1\n",
      "2017-01-11T02:17:52.428968: step 8818, loss 0.0116169, acc 0.992188\n",
      "2017-01-11T02:17:54.462169: step 8819, loss 0.0447034, acc 0.984375\n",
      "2017-01-11T02:17:56.511651: step 8820, loss 0.0392975, acc 0.992188\n",
      "2017-01-11T02:17:58.736626: step 8821, loss 0.000911966, acc 1\n",
      "2017-01-11T02:18:00.858723: step 8822, loss 0.0019251, acc 1\n",
      "2017-01-11T02:18:02.883577: step 8823, loss 0.00860677, acc 0.992188\n",
      "2017-01-11T02:18:04.976950: step 8824, loss 0.0198905, acc 0.992188\n",
      "2017-01-11T02:18:07.021457: step 8825, loss 0.0555476, acc 0.984375\n",
      "2017-01-11T02:18:09.174181: step 8826, loss 0.0344753, acc 0.992188\n",
      "2017-01-11T02:18:11.457607: step 8827, loss 0.0156867, acc 0.992188\n",
      "2017-01-11T02:18:14.090155: step 8828, loss 0.0166216, acc 1\n",
      "2017-01-11T02:18:16.577241: step 8829, loss 0.0206759, acc 0.992188\n",
      "2017-01-11T02:18:18.675854: step 8830, loss 0.0321302, acc 0.992188\n",
      "2017-01-11T02:18:20.725032: step 8831, loss 0.00605973, acc 1\n",
      "2017-01-11T02:18:22.783409: step 8832, loss 0.0350733, acc 0.984375\n",
      "2017-01-11T02:18:24.827192: step 8833, loss 0.0462282, acc 0.984375\n",
      "2017-01-11T02:18:26.911332: step 8834, loss 0.0322644, acc 0.992188\n",
      "2017-01-11T02:18:28.976145: step 8835, loss 0.0190845, acc 0.992188\n",
      "2017-01-11T02:18:31.127802: step 8836, loss 0.0710083, acc 0.984375\n",
      "2017-01-11T02:18:33.167321: step 8837, loss 0.0426077, acc 0.984375\n",
      "2017-01-11T02:18:35.229955: step 8838, loss 0.00353243, acc 1\n",
      "2017-01-11T02:18:37.263862: step 8839, loss 0.00144178, acc 1\n",
      "2017-01-11T02:18:39.296115: step 8840, loss 0.0151359, acc 1\n",
      "2017-01-11T02:18:41.368283: step 8841, loss 0.03553, acc 0.992188\n",
      "2017-01-11T02:18:43.661949: step 8842, loss 0.0332723, acc 0.992188\n",
      "2017-01-11T02:18:45.675062: step 8843, loss 0.130236, acc 0.992188\n",
      "2017-01-11T02:18:47.709032: step 8844, loss 0.000755017, acc 1\n",
      "2017-01-11T02:18:50.073318: step 8845, loss 0.00594139, acc 1\n",
      "2017-01-11T02:18:52.131820: step 8846, loss 0.0276108, acc 1\n",
      "2017-01-11T02:18:54.180407: step 8847, loss 0.0232278, acc 0.984375\n",
      "2017-01-11T02:18:56.200984: step 8848, loss 0.0138457, acc 0.992188\n",
      "2017-01-11T02:18:58.224120: step 8849, loss 0.00733617, acc 1\n",
      "2017-01-11T02:19:00.270468: step 8850, loss 0.014037, acc 0.992188\n",
      "2017-01-11T02:19:02.412751: step 8851, loss 0.0148485, acc 0.984375\n",
      "2017-01-11T02:19:04.461311: step 8852, loss 0.0743696, acc 0.976562\n",
      "2017-01-11T02:19:06.532967: step 8853, loss 0.000748624, acc 1\n",
      "2017-01-11T02:19:08.577384: step 8854, loss 0.0197609, acc 0.992188\n",
      "2017-01-11T02:19:10.606627: step 8855, loss 0.0019098, acc 1\n",
      "2017-01-11T02:19:12.635189: step 8856, loss 0.00187584, acc 1\n",
      "2017-01-11T02:19:15.016915: step 8857, loss 0.0104222, acc 0.992188\n",
      "2017-01-11T02:19:17.057473: step 8858, loss 0.00738551, acc 1\n",
      "2017-01-11T02:19:19.121777: step 8859, loss 0.0195355, acc 0.992188\n",
      "2017-01-11T02:19:21.175502: step 8860, loss 0.0191237, acc 0.992188\n",
      "2017-01-11T02:19:23.245181: step 8861, loss 0.00380924, acc 1\n",
      "2017-01-11T02:19:25.271439: step 8862, loss 0.00952117, acc 1\n",
      "2017-01-11T02:19:27.317664: step 8863, loss 0.0057827, acc 1\n",
      "2017-01-11T02:19:29.371474: step 8864, loss 0.00100954, acc 1\n",
      "2017-01-11T02:19:31.387241: step 8865, loss 0.00226992, acc 1\n",
      "2017-01-11T02:19:33.421403: step 8866, loss 0.0144049, acc 0.992188\n",
      "2017-01-11T02:19:35.513271: step 8867, loss 0.0110329, acc 0.992188\n",
      "2017-01-11T02:19:37.564172: step 8868, loss 0.0500434, acc 0.992188\n",
      "2017-01-11T02:19:39.618041: step 8869, loss 0.0400253, acc 0.992188\n",
      "2017-01-11T02:19:41.642240: step 8870, loss 0.000103481, acc 1\n",
      "2017-01-11T02:19:43.733979: step 8871, loss 0.00458131, acc 1\n",
      "2017-01-11T02:19:45.910936: step 8872, loss 0.000647294, acc 1\n",
      "2017-01-11T02:19:48.128038: step 8873, loss 5.37951e-05, acc 1\n",
      "2017-01-11T02:19:50.179648: step 8874, loss 0.0416625, acc 0.984375\n",
      "2017-01-11T02:19:52.252053: step 8875, loss 0.00221299, acc 1\n",
      "2017-01-11T02:19:54.302952: step 8876, loss 0.00393386, acc 1\n",
      "2017-01-11T02:19:56.377404: step 8877, loss 0.0632784, acc 0.984375\n",
      "2017-01-11T02:19:58.379107: step 8878, loss 0.0533001, acc 0.984375\n",
      "2017-01-11T02:20:00.448240: step 8879, loss 0.00748808, acc 1\n",
      "2017-01-11T02:20:02.608935: step 8880, loss 0.00782971, acc 1\n",
      "2017-01-11T02:20:04.701812: step 8881, loss 0.0104861, acc 0.992188\n",
      "2017-01-11T02:20:06.758183: step 8882, loss 0.00256364, acc 1\n",
      "2017-01-11T02:20:08.769245: step 8883, loss 0.00883391, acc 1\n",
      "2017-01-11T02:20:10.811399: step 8884, loss 0.0139794, acc 1\n",
      "2017-01-11T02:20:12.862132: step 8885, loss 0.000849909, acc 1\n",
      "2017-01-11T02:20:14.878639: step 8886, loss 0.0377251, acc 0.992188\n",
      "2017-01-11T02:20:16.928654: step 8887, loss 0.0408433, acc 0.984375\n",
      "2017-01-11T02:20:19.286989: step 8888, loss 0.000138482, acc 1\n",
      "2017-01-11T02:20:21.380172: step 8889, loss 0.0218083, acc 0.992188\n",
      "2017-01-11T02:20:23.429028: step 8890, loss 0.00682347, acc 1\n",
      "2017-01-11T02:20:25.513019: step 8891, loss 0.0150845, acc 0.992188\n",
      "2017-01-11T02:20:27.541455: step 8892, loss 0.00763492, acc 1\n",
      "2017-01-11T02:20:29.981465: step 8893, loss 0.00207033, acc 1\n",
      "2017-01-11T02:20:32.167477: step 8894, loss 0.0112005, acc 1\n",
      "2017-01-11T02:20:34.221631: step 8895, loss 0.000126326, acc 1\n",
      "2017-01-11T02:20:36.289354: step 8896, loss 0.00491143, acc 1\n",
      "2017-01-11T02:20:38.324128: step 8897, loss 0.00140547, acc 1\n",
      "2017-01-11T02:20:40.368424: step 8898, loss 0.0116763, acc 0.992188\n",
      "2017-01-11T02:20:42.424762: step 8899, loss 0.0145945, acc 1\n",
      "2017-01-11T02:20:44.478860: step 8900, loss 0.00141245, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:21:21.534448: step 8900, loss 0.0684658, acc 0.98316\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-8900\n",
      "\n",
      "2017-01-11T02:21:27.100406: step 8901, loss 0.00530454, acc 1\n",
      "2017-01-11T02:21:29.401602: step 8902, loss 0.0155475, acc 0.992188\n",
      "2017-01-11T02:21:31.565007: step 8903, loss 0.00287814, acc 1\n",
      "2017-01-11T02:21:33.785879: step 8904, loss 0.033701, acc 0.984375\n",
      "2017-01-11T02:21:35.917835: step 8905, loss 0.0278295, acc 0.992188\n",
      "2017-01-11T02:21:38.013709: step 8906, loss 0.00612275, acc 1\n",
      "2017-01-11T02:21:40.066279: step 8907, loss 0.0427117, acc 0.984375\n",
      "2017-01-11T02:21:42.194203: step 8908, loss 0.0110464, acc 0.992188\n",
      "2017-01-11T02:21:44.239419: step 8909, loss 0.0190931, acc 0.992188\n",
      "2017-01-11T02:21:46.312768: step 8910, loss 0.00290165, acc 1\n",
      "2017-01-11T02:21:48.338141: step 8911, loss 0.0645907, acc 0.976562\n",
      "2017-01-11T02:21:50.398025: step 8912, loss 0.0157231, acc 0.992188\n",
      "2017-01-11T02:21:52.513590: step 8913, loss 0.021041, acc 0.992188\n",
      "2017-01-11T02:21:54.530248: step 8914, loss 0.0226613, acc 0.992188\n",
      "2017-01-11T02:21:56.589552: step 8915, loss 0.000738505, acc 1\n",
      "2017-01-11T02:21:59.198622: step 8916, loss 0.0208475, acc 0.992188\n",
      "2017-01-11T02:22:01.200520: step 8917, loss 0.0242531, acc 0.984375\n",
      "2017-01-11T02:22:03.299346: step 8918, loss 0.036719, acc 0.992188\n",
      "2017-01-11T02:22:05.366380: step 8919, loss 0.0177134, acc 0.992188\n",
      "2017-01-11T02:22:07.392620: step 8920, loss 0.00688986, acc 1\n",
      "2017-01-11T02:22:09.430897: step 8921, loss 0.0231868, acc 0.984375\n",
      "2017-01-11T02:22:11.476771: step 8922, loss 0.024074, acc 0.992188\n",
      "2017-01-11T02:22:13.577088: step 8923, loss 0.045664, acc 0.992188\n",
      "2017-01-11T02:22:16.590532: step 8924, loss 0.0134439, acc 1\n",
      "2017-01-11T02:22:18.573535: step 8925, loss 0.00483365, acc 1\n",
      "2017-01-11T02:22:20.678376: step 8926, loss 0.0184664, acc 1\n",
      "2017-01-11T02:22:22.758603: step 8927, loss 0.0102559, acc 1\n",
      "2017-01-11T02:22:24.808003: step 8928, loss 0.0557666, acc 0.984375\n",
      "2017-01-11T02:22:26.888687: step 8929, loss 0.0208258, acc 0.992188\n",
      "2017-01-11T02:22:28.956270: step 8930, loss 0.0423872, acc 0.992188\n",
      "2017-01-11T02:22:31.425698: step 8931, loss 0.0577288, acc 0.984375\n",
      "2017-01-11T02:22:33.423868: step 8932, loss 0.0100352, acc 0.992188\n",
      "2017-01-11T02:22:35.489585: step 8933, loss 0.000401745, acc 1\n",
      "2017-01-11T02:22:37.525490: step 8934, loss 0.0131749, acc 0.992188\n",
      "2017-01-11T02:22:39.568799: step 8935, loss 0.00211151, acc 1\n",
      "2017-01-11T02:22:41.618190: step 8936, loss 3.72483e-05, acc 1\n",
      "2017-01-11T02:22:43.663186: step 8937, loss 0.0318476, acc 0.992188\n",
      "2017-01-11T02:22:45.645799: step 8938, loss 0.0345842, acc 0.984375\n",
      "2017-01-11T02:22:47.661213: step 8939, loss 0.0121238, acc 0.992188\n",
      "2017-01-11T02:22:49.754737: step 8940, loss 0.00404264, acc 1\n",
      "2017-01-11T02:22:51.813407: step 8941, loss 0.00207809, acc 1\n",
      "2017-01-11T02:22:53.855941: step 8942, loss 0.0982938, acc 0.984375\n",
      "2017-01-11T02:22:56.003407: step 8943, loss 0.0268448, acc 0.992188\n",
      "2017-01-11T02:22:58.067090: step 8944, loss 0.00236118, acc 1\n",
      "2017-01-11T02:23:00.087519: step 8945, loss 0.00133733, acc 1\n",
      "2017-01-11T02:23:02.357578: step 8946, loss 0.0606854, acc 0.984375\n",
      "2017-01-11T02:23:04.883481: step 8947, loss 0.00270711, acc 1\n",
      "2017-01-11T02:23:06.896834: step 8948, loss 0.0429593, acc 0.984375\n",
      "2017-01-11T02:23:08.959803: step 8949, loss 0.0229631, acc 0.984375\n",
      "2017-01-11T02:23:10.991786: step 8950, loss 0.0017631, acc 1\n",
      "2017-01-11T02:23:12.975725: step 8951, loss 0.0486411, acc 0.992188\n",
      "2017-01-11T02:23:15.017487: step 8952, loss 0.0256932, acc 0.992188\n",
      "2017-01-11T02:23:17.103671: step 8953, loss 0.00583836, acc 1\n",
      "2017-01-11T02:23:19.172258: step 8954, loss 0.041798, acc 0.984375\n",
      "2017-01-11T02:23:22.391939: step 8955, loss 0.0134909, acc 1\n",
      "2017-01-11T02:23:24.396301: step 8956, loss 0.0309251, acc 0.992188\n",
      "2017-01-11T02:23:26.436210: step 8957, loss 0.00126455, acc 1\n",
      "2017-01-11T02:23:28.501283: step 8958, loss 0.0162146, acc 1\n",
      "2017-01-11T02:23:30.578038: step 8959, loss 0.019847, acc 0.992188\n",
      "2017-01-11T02:23:32.568036: step 8960, loss 0.00990423, acc 1\n",
      "2017-01-11T02:23:34.971531: step 8961, loss 0.0162887, acc 1\n",
      "2017-01-11T02:23:37.007633: step 8962, loss 0.012305, acc 1\n",
      "2017-01-11T02:23:39.049004: step 8963, loss 0.0166096, acc 0.992188\n",
      "2017-01-11T02:23:41.086268: step 8964, loss 0.0104283, acc 1\n",
      "2017-01-11T02:23:43.110335: step 8965, loss 0.0332034, acc 0.984375\n",
      "2017-01-11T02:23:45.145376: step 8966, loss 0.0219693, acc 0.992188\n",
      "2017-01-11T02:23:47.200781: step 8967, loss 0.0168524, acc 0.992188\n",
      "2017-01-11T02:23:49.226382: step 8968, loss 0.0666315, acc 0.984375\n",
      "2017-01-11T02:23:51.312536: step 8969, loss 0.0279797, acc 0.992188\n",
      "2017-01-11T02:23:53.306468: step 8970, loss 0.0049439, acc 1\n",
      "2017-01-11T02:23:55.379650: step 8971, loss 0.000109809, acc 1\n",
      "2017-01-11T02:23:57.420083: step 8972, loss 0.0169295, acc 1\n",
      "2017-01-11T02:23:59.477832: step 8973, loss 0.0366559, acc 0.992188\n",
      "2017-01-11T02:24:01.520026: step 8974, loss 0.000229138, acc 1\n",
      "2017-01-11T02:24:03.577306: step 8975, loss 0.00189079, acc 1\n",
      "2017-01-11T02:24:05.943164: step 8976, loss 0.0457932, acc 0.976562\n",
      "2017-01-11T02:24:08.109579: step 8977, loss 0.0564586, acc 0.992188\n",
      "2017-01-11T02:24:10.169554: step 8978, loss 0.00013985, acc 1\n",
      "2017-01-11T02:24:12.217621: step 8979, loss 0.0443497, acc 0.992188\n",
      "2017-01-11T02:24:14.282752: step 8980, loss 0.00428982, acc 1\n",
      "2017-01-11T02:24:16.318168: step 8981, loss 0.0100473, acc 1\n",
      "2017-01-11T02:24:18.377036: step 8982, loss 0.0230383, acc 0.992188\n",
      "2017-01-11T02:24:20.438413: step 8983, loss 0.00246298, acc 1\n",
      "2017-01-11T02:24:22.565769: step 8984, loss 0.0231016, acc 0.992188\n",
      "2017-01-11T02:24:24.607251: step 8985, loss 0.0819477, acc 0.984375\n",
      "2017-01-11T02:24:26.664113: step 8986, loss 0.0215389, acc 0.992188\n",
      "2017-01-11T02:24:28.736667: step 8987, loss 0.015768, acc 1\n",
      "2017-01-11T02:24:30.868299: step 8988, loss 0.000754769, acc 1\n",
      "2017-01-11T02:24:32.870377: step 8989, loss 0.00177766, acc 1\n",
      "2017-01-11T02:24:34.942514: step 8990, loss 0.0147317, acc 1\n",
      "2017-01-11T02:24:37.007935: step 8991, loss 0.0173992, acc 1\n",
      "2017-01-11T02:24:39.342985: step 8992, loss 0.0317481, acc 0.992188\n",
      "2017-01-11T02:24:41.387107: step 8993, loss 0.011207, acc 0.992188\n",
      "2017-01-11T02:24:43.399208: step 8994, loss 0.0162592, acc 0.992188\n",
      "2017-01-11T02:24:45.428293: step 8995, loss 0.00308429, acc 1\n",
      "2017-01-11T02:24:47.463715: step 8996, loss 0.00986643, acc 1\n",
      "2017-01-11T02:24:49.516448: step 8997, loss 0.000911831, acc 1\n",
      "2017-01-11T02:24:51.601679: step 8998, loss 0.0170209, acc 0.992188\n",
      "2017-01-11T02:24:53.667767: step 8999, loss 0.000473454, acc 1\n",
      "2017-01-11T02:24:55.701361: step 9000, loss 0.000161147, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:25:25.215193: step 9000, loss 0.068267, acc 0.98324\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9000\n",
      "\n",
      "2017-01-11T02:25:29.954067: step 9001, loss 0.0106162, acc 1\n",
      "2017-01-11T02:25:31.948933: step 9002, loss 0.177931, acc 0.992188\n",
      "2017-01-11T02:25:34.011216: step 9003, loss 0.0149833, acc 1\n",
      "2017-01-11T02:25:36.080631: step 9004, loss 0.000210557, acc 1\n",
      "2017-01-11T02:25:38.126643: step 9005, loss 0.00166331, acc 1\n",
      "2017-01-11T02:25:40.173365: step 9006, loss 0.0256229, acc 0.984375\n",
      "2017-01-11T02:25:42.225018: step 9007, loss 0.0135689, acc 1\n",
      "2017-01-11T02:25:45.401032: step 9008, loss 0.0170004, acc 0.992188\n",
      "2017-01-11T02:25:47.740383: step 9009, loss 0.0153719, acc 1\n",
      "2017-01-11T02:25:49.805938: step 9010, loss 0.000993844, acc 1\n",
      "2017-01-11T02:25:51.858880: step 9011, loss 0.000745199, acc 1\n",
      "2017-01-11T02:25:53.937006: step 9012, loss 0.0491964, acc 0.992188\n",
      "2017-01-11T02:25:56.018596: step 9013, loss 0.000591323, acc 1\n",
      "2017-01-11T02:25:58.044329: step 9014, loss 0.00448388, acc 1\n",
      "2017-01-11T02:26:00.061647: step 9015, loss 0.00162377, acc 1\n",
      "2017-01-11T02:26:02.090052: step 9016, loss 0.00916284, acc 0.992188\n",
      "2017-01-11T02:26:04.143656: step 9017, loss 0.0896774, acc 0.992188\n",
      "2017-01-11T02:26:06.280250: step 9018, loss 0.0517557, acc 0.992188\n",
      "2017-01-11T02:26:08.326822: step 9019, loss 0.021095, acc 0.992188\n",
      "2017-01-11T02:26:10.371031: step 9020, loss 0.00886802, acc 1\n",
      "2017-01-11T02:26:12.416930: step 9021, loss 0.00216588, acc 1\n",
      "2017-01-11T02:26:14.445488: step 9022, loss 0.104434, acc 0.992188\n",
      "2017-01-11T02:26:16.841204: step 9023, loss 0.0214765, acc 0.992188\n",
      "2017-01-11T02:26:18.885646: step 9024, loss 0.00451794, acc 1\n",
      "2017-01-11T02:26:20.961181: step 9025, loss 0.0218826, acc 0.992188\n",
      "2017-01-11T02:26:23.060902: step 9026, loss 0.00764718, acc 0.992188\n",
      "2017-01-11T02:26:25.109017: step 9027, loss 0.000665249, acc 1\n",
      "2017-01-11T02:26:27.160203: step 9028, loss 0.00147092, acc 1\n",
      "2017-01-11T02:26:29.221943: step 9029, loss 0.00842889, acc 0.992188\n",
      "2017-01-11T02:26:31.356985: step 9030, loss 0.0539439, acc 0.984375\n",
      "2017-01-11T02:26:33.406284: step 9031, loss 0.0192801, acc 0.992188\n",
      "2017-01-11T02:26:35.499509: step 9032, loss 0.022227, acc 0.992188\n",
      "2017-01-11T02:26:37.523861: step 9033, loss 0.00954567, acc 0.992188\n",
      "2017-01-11T02:26:39.567838: step 9034, loss 0.000411708, acc 1\n",
      "2017-01-11T02:26:41.591591: step 9035, loss 0.0524114, acc 0.984375\n",
      "2017-01-11T02:26:43.621348: step 9036, loss 0.00399964, acc 1\n",
      "2017-01-11T02:26:45.639043: step 9037, loss 0.0343223, acc 0.984375\n",
      "2017-01-11T02:26:47.966580: step 9038, loss 0.0391326, acc 0.992188\n",
      "2017-01-11T02:26:49.985940: step 9039, loss 0.000254988, acc 1\n",
      "2017-01-11T02:26:52.047182: step 9040, loss 0.00525694, acc 1\n",
      "2017-01-11T02:26:54.065561: step 9041, loss 0.00450563, acc 1\n",
      "2017-01-11T02:26:56.116114: step 9042, loss 0.0250237, acc 0.992188\n",
      "2017-01-11T02:26:58.172555: step 9043, loss 0.000224545, acc 1\n",
      "2017-01-11T02:27:00.200001: step 9044, loss 0.0570411, acc 0.984375\n",
      "2017-01-11T02:27:02.233415: step 9045, loss 0.0674969, acc 0.992188\n",
      "2017-01-11T02:27:04.449846: step 9046, loss 0.00879346, acc 1\n",
      "2017-01-11T02:27:06.591530: step 9047, loss 0.00243992, acc 1\n",
      "2017-01-11T02:27:08.612520: step 9048, loss 0.0172791, acc 0.992188\n",
      "2017-01-11T02:27:10.649846: step 9049, loss 0.063008, acc 0.976562\n",
      "2017-01-11T02:27:12.704572: step 9050, loss 0.0215228, acc 0.992188\n",
      "2017-01-11T02:27:14.750925: step 9051, loss 0.0221188, acc 0.992188\n",
      "2017-01-11T02:27:16.813052: step 9052, loss 0.0146304, acc 1\n",
      "2017-01-11T02:27:19.059271: step 9053, loss 0.0274379, acc 0.992188\n",
      "2017-01-11T02:27:22.328091: step 9054, loss 0.0273252, acc 0.992188\n",
      "2017-01-11T02:27:24.380012: step 9055, loss 0.00066754, acc 1\n",
      "2017-01-11T02:27:26.393581: step 9056, loss 0.0134519, acc 0.992188\n",
      "2017-01-11T02:27:28.439128: step 9057, loss 0.00122803, acc 1\n",
      "2017-01-11T02:27:30.487813: step 9058, loss 0.0313676, acc 0.992188\n",
      "2017-01-11T02:27:32.503828: step 9059, loss 0.0294499, acc 0.992188\n",
      "2017-01-11T02:27:34.602106: step 9060, loss 0.0291454, acc 0.992188\n",
      "2017-01-11T02:27:36.646630: step 9061, loss 0.00194615, acc 1\n",
      "2017-01-11T02:27:38.674880: step 9062, loss 0.00972167, acc 1\n",
      "2017-01-11T02:27:40.724799: step 9063, loss 0.0265035, acc 0.992188\n",
      "2017-01-11T02:27:42.763998: step 9064, loss 0.00187319, acc 1\n",
      "2017-01-11T02:27:44.794241: step 9065, loss 0.000171795, acc 1\n",
      "2017-01-11T02:27:46.824412: step 9066, loss 0.0458877, acc 0.984375\n",
      "2017-01-11T02:27:48.883041: step 9067, loss 0.00439748, acc 1\n",
      "2017-01-11T02:27:51.007045: step 9068, loss 0.019836, acc 0.992188\n",
      "2017-01-11T02:27:53.288884: step 9069, loss 0.0102836, acc 0.992188\n",
      "2017-01-11T02:27:55.358616: step 9070, loss 0.00941691, acc 1\n",
      "2017-01-11T02:27:57.361165: step 9071, loss 0.00789041, acc 1\n",
      "2017-01-11T02:27:59.409872: step 9072, loss 0.000224043, acc 1\n",
      "2017-01-11T02:28:01.442968: step 9073, loss 0.00420499, acc 1\n",
      "2017-01-11T02:28:03.444471: step 9074, loss 0.00383974, acc 1\n",
      "2017-01-11T02:28:05.545254: step 9075, loss 0.00211238, acc 1\n",
      "2017-01-11T02:28:07.673204: step 9076, loss 0.0112906, acc 0.992188\n",
      "2017-01-11T02:28:09.876756: step 9077, loss 0.0060038, acc 1\n",
      "2017-01-11T02:28:11.908903: step 9078, loss 0.0247112, acc 0.992188\n",
      "2017-01-11T02:28:13.938014: step 9079, loss 0.0126414, acc 0.992188\n",
      "2017-01-11T02:28:15.981133: step 9080, loss 0.0541802, acc 0.984375\n",
      "2017-01-11T02:28:18.045475: step 9081, loss 0.0053397, acc 1\n",
      "2017-01-11T02:28:20.167224: step 9082, loss 0.00644305, acc 1\n",
      "2017-01-11T02:28:22.260045: step 9083, loss 0.0142877, acc 0.992188\n",
      "2017-01-11T02:28:24.632746: step 9084, loss 0.013665, acc 0.992188\n",
      "2017-01-11T02:28:27.714138: step 9085, loss 0.00166173, acc 1\n",
      "2017-01-11T02:28:29.744887: step 9086, loss 0.0322285, acc 0.992188\n",
      "2017-01-11T02:28:31.887431: step 9087, loss 0.0193378, acc 0.992188\n",
      "2017-01-11T02:28:33.947664: step 9088, loss 0.0174451, acc 0.992188\n",
      "2017-01-11T02:28:36.025532: step 9089, loss 0.00116827, acc 1\n",
      "2017-01-11T02:28:38.028502: step 9090, loss 0.00133358, acc 1\n",
      "2017-01-11T02:28:40.070084: step 9091, loss 0.00622102, acc 1\n",
      "2017-01-11T02:28:42.097585: step 9092, loss 0.0170985, acc 0.992188\n",
      "2017-01-11T02:33:57.491679: step 9206, loss 0.056414, acc 0.984375\n",
      "2017-01-11T02:33:59.594813: step 9207, loss 0.0181292, acc 0.992188\n",
      "2017-01-11T02:34:01.679808: step 9208, loss 0.00501174, acc 1\n",
      "2017-01-11T02:34:03.724153: step 9209, loss 0.00450689, acc 1\n",
      "2017-01-11T02:34:05.801885: step 9210, loss 0.0156791, acc 0.992188\n",
      "2017-01-11T02:34:07.832583: step 9211, loss 0.00715937, acc 1\n",
      "2017-01-11T02:34:09.903620: step 9212, loss 0.00174083, acc 1\n",
      "2017-01-11T02:34:11.910392: step 9213, loss 0.00151172, acc 1\n",
      "2017-01-11T02:34:13.982032: step 9214, loss 0.0221385, acc 0.992188\n",
      "2017-01-11T02:34:16.014099: step 9215, loss 0.100186, acc 0.984375\n",
      "2017-01-11T02:34:18.035619: step 9216, loss 0.00227464, acc 1\n",
      "2017-01-11T02:34:20.308561: step 9217, loss 0.0248769, acc 0.992188\n",
      "2017-01-11T02:34:22.509386: step 9218, loss 0.0289296, acc 0.984375\n",
      "2017-01-11T02:34:24.522812: step 9219, loss 0.0415471, acc 0.992188\n",
      "2017-01-11T02:34:26.578427: step 9220, loss 0.00337757, acc 1\n",
      "2017-01-11T02:34:28.571729: step 9221, loss 0.00299615, acc 1\n",
      "2017-01-11T02:34:30.699033: step 9222, loss 0.053701, acc 0.976562\n",
      "2017-01-11T02:34:32.688709: step 9223, loss 0.0237591, acc 1\n",
      "2017-01-11T02:34:34.780906: step 9224, loss 0.0280439, acc 0.992188\n",
      "2017-01-11T02:34:36.805414: step 9225, loss 0.000227244, acc 1\n",
      "2017-01-11T02:34:38.841129: step 9226, loss 0.0429958, acc 0.992188\n",
      "2017-01-11T02:34:40.915690: step 9227, loss 0.00936522, acc 1\n",
      "2017-01-11T02:34:42.957548: step 9228, loss 0.0194274, acc 0.992188\n",
      "2017-01-11T02:34:45.025683: step 9229, loss 0.00142809, acc 1\n",
      "2017-01-11T02:34:47.059057: step 9230, loss 0.0236784, acc 0.992188\n",
      "2017-01-11T02:34:49.093249: step 9231, loss 8.2522e-05, acc 1\n",
      "2017-01-11T02:34:51.146521: step 9232, loss 0.00116817, acc 1\n",
      "2017-01-11T02:34:53.565942: step 9233, loss 0.0100357, acc 1\n",
      "2017-01-11T02:34:55.600184: step 9234, loss 0.0408732, acc 0.992188\n",
      "2017-01-11T02:34:57.659639: step 9235, loss 0.0040082, acc 1\n",
      "2017-01-11T02:34:59.706127: step 9236, loss 0.001083, acc 1\n",
      "2017-01-11T02:35:01.746363: step 9237, loss 0.000168617, acc 1\n",
      "2017-01-11T02:35:03.789984: step 9238, loss 4.5534e-05, acc 1\n",
      "2017-01-11T02:35:05.865028: step 9239, loss 0.0588904, acc 0.976562\n",
      "2017-01-11T02:35:07.907602: step 9240, loss 0.00675648, acc 1\n",
      "2017-01-11T02:35:10.004481: step 9241, loss 5.76906e-05, acc 1\n",
      "2017-01-11T02:35:12.028494: step 9242, loss 0.0168058, acc 0.992188\n",
      "2017-01-11T02:35:14.030909: step 9243, loss 0.0225424, acc 0.992188\n",
      "2017-01-11T02:35:16.038968: step 9244, loss 0.00296726, acc 1\n",
      "2017-01-11T02:35:18.057190: step 9245, loss 0.0945545, acc 0.984375\n",
      "2017-01-11T02:35:20.120020: step 9246, loss 0.0166215, acc 1\n",
      "2017-01-11T02:35:22.216718: step 9247, loss 0.00857043, acc 0.992188\n",
      "2017-01-11T02:35:24.552902: step 9248, loss 0.00365757, acc 1\n",
      "2017-01-11T02:35:26.659007: step 9249, loss 0.0221971, acc 0.992188\n",
      "2017-01-11T02:35:28.927162: step 9250, loss 0.000176661, acc 1\n",
      "2017-01-11T02:35:30.968020: step 9251, loss 0.0203628, acc 0.992188\n",
      "2017-01-11T02:35:32.957681: step 9252, loss 0.0112022, acc 0.992188\n",
      "2017-01-11T02:35:35.046191: step 9253, loss 0.00385709, acc 1\n",
      "2017-01-11T02:35:37.095037: step 9254, loss 0.000406078, acc 1\n",
      "2017-01-11T02:35:39.140954: step 9255, loss 0.00308679, acc 1\n",
      "2017-01-11T02:35:41.205596: step 9256, loss 0.0546685, acc 0.992188\n",
      "2017-01-11T02:35:43.274100: step 9257, loss 0.0189606, acc 0.992188\n",
      "2017-01-11T02:35:46.428185: step 9258, loss 0.0320314, acc 0.984375\n",
      "2017-01-11T02:35:48.441979: step 9259, loss 0.0256266, acc 0.992188\n",
      "2017-01-11T02:35:50.525135: step 9260, loss 1.07939e-05, acc 1\n",
      "2017-01-11T02:35:52.603226: step 9261, loss 0.00402426, acc 1\n",
      "2017-01-11T02:35:54.636365: step 9262, loss 0.123992, acc 0.976562\n",
      "2017-01-11T02:35:57.006975: step 9263, loss 0.0259891, acc 0.992188\n",
      "2017-01-11T02:35:59.014571: step 9264, loss 0.0296246, acc 0.984375\n",
      "2017-01-11T02:36:01.105811: step 9265, loss 0.0189571, acc 0.992188\n",
      "2017-01-11T02:36:03.175218: step 9266, loss 0.00042756, acc 1\n",
      "2017-01-11T02:36:05.266556: step 9267, loss 0.0547988, acc 0.976562\n",
      "2017-01-11T02:36:07.296215: step 9268, loss 0.00909005, acc 1\n",
      "2017-01-11T02:36:09.326845: step 9269, loss 0.000658081, acc 1\n",
      "2017-01-11T02:36:11.486103: step 9270, loss 0.0201836, acc 1\n",
      "2017-01-11T02:36:13.482157: step 9271, loss 0.0347961, acc 0.992188\n",
      "2017-01-11T02:36:15.523911: step 9272, loss 0.00292945, acc 1\n",
      "2017-01-11T02:36:17.581912: step 9273, loss 0.00551188, acc 1\n",
      "2017-01-11T02:36:19.652888: step 9274, loss 0.00775052, acc 0.992188\n",
      "2017-01-11T02:36:21.755292: step 9275, loss 0.000633082, acc 1\n",
      "2017-01-11T02:36:23.807682: step 9276, loss 0.0147047, acc 0.992188\n",
      "2017-01-11T02:36:25.842889: step 9277, loss 0.0337444, acc 0.984375\n",
      "2017-01-11T02:36:28.034508: step 9278, loss 0.00629371, acc 1\n",
      "2017-01-11T02:36:30.184116: step 9279, loss 0.108249, acc 0.976562\n",
      "2017-01-11T02:36:32.386377: step 9280, loss 0.00655515, acc 1\n",
      "2017-01-11T02:36:34.427782: step 9281, loss 0.0492517, acc 0.984375\n",
      "2017-01-11T02:36:36.509319: step 9282, loss 0.00971097, acc 1\n",
      "2017-01-11T02:36:38.542231: step 9283, loss 0.0229495, acc 0.992188\n",
      "2017-01-11T02:36:40.560822: step 9284, loss 0.0363799, acc 0.992188\n",
      "2017-01-11T02:36:42.595467: step 9285, loss 0.0987669, acc 0.992188\n",
      "2017-01-11T02:36:44.652355: step 9286, loss 0.000848602, acc 1\n",
      "2017-01-11T02:36:46.693923: step 9287, loss 0.0456794, acc 0.984375\n",
      "2017-01-11T02:36:48.730446: step 9288, loss 0.000657993, acc 1\n",
      "2017-01-11T02:36:50.823125: step 9289, loss 0.0588315, acc 0.992188\n",
      "2017-01-11T02:36:52.919566: step 9290, loss 0.00255641, acc 1\n",
      "2017-01-11T02:36:54.952161: step 9291, loss 0.0195494, acc 0.992188\n",
      "2017-01-11T02:36:57.018877: step 9292, loss 0.0329807, acc 0.992188\n",
      "2017-01-11T02:36:59.051941: step 9293, loss 0.0226873, acc 0.992188\n",
      "2017-01-11T02:37:01.441054: step 9294, loss 0.0077805, acc 1\n",
      "2017-01-11T02:37:03.479489: step 9295, loss 0.00734156, acc 1\n",
      "2017-01-11T02:37:05.598639: step 9296, loss 0.0225707, acc 0.984375\n",
      "2017-01-11T02:37:07.663400: step 9297, loss 0.000404667, acc 1\n",
      "2017-01-11T02:37:09.688482: step 9298, loss 0.02312, acc 0.992188\n",
      "2017-01-11T02:37:11.845488: step 9299, loss 0.0129507, acc 0.992188\n",
      "2017-01-11T02:37:13.914856: step 9300, loss 0.0350194, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:37:54.771396: step 9300, loss 0.0701947, acc 0.98316\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9300\n",
      "\n",
      "2017-01-11T02:38:00.055166: step 9301, loss 5.18374e-05, acc 1\n",
      "2017-01-11T02:38:02.093692: step 9302, loss 0.031548, acc 0.984375\n",
      "2017-01-11T02:38:04.160120: step 9303, loss 0.022236, acc 0.992188\n",
      "2017-01-11T02:38:06.256013: step 9304, loss 0.0100707, acc 1\n",
      "2017-01-11T02:38:08.559490: step 9305, loss 0.0338676, acc 0.984375\n",
      "2017-01-11T02:38:10.898723: step 9306, loss 0.0106389, acc 0.992188\n",
      "2017-01-11T02:38:13.365198: step 9307, loss 0.000594647, acc 1\n",
      "2017-01-11T02:38:15.494251: step 9308, loss 0.0262708, acc 0.992188\n",
      "2017-01-11T02:38:17.596740: step 9309, loss 0.0269987, acc 0.992188\n",
      "2017-01-11T02:38:19.673791: step 9310, loss 0.000457888, acc 1\n",
      "2017-01-11T02:38:21.952747: step 9311, loss 0.00446559, acc 1\n",
      "2017-01-11T02:38:24.000443: step 9312, loss 0.0135775, acc 0.992188\n",
      "2017-01-11T02:38:26.041784: step 9313, loss 0.0583128, acc 0.984375\n",
      "2017-01-11T02:38:28.078820: step 9314, loss 0.0553612, acc 0.96875\n",
      "2017-01-11T02:38:30.160152: step 9315, loss 0.000263195, acc 1\n",
      "2017-01-11T02:38:32.475395: step 9316, loss 0.0359113, acc 0.992188\n",
      "2017-01-11T02:38:34.623165: step 9317, loss 0.0157358, acc 1\n",
      "2017-01-11T02:38:36.648955: step 9318, loss 0.0214975, acc 0.992188\n",
      "2017-01-11T02:38:39.617809: step 9319, loss 0.00104419, acc 1\n",
      "2017-01-11T02:38:42.160674: step 9320, loss 0.0251089, acc 0.992188\n",
      "2017-01-11T02:38:44.186299: step 9321, loss 0.0110889, acc 1\n",
      "2017-01-11T02:38:46.237523: step 9322, loss 0.00443141, acc 1\n",
      "2017-01-11T02:38:48.278899: step 9323, loss 0.0592539, acc 0.984375\n",
      "2017-01-11T02:38:50.394418: step 9324, loss 0.00172012, acc 1\n",
      "2017-01-11T02:38:52.465437: step 9325, loss 0.000362233, acc 1\n",
      "2017-01-11T02:38:54.539803: step 9326, loss 0.000639365, acc 1\n",
      "2017-01-11T02:38:56.584309: step 9327, loss 0.0098928, acc 1\n",
      "2017-01-11T02:38:58.661471: step 9328, loss 0.0344023, acc 0.992188\n",
      "2017-01-11T02:39:00.723066: step 9329, loss 0.0440048, acc 0.984375\n",
      "2017-01-11T02:39:02.774384: step 9330, loss 0.0292552, acc 0.984375\n",
      "2017-01-11T02:39:04.837355: step 9331, loss 0.0204705, acc 0.992188\n",
      "2017-01-11T02:39:06.843690: step 9332, loss 0.0531202, acc 0.984375\n",
      "2017-01-11T02:39:08.901011: step 9333, loss 0.0347944, acc 0.992188\n",
      "2017-01-11T02:39:10.953138: step 9334, loss 0.0374957, acc 0.984375\n",
      "2017-01-11T02:39:13.214576: step 9335, loss 0.0676593, acc 0.984375\n",
      "2017-01-11T02:39:15.416093: step 9336, loss 0.0253446, acc 0.984375\n",
      "2017-01-11T02:39:17.482895: step 9337, loss 0.0241004, acc 0.992188\n",
      "2017-01-11T02:39:19.527480: step 9338, loss 0.00600025, acc 1\n",
      "2017-01-11T02:39:21.616500: step 9339, loss 0.0255334, acc 0.992188\n",
      "2017-01-11T02:39:23.649799: step 9340, loss 0.0421644, acc 0.992188\n",
      "2017-01-11T02:39:25.697829: step 9341, loss 0.00909339, acc 0.992188\n",
      "2017-01-11T02:39:27.746090: step 9342, loss 0.0140819, acc 0.992188\n",
      "2017-01-11T02:39:29.772488: step 9343, loss 0.000229296, acc 1\n",
      "2017-01-11T02:39:31.805714: step 9344, loss 0.0404029, acc 0.984375\n",
      "2017-01-11T02:39:33.822586: step 9345, loss 0.0264259, acc 0.992188\n",
      "2017-01-11T02:39:35.894591: step 9346, loss 0.0300656, acc 0.984375\n",
      "2017-01-11T02:39:37.954711: step 9347, loss 0.0208129, acc 0.992188\n",
      "2017-01-11T02:39:39.995718: step 9348, loss 0.0130034, acc 0.992188\n",
      "2017-01-11T02:39:42.038794: step 9349, loss 0.00386636, acc 1\n",
      "2017-01-11T02:39:44.051243: step 9350, loss 0.0255379, acc 0.992188\n",
      "2017-01-11T02:39:46.415496: step 9351, loss 0.0246507, acc 1\n",
      "2017-01-11T02:39:48.451238: step 9352, loss 0.0054277, acc 1\n",
      "2017-01-11T02:39:50.520480: step 9353, loss 0.00149976, acc 1\n",
      "2017-01-11T02:39:52.627735: step 9354, loss 0.0190054, acc 0.992188\n",
      "2017-01-11T02:39:54.705986: step 9355, loss 0.0110925, acc 1\n",
      "2017-01-11T02:39:56.769273: step 9356, loss 0.011134, acc 0.992188\n",
      "2017-01-11T02:39:58.800412: step 9357, loss 0.00181595, acc 1\n",
      "2017-01-11T02:40:00.848302: step 9358, loss 0.00683201, acc 1\n",
      "2017-01-11T02:40:02.931813: step 9359, loss 0.00130636, acc 1\n",
      "2017-01-11T02:40:04.997354: step 9360, loss 0.0137323, acc 0.992188\n",
      "2017-01-11T02:40:06.966020: step 9361, loss 0.000482331, acc 1\n",
      "2017-01-11T02:40:09.010615: step 9362, loss 0.0255328, acc 0.992188\n",
      "2017-01-11T02:40:11.041822: step 9363, loss 0.000679199, acc 1\n",
      "2017-01-11T02:40:13.123688: step 9364, loss 0.0469614, acc 0.984375\n",
      "2017-01-11T02:40:15.180941: step 9365, loss 0.0188717, acc 0.992188\n",
      "2017-01-11T02:40:17.437481: step 9366, loss 0.00264517, acc 1\n",
      "2017-01-11T02:40:19.599563: step 9367, loss 0.03612, acc 0.984375\n",
      "2017-01-11T02:40:21.693143: step 9368, loss 0.00709326, acc 1\n",
      "2017-01-11T02:40:23.740900: step 9369, loss 0.0159608, acc 0.992188\n",
      "2017-01-11T02:40:25.776271: step 9370, loss 1.24332e-05, acc 1\n",
      "2017-01-11T02:40:27.801590: step 9371, loss 0.0036286, acc 1\n",
      "2017-01-11T02:40:30.228013: step 9372, loss 0.0338043, acc 0.984375\n",
      "2017-01-11T02:40:32.351364: step 9373, loss 0.0148579, acc 1\n",
      "2017-01-11T02:40:34.407689: step 9374, loss 0.0446898, acc 0.984375\n",
      "2017-01-11T02:40:36.437853: step 9375, loss 0.0178195, acc 0.992188\n",
      "2017-01-11T02:40:38.463302: step 9376, loss 0.0143115, acc 1\n",
      "2017-01-11T02:40:40.541747: step 9377, loss 0.0303331, acc 0.984375\n",
      "2017-01-11T02:40:42.576636: step 9378, loss 0.00396629, acc 1\n",
      "2017-01-11T02:40:44.615398: step 9379, loss 0.00013674, acc 1\n",
      "2017-01-11T02:40:47.640386: step 9380, loss 0.128548, acc 0.984375\n",
      "2017-01-11T02:40:50.174165: step 9381, loss 0.0211254, acc 0.992188\n",
      "2017-01-11T02:40:52.229670: step 9382, loss 0.00507048, acc 1\n",
      "2017-01-11T02:40:54.281767: step 9383, loss 0.0023592, acc 1\n",
      "2017-01-11T02:40:56.293746: step 9384, loss 0.00706961, acc 1\n",
      "2017-01-11T02:40:58.315149: step 9385, loss 0.0548644, acc 0.984375\n",
      "2017-01-11T02:41:00.388747: step 9386, loss 0.0266145, acc 1\n",
      "2017-01-11T02:41:02.768661: step 9387, loss 0.0023334, acc 1\n",
      "2017-01-11T02:41:05.227592: step 9388, loss 0.0287724, acc 0.992188\n",
      "2017-01-11T02:41:07.310718: step 9389, loss 0.00222795, acc 1\n",
      "2017-01-11T02:41:10.336834: step 9390, loss 0.0409856, acc 0.984375\n",
      "2017-01-11T02:41:12.866951: step 9391, loss 0.0040387, acc 1\n",
      "2017-01-11T02:41:14.889427: step 9392, loss 0.031922, acc 0.992188\n",
      "2017-01-11T02:41:16.940405: step 9393, loss 0.00306047, acc 1\n",
      "2017-01-11T02:41:18.989052: step 9394, loss 0.00424132, acc 1\n",
      "2017-01-11T02:41:21.298650: step 9395, loss 0.00632173, acc 1\n",
      "2017-01-11T02:41:23.449681: step 9396, loss 0.00665901, acc 1\n",
      "2017-01-11T02:41:25.497347: step 9397, loss 0.00385476, acc 1\n",
      "2017-01-11T02:41:27.531176: step 9398, loss 0.043181, acc 0.992188\n",
      "2017-01-11T02:41:29.543427: step 9399, loss 0.0323957, acc 0.984375\n",
      "2017-01-11T02:41:31.605006: step 9400, loss 0.0431458, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:42:08.887938: step 9400, loss 0.0701359, acc 0.98328\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9400\n",
      "\n",
      "2017-01-11T02:42:14.380938: step 9401, loss 0.0176969, acc 1\n",
      "2017-01-11T02:42:16.390790: step 9402, loss 0.00274058, acc 1\n",
      "2017-01-11T02:42:18.460907: step 9403, loss 0.0573188, acc 0.984375\n",
      "2017-01-11T02:42:20.543035: step 9404, loss 0.0104364, acc 0.992188\n",
      "2017-01-11T02:42:22.965145: step 9405, loss 0.0163503, acc 0.992188\n",
      "2017-01-11T02:42:25.116093: step 9406, loss 0.0207312, acc 0.992188\n",
      "2017-01-11T02:42:27.238476: step 9407, loss 0.0131588, acc 1\n",
      "2017-01-11T02:42:29.686663: step 9408, loss 0.00815277, acc 1\n",
      "2017-01-11T02:42:31.954452: step 9409, loss 0.000536558, acc 1\n",
      "2017-01-11T02:42:34.034132: step 9410, loss 0.000129251, acc 1\n",
      "2017-01-11T02:42:36.133213: step 9411, loss 0.0244459, acc 0.992188\n",
      "2017-01-11T02:42:39.080787: step 9412, loss 0.0196566, acc 1\n",
      "2017-01-11T02:42:41.337645: step 9413, loss 0.00144876, acc 1\n",
      "2017-01-11T02:42:43.409174: step 9414, loss 0.0306324, acc 0.992188\n",
      "2017-01-11T02:42:45.387574: step 9415, loss 0.000421003, acc 1\n",
      "2017-01-11T02:42:47.451512: step 9416, loss 0.0430343, acc 0.984375\n",
      "2017-01-11T02:42:49.506139: step 9417, loss 0.0260586, acc 0.992188\n",
      "2017-01-11T02:42:51.546837: step 9418, loss 0.053427, acc 0.984375\n",
      "2017-01-11T02:42:53.602972: step 9419, loss 0.00240701, acc 1\n",
      "2017-01-11T02:42:55.619783: step 9420, loss 0.00198152, acc 1\n",
      "2017-01-11T02:42:57.678248: step 9421, loss 0.00155547, acc 1\n",
      "2017-01-11T02:42:59.713010: step 9422, loss 0.00575294, acc 1\n",
      "2017-01-11T02:43:01.973683: step 9423, loss 0.0521346, acc 0.984375\n",
      "2017-01-11T02:43:04.106343: step 9424, loss 0.0360757, acc 0.992188\n",
      "2017-01-11T02:43:06.178239: step 9425, loss 0.0131241, acc 1\n",
      "2017-01-11T02:43:08.280456: step 9426, loss 0.0276617, acc 0.992188\n",
      "2017-01-11T02:43:10.297504: step 9427, loss 0.00910961, acc 1\n",
      "2017-01-11T02:43:12.350472: step 9428, loss 0.00240188, acc 1\n",
      "2017-01-11T02:43:14.451131: step 9429, loss 0.000364568, acc 1\n",
      "2017-01-11T02:43:16.483888: step 9430, loss 0.106369, acc 0.976562\n",
      "2017-01-11T02:43:18.523869: step 9431, loss 6.05037e-05, acc 1\n",
      "2017-01-11T02:43:20.565648: step 9432, loss 0.0211476, acc 1\n",
      "2017-01-11T02:43:22.647400: step 9433, loss 0.0236423, acc 0.992188\n",
      "2017-01-11T02:43:24.656394: step 9434, loss 0.0800026, acc 0.992188\n",
      "2017-01-11T02:43:26.730190: step 9435, loss 0.0363327, acc 0.984375\n",
      "2017-01-11T02:43:28.960774: step 9436, loss 0.035852, acc 0.984375\n",
      "2017-01-11T02:43:31.013522: step 9437, loss 0.00746715, acc 1\n",
      "2017-01-11T02:43:33.107497: step 9438, loss 0.00770899, acc 0.992188\n",
      "2017-01-11T02:43:35.409255: step 9439, loss 0.0545365, acc 0.992188\n",
      "2017-01-11T02:43:37.438251: step 9440, loss 0.0833701, acc 0.992188\n",
      "2017-01-11T02:43:39.520264: step 9441, loss 0.00802241, acc 1\n",
      "2017-01-11T02:43:41.548556: step 9442, loss 0.00824503, acc 1\n",
      "2017-01-11T02:43:44.286355: step 9443, loss 0.00557584, acc 1\n",
      "2017-01-11T02:43:46.680263: step 9444, loss 0.000639189, acc 1\n",
      "2017-01-11T02:43:48.686864: step 9445, loss 0.0099898, acc 1\n",
      "2017-01-11T02:43:50.752477: step 9446, loss 0.00170983, acc 1\n",
      "2017-01-11T02:43:52.808171: step 9447, loss 0.0574525, acc 0.984375\n",
      "2017-01-11T02:43:54.901890: step 9448, loss 0.00690197, acc 1\n",
      "2017-01-11T02:43:56.952657: step 9449, loss 0.0179856, acc 0.992188\n",
      "2017-01-11T02:43:59.005560: step 9450, loss 0.0189878, acc 0.992188\n",
      "2017-01-11T02:44:01.050438: step 9451, loss 0.0321835, acc 0.992188\n",
      "2017-01-11T02:44:03.063606: step 9452, loss 0.0596248, acc 0.984375\n",
      "2017-01-11T02:44:05.173630: step 9453, loss 0.00983507, acc 1\n",
      "2017-01-11T02:44:07.558592: step 9454, loss 0.000129441, acc 1\n",
      "2017-01-11T02:44:09.589009: step 9455, loss 0.0568445, acc 0.976562\n",
      "2017-01-11T02:44:11.617112: step 9456, loss 0.0751578, acc 0.976562\n",
      "2017-01-11T02:44:13.658489: step 9457, loss 0.0172741, acc 0.992188\n",
      "2017-01-11T02:44:15.813272: step 9458, loss 0.061453, acc 0.976562\n",
      "2017-01-11T02:44:17.847606: step 9459, loss 0.00878837, acc 1\n",
      "2017-01-11T02:44:19.934534: step 9460, loss 0.00310414, acc 1\n",
      "2017-01-11T02:44:22.007504: step 9461, loss 0.0128843, acc 0.992188\n",
      "2017-01-11T02:44:24.036683: step 9462, loss 0.02258, acc 0.992188\n",
      "2017-01-11T02:44:26.101346: step 9463, loss 0.0498928, acc 0.984375\n",
      "2017-01-11T02:44:28.149038: step 9464, loss 7.47468e-05, acc 1\n",
      "2017-01-11T02:44:30.198158: step 9465, loss 0.0405727, acc 0.984375\n",
      "2017-01-11T02:44:32.431370: step 9466, loss 0.0313491, acc 0.992188\n",
      "2017-01-11T02:44:34.482833: step 9467, loss 0.00328149, acc 1\n",
      "2017-01-11T02:44:36.561282: step 9468, loss 0.0275637, acc 0.992188\n",
      "2017-01-11T02:44:38.940673: step 9469, loss 0.0469932, acc 0.992188\n",
      "2017-01-11T02:44:40.960433: step 9470, loss 0.0123366, acc 0.992188\n",
      "2017-01-11T02:44:43.022160: step 9471, loss 0.0435442, acc 0.992188\n",
      "2017-01-11T02:44:45.043509: step 9472, loss 0.00287443, acc 1\n",
      "2017-01-11T02:44:47.074797: step 9473, loss 0.00168756, acc 1\n",
      "2017-01-11T02:44:49.152607: step 9474, loss 0.0112127, acc 0.992188\n",
      "2017-01-11T02:44:51.276606: step 9475, loss 0.000531058, acc 1\n",
      "2017-01-11T02:44:53.342161: step 9476, loss 0.0419503, acc 0.984375\n",
      "2017-01-11T02:44:55.400151: step 9477, loss 0.000634208, acc 1\n",
      "2017-01-11T02:44:57.433609: step 9478, loss 0.00217994, acc 1\n",
      "2017-01-11T02:44:59.459370: step 9479, loss 0.000906102, acc 1\n",
      "2017-01-11T02:45:01.510818: step 9480, loss 0.0428092, acc 0.984375\n",
      "2017-01-11T02:45:03.582959: step 9481, loss 0.0206427, acc 0.992188\n",
      "2017-01-11T02:45:05.657160: step 9482, loss 0.0134867, acc 0.992188\n",
      "2017-01-11T02:45:07.693033: step 9483, loss 0.0380909, acc 0.992188\n",
      "2017-01-11T02:45:09.981383: step 9484, loss 0.0148933, acc 0.992188\n",
      "2017-01-11T02:45:12.057759: step 9485, loss 0.0306073, acc 0.992188\n",
      "2017-01-11T02:45:14.114910: step 9486, loss 0.0128505, acc 1\n",
      "2017-01-11T02:45:16.280631: step 9487, loss 0.074979, acc 0.984375\n",
      "2017-01-11T02:45:18.334035: step 9488, loss 0.000396733, acc 1\n",
      "2017-01-11T02:45:20.407811: step 9489, loss 0.0292914, acc 0.992188\n",
      "2017-01-11T02:45:22.486417: step 9490, loss 0.000373805, acc 1\n",
      "2017-01-11T02:45:24.539069: step 9491, loss 0.0660623, acc 0.976562\n",
      "2017-01-11T02:45:26.595746: step 9492, loss 0.0419545, acc 0.984375\n",
      "2017-01-11T02:45:28.820093: step 9493, loss 0.0390652, acc 0.984375\n",
      "2017-01-11T02:45:30.842352: step 9494, loss 0.0237581, acc 0.992188\n",
      "2017-01-11T02:45:32.888590: step 9495, loss 0.000580091, acc 1\n",
      "2017-01-11T02:45:34.972839: step 9496, loss 0.016031, acc 0.992188\n",
      "2017-01-11T02:45:37.013424: step 9497, loss 0.0893771, acc 0.976562\n",
      "2017-01-11T02:45:39.071235: step 9498, loss 0.005207, acc 1\n",
      "2017-01-11T02:45:41.140523: step 9499, loss 0.000837319, acc 1\n",
      "2017-01-11T02:45:43.442961: step 9500, loss 0.0219296, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:46:09.736790: step 9500, loss 0.0718863, acc 0.98284\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9500\n",
      "\n",
      "2017-01-11T02:46:15.064434: step 9501, loss 0.0501926, acc 0.984375\n",
      "2017-01-11T02:46:17.384523: step 9502, loss 0.00197909, acc 1\n",
      "2017-01-11T02:46:19.430610: step 9503, loss 0.135386, acc 0.984375\n",
      "2017-01-11T02:46:21.552975: step 9504, loss 0.0238405, acc 0.992188\n",
      "2017-01-11T02:46:23.602746: step 9505, loss 0.0253229, acc 0.984375\n",
      "2017-01-11T02:46:25.623961: step 9506, loss 0.00317673, acc 1\n",
      "2017-01-11T02:46:27.667053: step 9507, loss 0.00394031, acc 1\n",
      "2017-01-11T02:46:29.729448: step 9508, loss 0.0577425, acc 0.976562\n",
      "2017-01-11T02:46:31.896267: step 9509, loss 0.0488288, acc 0.976562\n",
      "2017-01-11T02:46:33.912766: step 9510, loss 0.0348708, acc 0.984375\n",
      "2017-01-11T02:46:35.961935: step 9511, loss 0.00813402, acc 1\n",
      "2017-01-11T02:46:37.990397: step 9512, loss 0.000821934, acc 1\n",
      "2017-01-11T02:46:40.068087: step 9513, loss 0.0161746, acc 0.992188\n",
      "2017-01-11T02:46:42.114330: step 9514, loss 0.0222163, acc 0.992188\n",
      "2017-01-11T02:46:44.152243: step 9515, loss 0.000880777, acc 1\n",
      "2017-01-11T02:46:46.202738: step 9516, loss 0.0163068, acc 0.992188\n",
      "2017-01-11T02:46:48.547343: step 9517, loss 0.00195166, acc 1\n",
      "2017-01-11T02:46:50.583520: step 9518, loss 0.00328614, acc 1\n",
      "2017-01-11T02:46:52.675222: step 9519, loss 0.0129678, acc 0.992188\n",
      "2017-01-11T02:46:54.750239: step 9520, loss 0.050432, acc 0.992188\n",
      "2017-01-11T02:46:56.794433: step 9521, loss 0.000242865, acc 1\n",
      "2017-01-11T02:46:58.837810: step 9522, loss 0.0213986, acc 0.992188\n",
      "2017-01-11T02:47:00.884288: step 9523, loss 0.04398, acc 0.984375\n",
      "2017-01-11T02:47:02.918418: step 9524, loss 0.00221497, acc 1\n",
      "2017-01-11T02:47:05.016172: step 9525, loss 0.0384189, acc 1\n",
      "2017-01-11T02:47:07.040259: step 9526, loss 0.000603362, acc 1\n",
      "2017-01-11T02:47:09.016984: step 9527, loss 0.000411352, acc 1\n",
      "2017-01-11T02:47:11.075363: step 9528, loss 0.0122576, acc 0.992188\n",
      "2017-01-11T02:47:13.106734: step 9529, loss 0.0197957, acc 0.992188\n",
      "2017-01-11T02:47:15.132176: step 9530, loss 0.0148777, acc 0.992188\n",
      "2017-01-11T02:47:17.278304: step 9531, loss 0.0429473, acc 0.992188\n",
      "2017-01-11T02:47:19.427461: step 9532, loss 0.014273, acc 1\n",
      "2017-01-11T02:47:21.726494: step 9533, loss 0.00987524, acc 1\n",
      "2017-01-11T02:47:23.759689: step 9534, loss 0.00463779, acc 1\n",
      "2017-01-11T02:47:25.755344: step 9535, loss 0.00382366, acc 1\n",
      "2017-01-11T02:47:27.990821: step 9536, loss 0.00021712, acc 1\n",
      "2017-01-11T02:47:30.038194: step 9537, loss 0.00405192, acc 1\n",
      "2017-01-11T02:47:32.024128: step 9538, loss 0.0408612, acc 0.992188\n",
      "2017-01-11T02:47:34.070956: step 9539, loss 0.00135291, acc 1\n",
      "2017-01-11T02:47:36.143023: step 9540, loss 0.0330791, acc 0.984375\n",
      "2017-01-11T02:47:38.200957: step 9541, loss 0.0162307, acc 0.992188\n",
      "2017-01-11T02:47:40.280890: step 9542, loss 0.0332149, acc 0.992188\n",
      "2017-01-11T02:47:42.355334: step 9543, loss 0.0124239, acc 0.992188\n",
      "2017-01-11T02:47:45.312206: step 9544, loss 0.00842248, acc 1\n",
      "2017-01-11T02:47:47.518338: step 9545, loss 0.0113248, acc 1\n",
      "2017-01-11T02:47:49.583901: step 9546, loss 0.0437168, acc 0.984375\n",
      "2017-01-11T02:47:51.776694: step 9547, loss 0.00371364, acc 1\n",
      "2017-01-11T02:47:54.036663: step 9548, loss 0.0380455, acc 0.984375\n",
      "2017-01-11T02:47:56.079152: step 9549, loss 0.011908, acc 1\n",
      "2017-01-11T02:47:58.123358: step 9550, loss 0.000542334, acc 1\n",
      "2017-01-11T02:48:00.160335: step 9551, loss 0.000205896, acc 1\n",
      "2017-01-11T02:48:02.224919: step 9552, loss 0.000293302, acc 1\n",
      "2017-01-11T02:48:04.249437: step 9553, loss 0.0172665, acc 0.992188\n",
      "2017-01-11T02:48:06.312349: step 9554, loss 0.0161261, acc 0.992188\n",
      "2017-01-11T02:48:08.360769: step 9555, loss 0.0129274, acc 0.992188\n",
      "2017-01-11T02:48:10.415489: step 9556, loss 0.0462664, acc 0.984375\n",
      "2017-01-11T02:48:12.470798: step 9557, loss 0.0239916, acc 0.992188\n",
      "2017-01-11T02:48:14.535813: step 9558, loss 0.00447222, acc 1\n",
      "2017-01-11T02:48:16.686095: step 9559, loss 0.072143, acc 0.976562\n",
      "2017-01-11T02:48:18.705474: step 9560, loss 0.0218185, acc 0.992188\n",
      "2017-01-11T02:48:20.755395: step 9561, loss 0.0377133, acc 0.984375\n",
      "2017-01-11T02:48:22.844031: step 9562, loss 0.018639, acc 0.984375\n",
      "2017-01-11T02:48:25.246368: step 9563, loss 0.0296144, acc 0.992188\n",
      "2017-01-11T02:53:32.559479: step 9695, loss 0.07872, acc 0.992188\n",
      "2017-01-11T02:53:34.646292: step 9696, loss 0.0630263, acc 0.976562\n",
      "2017-01-11T02:53:36.680248: step 9697, loss 0.0265111, acc 0.992188\n",
      "2017-01-11T02:53:38.779406: step 9698, loss 0.0148586, acc 0.992188\n",
      "2017-01-11T02:53:40.926712: step 9699, loss 0.0259328, acc 0.992188\n",
      "2017-01-11T02:53:42.969581: step 9700, loss 0.0140606, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:54:14.002684: step 9700, loss 0.0709472, acc 0.9834\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9700\n",
      "\n",
      "2017-01-11T02:54:19.087021: step 9701, loss 0.00953524, acc 1\n",
      "2017-01-11T02:54:21.352335: step 9702, loss 0.00524603, acc 1\n",
      "2017-01-11T02:54:23.415967: step 9703, loss 0.0235327, acc 0.992188\n",
      "2017-01-11T02:54:25.438149: step 9704, loss 4.51654e-06, acc 1\n",
      "2017-01-11T02:54:27.440313: step 9705, loss 0.00596302, acc 1\n",
      "2017-01-11T02:54:29.482914: step 9706, loss 0.0463061, acc 0.976562\n",
      "2017-01-11T02:54:31.638599: step 9707, loss 0.030369, acc 0.984375\n",
      "2017-01-11T02:54:33.672562: step 9708, loss 0.00861966, acc 1\n",
      "2017-01-11T02:54:35.707060: step 9709, loss 0.0436327, acc 0.976562\n",
      "2017-01-11T02:54:37.770036: step 9710, loss 0.0256631, acc 0.992188\n",
      "2017-01-11T02:54:39.788740: step 9711, loss 0.0147507, acc 1\n",
      "2017-01-11T02:54:41.856602: step 9712, loss 0.00542475, acc 1\n",
      "2017-01-11T02:54:43.899748: step 9713, loss 0.0137442, acc 1\n",
      "2017-01-11T02:54:45.948812: step 9714, loss 0.0196847, acc 0.984375\n",
      "2017-01-11T02:54:47.998916: step 9715, loss 0.01541, acc 0.992188\n",
      "2017-01-11T02:54:50.078788: step 9716, loss 0.0293388, acc 0.992188\n",
      "2017-01-11T02:54:52.454754: step 9717, loss 0.0056314, acc 1\n",
      "2017-01-11T02:54:54.486183: step 9718, loss 0.0165971, acc 0.992188\n",
      "2017-01-11T02:54:56.490256: step 9719, loss 0.059839, acc 0.984375\n",
      "2017-01-11T02:54:58.525609: step 9720, loss 0.029435, acc 0.984375\n",
      "2017-01-11T02:55:00.556408: step 9721, loss 0.000146102, acc 1\n",
      "2017-01-11T02:55:02.565007: step 9722, loss 0.0239413, acc 0.992188\n",
      "2017-01-11T02:55:04.626946: step 9723, loss 0.0272428, acc 0.992188\n",
      "2017-01-11T02:55:06.649508: step 9724, loss 2.28719e-06, acc 1\n",
      "2017-01-11T02:55:08.673970: step 9725, loss 0.014579, acc 0.992188\n",
      "2017-01-11T02:55:10.691987: step 9726, loss 0.0208241, acc 0.992188\n",
      "2017-01-11T02:55:12.776301: step 9727, loss 0.00975144, acc 1\n",
      "2017-01-11T02:55:14.806389: step 9728, loss 0.00523274, acc 1\n",
      "2017-01-11T02:55:16.862154: step 9729, loss 0.0116145, acc 0.992188\n",
      "2017-01-11T02:55:18.883901: step 9730, loss 0.036449, acc 1\n",
      "2017-01-11T02:55:21.089179: step 9731, loss 0.000139764, acc 1\n",
      "2017-01-11T02:55:23.428769: step 9732, loss 0.0206498, acc 0.984375\n",
      "2017-01-11T02:55:25.482502: step 9733, loss 0.00491021, acc 1\n",
      "2017-01-11T02:55:27.509733: step 9734, loss 0.00162746, acc 1\n",
      "2017-01-11T02:55:29.729225: step 9735, loss 0.020891, acc 0.992188\n",
      "2017-01-11T02:55:31.791974: step 9736, loss 0.0163746, acc 0.992188\n",
      "2017-01-11T02:55:33.849638: step 9737, loss 0.00476828, acc 1\n",
      "2017-01-11T02:55:35.895057: step 9738, loss 0.0112633, acc 0.992188\n",
      "2017-01-11T02:55:37.899785: step 9739, loss 0.000376724, acc 1\n",
      "2017-01-11T02:55:39.929271: step 9740, loss 0.0114938, acc 1\n",
      "2017-01-11T02:55:41.962398: step 9741, loss 0.000496919, acc 1\n",
      "2017-01-11T02:55:43.967755: step 9742, loss 0.00695457, acc 1\n",
      "2017-01-11T02:55:47.067741: step 9743, loss 0.00210913, acc 1\n",
      "2017-01-11T02:55:49.158884: step 9744, loss 0.0333232, acc 0.984375\n",
      "2017-01-11T02:55:51.213899: step 9745, loss 0.0176055, acc 1\n",
      "2017-01-11T02:55:53.350847: step 9746, loss 0.00728617, acc 1\n",
      "2017-01-11T02:55:55.749694: step 9747, loss 0.0104091, acc 0.992188\n",
      "2017-01-11T02:55:57.775485: step 9748, loss 0.0302773, acc 0.992188\n",
      "2017-01-11T02:55:59.794866: step 9749, loss 0.0225189, acc 0.992188\n",
      "2017-01-11T02:56:01.801543: step 9750, loss 0.00910334, acc 1\n",
      "2017-01-11T02:56:03.878218: step 9751, loss 0.00565079, acc 1\n",
      "2017-01-11T02:56:05.953669: step 9752, loss 0.0299477, acc 0.992188\n",
      "2017-01-11T02:56:07.936578: step 9753, loss 0.0138276, acc 1\n",
      "2017-01-11T02:56:09.936030: step 9754, loss 0.0170756, acc 1\n",
      "2017-01-11T02:56:12.009509: step 9755, loss 0.00826305, acc 1\n",
      "2017-01-11T02:56:14.048034: step 9756, loss 0.00665708, acc 1\n",
      "2017-01-11T02:56:16.097538: step 9757, loss 0.106134, acc 0.984375\n",
      "2017-01-11T02:56:18.165467: step 9758, loss 0.00411634, acc 1\n",
      "2017-01-11T02:56:20.238703: step 9759, loss 0.00649696, acc 1\n",
      "2017-01-11T02:56:22.457062: step 9760, loss 0.0245516, acc 1\n",
      "2017-01-11T02:56:24.479941: step 9761, loss 0.0148687, acc 1\n",
      "2017-01-11T02:56:26.648372: step 9762, loss 0.000125109, acc 1\n",
      "2017-01-11T02:56:28.851689: step 9763, loss 0.0163965, acc 0.992188\n",
      "2017-01-11T02:56:30.992798: step 9764, loss 1.10404e-05, acc 1\n",
      "2017-01-11T02:56:33.013705: step 9765, loss 0.0272279, acc 0.992188\n",
      "2017-01-11T02:56:35.051729: step 9766, loss 0.00679075, acc 1\n",
      "2017-01-11T02:56:37.080200: step 9767, loss 0.0160777, acc 0.992188\n",
      "2017-01-11T02:56:39.147556: step 9768, loss 0.0145444, acc 0.992188\n",
      "2017-01-11T02:56:41.181027: step 9769, loss 0.000270603, acc 1\n",
      "2017-01-11T02:56:43.215349: step 9770, loss 0.0295485, acc 0.992188\n",
      "2017-01-11T02:56:45.206721: step 9771, loss 0.0524559, acc 0.984375\n",
      "2017-01-11T02:56:47.243673: step 9772, loss 0.0434876, acc 0.984375\n",
      "2017-01-11T02:56:49.311557: step 9773, loss 0.0365555, acc 0.992188\n",
      "2017-01-11T02:56:51.420700: step 9774, loss 0.0485012, acc 0.992188\n",
      "2017-01-11T02:56:53.434308: step 9775, loss 0.0119488, acc 1\n",
      "2017-01-11T02:56:55.455368: step 9776, loss 0.00548852, acc 1\n",
      "2017-01-11T02:56:57.504707: step 9777, loss 0.0794652, acc 0.984375\n",
      "2017-01-11T02:56:59.869330: step 9778, loss 0.0310333, acc 0.984375\n",
      "2017-01-11T02:57:01.903573: step 9779, loss 0.00060584, acc 1\n",
      "2017-01-11T02:57:03.930202: step 9780, loss 0.00525647, acc 1\n",
      "2017-01-11T02:57:05.967699: step 9781, loss 0.0163146, acc 0.992188\n",
      "2017-01-11T02:57:08.043508: step 9782, loss 0.0292801, acc 0.984375\n",
      "2017-01-11T02:57:10.104879: step 9783, loss 0.0108996, acc 1\n",
      "2017-01-11T02:57:12.149202: step 9784, loss 0.0383222, acc 0.992188\n",
      "2017-01-11T02:57:14.200698: step 9785, loss 7.64996e-05, acc 1\n",
      "2017-01-11T02:57:16.234920: step 9786, loss 0.000962121, acc 1\n",
      "2017-01-11T02:57:18.278837: step 9787, loss 0.00117505, acc 1\n",
      "2017-01-11T02:57:20.355181: step 9788, loss 0.0142048, acc 0.992188\n",
      "2017-01-11T02:57:22.573769: step 9789, loss 0.0215239, acc 0.992188\n",
      "2017-01-11T02:57:24.625539: step 9790, loss 0.00161445, acc 1\n",
      "2017-01-11T02:57:26.652899: step 9791, loss 0.00208852, acc 1\n",
      "2017-01-11T02:57:28.687448: step 9792, loss 0.00506987, acc 1\n",
      "2017-01-11T02:57:30.918913: step 9793, loss 0.0303019, acc 0.992188\n",
      "2017-01-11T02:57:33.044692: step 9794, loss 0.0209324, acc 0.992188\n",
      "2017-01-11T02:57:35.109162: step 9795, loss 0.00753675, acc 1\n",
      "2017-01-11T02:57:37.161323: step 9796, loss 0.0173222, acc 0.992188\n",
      "2017-01-11T02:57:39.414205: step 9797, loss 0.0274244, acc 0.992188\n",
      "2017-01-11T02:57:41.455309: step 9798, loss 0.0433135, acc 0.984375\n",
      "2017-01-11T02:57:43.488127: step 9799, loss 0.000578714, acc 1\n",
      "2017-01-11T02:57:45.532326: step 9800, loss 0.0976613, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T02:58:17.331930: step 9800, loss 0.071711, acc 0.98364\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9800\n",
      "\n",
      "2017-01-11T02:58:22.027901: step 9801, loss 0.00231004, acc 1\n",
      "2017-01-11T02:58:24.056725: step 9802, loss 0.0178493, acc 1\n",
      "2017-01-11T02:58:26.127231: step 9803, loss 0.00184201, acc 1\n",
      "2017-01-11T02:58:28.199098: step 9804, loss 0.0190206, acc 0.992188\n",
      "2017-01-11T02:58:30.232608: step 9805, loss 0.0136409, acc 0.992188\n",
      "2017-01-11T02:58:32.370482: step 9806, loss 0.020601, acc 0.992188\n",
      "2017-01-11T02:58:34.417083: step 9807, loss 0.00773035, acc 1\n",
      "2017-01-11T02:58:36.444645: step 9808, loss 0.0166647, acc 0.992188\n",
      "2017-01-11T02:58:38.622215: step 9809, loss 0.0288489, acc 0.992188\n",
      "2017-01-11T02:58:40.827661: step 9810, loss 0.000142551, acc 1\n",
      "2017-01-11T02:58:42.857040: step 9811, loss 0.000179286, acc 1\n",
      "2017-01-11T02:58:44.960814: step 9812, loss 0.0258868, acc 0.992188\n",
      "2017-01-11T02:58:47.072319: step 9813, loss 0.0321899, acc 0.984375\n",
      "2017-01-11T02:58:49.144499: step 9814, loss 0.00241335, acc 1\n",
      "2017-01-11T02:58:51.282839: step 9815, loss 0.00272764, acc 1\n",
      "2017-01-11T02:58:53.332645: step 9816, loss 0.0233827, acc 0.992188\n",
      "2017-01-11T02:58:55.563426: step 9817, loss 0.000365531, acc 1\n",
      "2017-01-11T02:58:57.862415: step 9818, loss 0.0115616, acc 0.992188\n",
      "2017-01-11T02:59:00.158356: step 9819, loss 0.0414022, acc 0.992188\n",
      "2017-01-11T02:59:03.240874: step 9820, loss 0.0512844, acc 0.976562\n",
      "2017-01-11T02:59:05.644441: step 9821, loss 0.0288473, acc 0.984375\n",
      "2017-01-11T02:59:07.673808: step 9822, loss 0.0170008, acc 0.992188\n",
      "2017-01-11T02:59:09.685531: step 9823, loss 0.00703183, acc 0.992188\n",
      "2017-01-11T02:59:12.053462: step 9824, loss 0.000370301, acc 1\n",
      "2017-01-11T02:59:14.068253: step 9825, loss 0.0273289, acc 0.992188\n",
      "2017-01-11T02:59:16.128835: step 9826, loss 0.0174557, acc 1\n",
      "2017-01-11T02:59:18.176160: step 9827, loss 0.000674248, acc 1\n",
      "2017-01-11T02:59:20.250957: step 9828, loss 0.0151656, acc 1\n",
      "2017-01-11T02:59:22.406779: step 9829, loss 0.0100858, acc 1\n",
      "2017-01-11T02:59:24.414473: step 9830, loss 0.0191168, acc 0.992188\n",
      "2017-01-11T02:59:26.461709: step 9831, loss 0.000238127, acc 1\n",
      "2017-01-11T02:59:28.483518: step 9832, loss 3.95525e-05, acc 1\n",
      "2017-01-11T02:59:30.484675: step 9833, loss 0.0226862, acc 0.992188\n",
      "2017-01-11T02:59:32.519520: step 9834, loss 0.0458063, acc 0.984375\n",
      "2017-01-11T02:59:34.515190: step 9835, loss 0.0438089, acc 0.992188\n",
      "2017-01-11T02:59:36.612025: step 9836, loss 0.00868402, acc 0.992188\n",
      "2017-01-11T02:59:38.654535: step 9837, loss 0.0381376, acc 0.976562\n",
      "2017-01-11T02:59:40.698930: step 9838, loss 0.02346, acc 0.992188\n",
      "2017-01-11T02:59:42.957795: step 9839, loss 0.00229486, acc 1\n",
      "2017-01-11T02:59:45.146779: step 9840, loss 0.0472445, acc 0.984375\n",
      "2017-01-11T02:59:47.169235: step 9841, loss 0.000336432, acc 1\n",
      "2017-01-11T02:59:49.236014: step 9842, loss 0.000975952, acc 1\n",
      "2017-01-11T02:59:51.329184: step 9843, loss 0.0340625, acc 0.992188\n",
      "2017-01-11T02:59:53.372753: step 9844, loss 0.0657906, acc 0.984375\n",
      "2017-01-11T02:59:55.391887: step 9845, loss 0.0245119, acc 0.992188\n",
      "2017-01-11T02:59:57.449471: step 9846, loss 0.0118918, acc 0.992188\n",
      "2017-01-11T02:59:59.459561: step 9847, loss 0.00053195, acc 1\n",
      "2017-01-11T03:00:01.486153: step 9848, loss 0.0259326, acc 0.992188\n",
      "2017-01-11T03:00:03.527996: step 9849, loss 0.00816467, acc 1\n",
      "2017-01-11T03:00:05.625384: step 9850, loss 0.048303, acc 0.992188\n",
      "2017-01-11T03:00:07.675928: step 9851, loss 0.0215721, acc 0.992188\n",
      "2017-01-11T03:00:09.741855: step 9852, loss 0.0292918, acc 0.992188\n",
      "2017-01-11T03:00:11.789073: step 9853, loss 0.000326293, acc 1\n",
      "2017-01-11T03:00:13.810346: step 9854, loss 0.024242, acc 0.992188\n",
      "2017-01-11T03:00:16.202437: step 9855, loss 0.0204183, acc 0.992188\n",
      "2017-01-11T03:00:18.240413: step 9856, loss 0.0278131, acc 0.992188\n",
      "2017-01-11T03:00:20.308058: step 9857, loss 0.00126591, acc 1\n",
      "2017-01-11T03:00:22.370666: step 9858, loss 0.0118294, acc 1\n",
      "2017-01-11T03:00:24.516778: step 9859, loss 0.00154279, acc 1\n",
      "2017-01-11T03:00:26.571495: step 9860, loss 0.00181273, acc 1\n",
      "2017-01-11T03:00:28.896926: step 9861, loss 0.0557915, acc 0.992188\n",
      "2017-01-11T03:00:31.220391: step 9862, loss 0.0356258, acc 0.984375\n",
      "2017-01-11T03:00:33.231392: step 9863, loss 0.0206453, acc 1\n",
      "2017-01-11T03:00:35.317551: step 9864, loss 0.0269225, acc 0.992188\n",
      "2017-01-11T03:00:37.337020: step 9865, loss 0.00405803, acc 1\n",
      "2017-01-11T03:00:39.370242: step 9866, loss 0.0412693, acc 0.976562\n",
      "2017-01-11T03:00:41.434964: step 9867, loss 0.00515687, acc 1\n",
      "2017-01-11T03:00:43.483980: step 9868, loss 0.0272754, acc 0.992188\n",
      "2017-01-11T03:00:45.525406: step 9869, loss 0.0152549, acc 0.992188\n",
      "2017-01-11T03:00:49.019836: step 9870, loss 0.039468, acc 0.992188\n",
      "2017-01-11T03:00:51.143564: step 9871, loss 0.0152716, acc 1\n",
      "2017-01-11T03:00:53.236109: step 9872, loss 0.0441656, acc 0.992188\n",
      "2017-01-11T03:00:55.216548: step 9873, loss 0.00158618, acc 1\n",
      "2017-01-11T03:00:57.266211: step 9874, loss 0.00460112, acc 1\n",
      "2017-01-11T03:00:59.300860: step 9875, loss 0.00163479, acc 1\n",
      "2017-01-11T03:01:01.333278: step 9876, loss 0.0135159, acc 1\n",
      "2017-01-11T03:01:03.323728: step 9877, loss 0.0268438, acc 1\n",
      "2017-01-11T03:01:05.389696: step 9878, loss 0.0521446, acc 0.984375\n",
      "2017-01-11T03:01:07.404954: step 9879, loss 0.00904842, acc 0.992188\n",
      "2017-01-11T03:01:09.431312: step 9880, loss 0.00160593, acc 1\n",
      "2017-01-11T03:01:11.452896: step 9881, loss 0.0265553, acc 0.992188\n",
      "2017-01-11T03:01:13.508316: step 9882, loss 0.0166713, acc 0.992188\n",
      "2017-01-11T03:01:15.550166: step 9883, loss 0.00860013, acc 1\n",
      "2017-01-11T03:01:17.569821: step 9884, loss 0.0158131, acc 0.992188\n",
      "2017-01-11T03:01:19.873291: step 9885, loss 0.00231828, acc 1\n",
      "2017-01-11T03:01:22.043556: step 9886, loss 0.00166732, acc 1\n",
      "2017-01-11T03:01:24.108437: step 9887, loss 0.0166037, acc 0.992188\n",
      "2017-01-11T03:01:26.154826: step 9888, loss 0.064711, acc 0.976562\n",
      "2017-01-11T03:01:28.208711: step 9889, loss 0.00540553, acc 1\n",
      "2017-01-11T03:01:30.231323: step 9890, loss 0.00025759, acc 1\n",
      "2017-01-11T03:01:32.285049: step 9891, loss 0.000239216, acc 1\n",
      "2017-01-11T03:01:34.333123: step 9892, loss 0.000563095, acc 1\n",
      "2017-01-11T03:01:36.409640: step 9893, loss 0.0158566, acc 1\n",
      "2017-01-11T03:01:38.467009: step 9894, loss 0.0141522, acc 0.992188\n",
      "2017-01-11T03:01:40.503448: step 9895, loss 0.016107, acc 0.992188\n",
      "2017-01-11T03:01:42.530008: step 9896, loss 0.000274297, acc 1\n",
      "2017-01-11T03:01:44.532412: step 9897, loss 0.0222513, acc 0.992188\n",
      "2017-01-11T03:01:46.558532: step 9898, loss 0.00140909, acc 1\n",
      "2017-01-11T03:01:48.582298: step 9899, loss 0.00142782, acc 1\n",
      "2017-01-11T03:01:50.669288: step 9900, loss 0.0181445, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:02:17.171079: step 9900, loss 0.0688479, acc 0.98388\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-9900\n",
      "\n",
      "2017-01-11T03:02:21.919918: step 9901, loss 0.0035115, acc 1\n",
      "2017-01-11T03:02:24.138549: step 9902, loss 0.0182856, acc 0.992188\n",
      "2017-01-11T03:02:26.474319: step 9903, loss 4.28846e-05, acc 1\n",
      "2017-01-11T03:02:28.477169: step 9904, loss 0.000154239, acc 1\n",
      "2017-01-11T03:02:30.584133: step 9905, loss 0.0765272, acc 0.984375\n",
      "2017-01-11T03:02:32.653943: step 9906, loss 0.000258199, acc 1\n",
      "2017-01-11T03:02:34.746328: step 9907, loss 0.0324468, acc 0.984375\n",
      "2017-01-11T03:02:36.787552: step 9908, loss 0.000273868, acc 1\n",
      "2017-01-11T03:02:38.839679: step 9909, loss 0.0489939, acc 0.984375\n",
      "2017-01-11T03:02:40.885435: step 9910, loss 0.0459704, acc 0.976562\n",
      "2017-01-11T03:02:42.956652: step 9911, loss 0.00693734, acc 1\n",
      "2017-01-11T03:02:45.204435: step 9912, loss 0.00382814, acc 1\n",
      "2017-01-11T03:02:47.223816: step 9913, loss 0.0134549, acc 1\n",
      "2017-01-11T03:02:49.255714: step 9914, loss 0.00106718, acc 1\n",
      "2017-01-11T03:02:51.363451: step 9915, loss 0.00742048, acc 1\n",
      "2017-01-11T03:02:53.425685: step 9916, loss 0.00800221, acc 1\n",
      "2017-01-11T03:02:55.446445: step 9917, loss 0.0419191, acc 0.992188\n",
      "2017-01-11T03:02:57.808235: step 9918, loss 0.0190758, acc 0.992188\n",
      "2017-01-11T03:03:00.288450: step 9919, loss 0.00509947, acc 1\n",
      "2017-01-11T03:03:03.017165: step 9920, loss 0.0395655, acc 0.984375\n",
      "2017-01-11T03:03:05.104037: step 9921, loss 0.0187394, acc 0.992188\n",
      "2017-01-11T03:03:07.158289: step 9922, loss 0.0189838, acc 0.992188\n",
      "2017-01-11T03:03:09.233659: step 9923, loss 0.00284184, acc 1\n",
      "2017-01-11T03:03:11.316820: step 9924, loss 0.000537352, acc 1\n",
      "2017-01-11T03:03:13.388013: step 9925, loss 0.000838773, acc 1\n",
      "2017-01-11T03:03:15.426256: step 9926, loss 0.0130992, acc 0.992188\n",
      "2017-01-11T03:03:17.453867: step 9927, loss 0.0324657, acc 0.992188\n",
      "2017-01-11T03:03:19.495609: step 9928, loss 0.00612923, acc 1\n",
      "2017-01-11T03:03:21.572635: step 9929, loss 0.00287206, acc 1\n",
      "2017-01-11T03:03:23.590774: step 9930, loss 0.0126787, acc 0.992188\n",
      "2017-01-11T03:03:25.714764: step 9931, loss 0.000458293, acc 1\n",
      "2017-01-11T03:03:27.766281: step 9932, loss 0.0497834, acc 0.984375\n",
      "2017-01-11T03:03:30.102046: step 9933, loss 0.0187547, acc 0.992188\n",
      "2017-01-11T03:03:32.123218: step 9934, loss 0.00241434, acc 1\n",
      "2017-01-11T03:03:34.153022: step 9935, loss 0.014513, acc 0.992188\n",
      "2017-01-11T03:03:36.245281: step 9936, loss 0.0231399, acc 0.992188\n",
      "2017-01-11T03:03:38.251636: step 9937, loss 0.017867, acc 0.992188\n",
      "2017-01-11T03:03:40.296634: step 9938, loss 0.0555538, acc 0.976562\n",
      "2017-01-11T03:03:42.354541: step 9939, loss 0.000733769, acc 1\n",
      "2017-01-11T03:03:44.389531: step 9940, loss 0.000243082, acc 1\n",
      "2017-01-11T03:03:46.431490: step 9941, loss 0.00102386, acc 1\n",
      "2017-01-11T03:03:48.475354: step 9942, loss 5.53042e-05, acc 1\n",
      "2017-01-11T03:03:50.539014: step 9943, loss 0.0488309, acc 0.992188\n",
      "2017-01-11T03:03:52.793839: step 9944, loss 0.0699619, acc 0.984375\n",
      "2017-01-11T03:03:54.841442: step 9945, loss 0.0445917, acc 0.984375\n",
      "2017-01-11T03:03:56.847884: step 9946, loss 0.0051369, acc 1\n",
      "2017-01-11T03:03:58.869148: step 9947, loss 0.0118463, acc 1\n",
      "2017-01-11T03:04:01.130693: step 9948, loss 0.000823177, acc 1\n",
      "2017-01-11T03:04:03.277904: step 9949, loss 0.00251979, acc 1\n",
      "2017-01-11T03:04:05.394805: step 9950, loss 0.00218073, acc 1\n",
      "2017-01-11T03:04:07.946571: step 9951, loss 0.0226989, acc 0.992188\n",
      "2017-01-11T03:04:10.509247: step 9952, loss 0.010083, acc 0.992188\n",
      "2017-01-11T03:04:12.504366: step 9953, loss 0.0231126, acc 0.984375\n",
      "2017-01-11T03:04:14.534071: step 9954, loss 0.00567899, acc 1\n",
      "2017-01-11T03:04:16.550800: step 9955, loss 0.00753627, acc 1\n",
      "2017-01-11T03:04:18.649356: step 9956, loss 0.0300902, acc 0.992188\n",
      "2017-01-11T03:04:20.734951: step 9957, loss 0.0427437, acc 0.976562\n",
      "2017-01-11T03:04:22.818563: step 9958, loss 0.00082349, acc 1\n",
      "2017-01-11T03:04:24.928964: step 9959, loss 0.000608622, acc 1\n",
      "2017-01-11T03:04:26.929913: step 9960, loss 0.000145503, acc 1\n",
      "2017-01-11T03:04:28.954301: step 9961, loss 0.0262665, acc 0.984375\n",
      "2017-01-11T03:04:31.069566: step 9962, loss 0.0121512, acc 0.992188\n",
      "2017-01-11T03:04:33.337766: step 9963, loss 0.0817113, acc 0.976562\n",
      "2017-01-11T03:04:35.445910: step 9964, loss 0.00676078, acc 0.992188\n",
      "2017-01-11T03:04:37.491569: step 9965, loss 0.0141884, acc 0.992188\n",
      "2017-01-11T03:04:39.517239: step 9966, loss 0.0263427, acc 0.992188\n",
      "2017-01-11T03:04:41.550473: step 9967, loss 0.00490482, acc 1\n",
      "2017-01-11T03:04:43.621766: step 9968, loss 0.00128571, acc 1\n",
      "2017-01-11T03:04:45.675584: step 9969, loss 0.0356486, acc 0.992188\n",
      "2017-01-11T03:04:47.680989: step 9970, loss 0.000101788, acc 1\n",
      "2017-01-11T03:04:49.749726: step 9971, loss 0.000667628, acc 1\n",
      "2017-01-11T03:04:51.827844: step 9972, loss 0.030711, acc 0.992188\n",
      "2017-01-11T03:04:53.846458: step 9973, loss 0.0449034, acc 0.992188\n",
      "2017-01-11T03:04:55.903141: step 9974, loss 0.00614133, acc 1\n",
      "2017-01-11T03:04:57.925384: step 9975, loss 0.0055516, acc 1\n",
      "2017-01-11T03:04:59.956097: step 9976, loss 0.00981651, acc 1\n",
      "2017-01-11T03:05:01.990315: step 9977, loss 0.0767077, acc 0.984375\n",
      "2017-01-11T03:05:04.073315: step 9978, loss 0.0047547, acc 1\n",
      "2017-01-11T03:05:06.474020: step 9979, loss 0.0510017, acc 0.984375\n",
      "2017-01-11T03:05:08.542969: step 9980, loss 0.0253248, acc 0.984375\n",
      "2017-01-11T03:05:10.581052: step 9981, loss 0.00542184, acc 1\n",
      "2017-01-11T03:05:12.609876: step 9982, loss 0.0126028, acc 0.992188\n",
      "2017-01-11T03:05:14.650331: step 9983, loss 0.00156788, acc 1\n",
      "2017-01-11T03:05:16.655955: step 9984, loss 0.00133188, acc 1\n",
      "2017-01-11T03:05:18.699475: step 9985, loss 0.0263725, acc 0.992188\n",
      "2017-01-11T03:05:20.773291: step 9986, loss 0.000523536, acc 1\n",
      "2017-01-11T03:05:22.846471: step 9987, loss 0.000188414, acc 1\n",
      "2017-01-11T03:05:24.930718: step 9988, loss 0.0213556, acc 1\n",
      "2017-01-11T03:05:27.043057: step 9989, loss 0.0427133, acc 0.984375\n",
      "2017-01-11T03:05:29.262844: step 9990, loss 0.0355687, acc 0.992188\n",
      "2017-01-11T03:05:31.295321: step 9991, loss 0.0272931, acc 0.992188\n",
      "2017-01-11T03:05:33.349947: step 9992, loss 1.37795e-05, acc 1\n",
      "2017-01-11T03:05:35.401482: step 9993, loss 0.0167883, acc 1\n",
      "2017-01-11T03:05:37.754438: step 9994, loss 0.000337937, acc 1\n",
      "2017-01-11T03:05:39.765148: step 9995, loss 0.00632782, acc 1\n",
      "2017-01-11T03:05:41.826164: step 9996, loss 0.000777238, acc 1\n",
      "2017-01-11T03:05:44.286776: step 9997, loss 0.0268861, acc 0.992188\n",
      "2017-01-11T03:05:46.957866: step 9998, loss 0.00140344, acc 1\n",
      "2017-01-11T03:05:48.976638: step 9999, loss 0.00548069, acc 1\n",
      "2017-01-11T03:05:51.059508: step 10000, loss 0.000758068, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:06:17.537501: step 10000, loss 0.0730858, acc 0.98332\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10000\n",
      "\n",
      "2017-01-11T03:06:22.124845: step 10001, loss 0.0135718, acc 1\n",
      "2017-01-11T03:06:24.291255: step 10002, loss 0.0362613, acc 0.992188\n",
      "2017-01-11T03:06:26.389234: step 10003, loss 5.06632e-07, acc 1\n",
      "2017-01-11T03:06:28.447521: step 10004, loss 0.0225263, acc 0.992188\n",
      "2017-01-11T03:06:30.555713: step 10005, loss 0.0159177, acc 0.992188\n",
      "2017-01-11T03:11:34.239641: step 10132, loss 0.0912675, acc 0.976562\n",
      "2017-01-11T03:11:36.456768: step 10133, loss 0.0722681, acc 0.976562\n",
      "2017-01-11T03:11:38.551616: step 10134, loss 0.0134103, acc 0.992188\n",
      "2017-01-11T03:11:40.608629: step 10135, loss 0.043457, acc 0.984375\n",
      "2017-01-11T03:11:42.622559: step 10136, loss 0.026068, acc 0.992188\n",
      "2017-01-11T03:11:44.648726: step 10137, loss 0.0115351, acc 0.992188\n",
      "2017-01-11T03:11:46.681409: step 10138, loss 0.0510422, acc 0.984375\n",
      "2017-01-11T03:11:48.693852: step 10139, loss 0.0554443, acc 0.992188\n",
      "2017-01-11T03:11:50.759698: step 10140, loss 0.00324685, acc 1\n",
      "2017-01-11T03:11:52.837502: step 10141, loss 0.0259455, acc 0.984375\n",
      "2017-01-11T03:11:54.907168: step 10142, loss 0.00243542, acc 1\n",
      "2017-01-11T03:11:56.959326: step 10143, loss 0.00224034, acc 1\n",
      "2017-01-11T03:11:58.999606: step 10144, loss 0.034547, acc 0.992188\n",
      "2017-01-11T03:12:01.054589: step 10145, loss 0.000638914, acc 1\n",
      "2017-01-11T03:12:03.100045: step 10146, loss 0.0209533, acc 0.992188\n",
      "2017-01-11T03:12:05.181595: step 10147, loss 0.052227, acc 0.984375\n",
      "2017-01-11T03:12:07.566688: step 10148, loss 0.0186706, acc 0.992188\n",
      "2017-01-11T03:12:09.604285: step 10149, loss 0.0105564, acc 0.992188\n",
      "2017-01-11T03:12:11.626308: step 10150, loss 0.000487768, acc 1\n",
      "2017-01-11T03:12:13.674649: step 10151, loss 0.000804354, acc 1\n",
      "2017-01-11T03:12:15.726943: step 10152, loss 0.0297882, acc 0.992188\n",
      "2017-01-11T03:12:17.764899: step 10153, loss 0.00667542, acc 0.992188\n",
      "2017-01-11T03:12:19.826151: step 10154, loss 0.0616368, acc 0.976562\n",
      "2017-01-11T03:12:21.889463: step 10155, loss 0.00549881, acc 1\n",
      "2017-01-11T03:12:23.895497: step 10156, loss 4.89653e-05, acc 1\n",
      "2017-01-11T03:12:25.935395: step 10157, loss 0.0375144, acc 0.992188\n",
      "2017-01-11T03:12:28.023865: step 10158, loss 1.39375e-05, acc 1\n",
      "2017-01-11T03:12:30.148842: step 10159, loss 0.0303509, acc 0.992188\n",
      "2017-01-11T03:12:32.370322: step 10160, loss 0.00823411, acc 1\n",
      "2017-01-11T03:12:34.397885: step 10161, loss 0.0340449, acc 0.992188\n",
      "2017-01-11T03:12:36.443558: step 10162, loss 0.00648402, acc 1\n",
      "2017-01-11T03:12:38.820548: step 10163, loss 0.000432835, acc 1\n",
      "2017-01-11T03:12:40.841870: step 10164, loss 0.00632409, acc 1\n",
      "2017-01-11T03:12:42.889983: step 10165, loss 0.000703275, acc 1\n",
      "2017-01-11T03:12:44.939039: step 10166, loss 0.0560588, acc 0.976562\n",
      "2017-01-11T03:12:46.997949: step 10167, loss 0.00708663, acc 1\n",
      "2017-01-11T03:12:49.026514: step 10168, loss 0.01436, acc 1\n",
      "2017-01-11T03:12:51.077730: step 10169, loss 0.00477527, acc 1\n",
      "2017-01-11T03:12:53.132360: step 10170, loss 0.0165619, acc 0.992188\n",
      "2017-01-11T03:12:55.145134: step 10171, loss 0.0189312, acc 1\n",
      "2017-01-11T03:12:57.391168: step 10172, loss 0.0728924, acc 0.976562\n",
      "2017-01-11T03:12:59.438146: step 10173, loss 0.014685, acc 1\n",
      "2017-01-11T03:13:01.481479: step 10174, loss 8.10905e-05, acc 1\n",
      "2017-01-11T03:13:03.506603: step 10175, loss 0.0429999, acc 0.984375\n",
      "2017-01-11T03:13:05.587214: step 10176, loss 0.0149953, acc 0.992188\n",
      "2017-01-11T03:13:07.672982: step 10177, loss 0.0170126, acc 0.992188\n",
      "2017-01-11T03:13:10.016848: step 10178, loss 0.00169552, acc 1\n",
      "2017-01-11T03:13:12.566009: step 10179, loss 0.0025301, acc 1\n",
      "2017-01-11T03:13:15.213355: step 10180, loss 0.0391667, acc 0.992188\n",
      "2017-01-11T03:13:17.235004: step 10181, loss 0.00495295, acc 1\n",
      "2017-01-11T03:13:19.242021: step 10182, loss 0.0276515, acc 0.992188\n",
      "2017-01-11T03:13:21.351381: step 10183, loss 0.0349848, acc 0.984375\n",
      "2017-01-11T03:13:23.452919: step 10184, loss 0.00333578, acc 1\n",
      "2017-01-11T03:13:25.522108: step 10185, loss 0.0288462, acc 0.984375\n",
      "2017-01-11T03:13:27.569278: step 10186, loss 0.105401, acc 0.992188\n",
      "2017-01-11T03:13:29.692682: step 10187, loss 0.0131298, acc 0.992188\n",
      "2017-01-11T03:13:31.716676: step 10188, loss 0.0972807, acc 0.976562\n",
      "2017-01-11T03:13:33.762469: step 10189, loss 0.000690453, acc 1\n",
      "2017-01-11T03:13:35.836730: step 10190, loss 0.000271661, acc 1\n",
      "2017-01-11T03:13:37.840031: step 10191, loss 0.0369029, acc 0.984375\n",
      "2017-01-11T03:13:39.890350: step 10192, loss 0.0582594, acc 0.984375\n",
      "2017-01-11T03:13:42.241920: step 10193, loss 0.0240038, acc 0.992188\n",
      "2017-01-11T03:13:44.267950: step 10194, loss 0.0144615, acc 1\n",
      "2017-01-11T03:13:46.302264: step 10195, loss 0.0175829, acc 0.992188\n",
      "2017-01-11T03:13:48.355331: step 10196, loss 0.0351017, acc 0.984375\n",
      "2017-01-11T03:13:50.440189: step 10197, loss 0.00649441, acc 1\n",
      "2017-01-11T03:13:52.504263: step 10198, loss 4.35059e-05, acc 1\n",
      "2017-01-11T03:13:54.532951: step 10199, loss 0.0130697, acc 0.992188\n",
      "2017-01-11T03:13:56.544084: step 10200, loss 0.0250228, acc 0.992188\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:14:28.577643: step 10200, loss 0.0712689, acc 0.9832\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10200\n",
      "\n",
      "2017-01-11T03:14:33.321715: step 10201, loss 0.0254647, acc 0.984375\n",
      "2017-01-11T03:14:35.366150: step 10202, loss 0.0143096, acc 0.992188\n",
      "2017-01-11T03:14:37.428120: step 10203, loss 0.0148634, acc 1\n",
      "2017-01-11T03:14:39.434895: step 10204, loss 0.00704612, acc 1\n",
      "2017-01-11T03:14:41.480816: step 10205, loss 0.00337109, acc 1\n",
      "2017-01-11T03:14:43.534875: step 10206, loss 0.000197312, acc 1\n",
      "2017-01-11T03:14:45.591241: step 10207, loss 0.0116792, acc 0.992188\n",
      "2017-01-11T03:14:47.621148: step 10208, loss 0.0106189, acc 0.992188\n",
      "2017-01-11T03:14:50.016745: step 10209, loss 0.0205931, acc 0.992188\n",
      "2017-01-11T03:14:52.055030: step 10210, loss 0.0142924, acc 1\n",
      "2017-01-11T03:14:54.092800: step 10211, loss 0.0722526, acc 0.984375\n",
      "2017-01-11T03:14:56.149218: step 10212, loss 0.00332185, acc 1\n",
      "2017-01-11T03:14:58.185585: step 10213, loss 0.00238842, acc 1\n",
      "2017-01-11T03:15:00.194406: step 10214, loss 0.000230163, acc 1\n",
      "2017-01-11T03:15:02.224449: step 10215, loss 0.0200211, acc 0.992188\n",
      "2017-01-11T03:15:04.252594: step 10216, loss 0.000160869, acc 1\n",
      "2017-01-11T03:15:06.314580: step 10217, loss 0.0155832, acc 0.992188\n",
      "2017-01-11T03:15:08.346289: step 10218, loss 0.000254875, acc 1\n",
      "2017-01-11T03:15:10.403952: step 10219, loss 0.0121889, acc 0.992188\n",
      "2017-01-11T03:15:12.414368: step 10220, loss 0.0402153, acc 0.992188\n",
      "2017-01-11T03:15:14.467120: step 10221, loss 0.00491181, acc 1\n",
      "2017-01-11T03:15:16.492967: step 10222, loss 0.00911853, acc 1\n",
      "2017-01-11T03:15:18.543206: step 10223, loss 0.275239, acc 0.984375\n",
      "2017-01-11T03:15:20.742623: step 10224, loss 0.00034364, acc 1\n",
      "2017-01-11T03:15:22.976075: step 10225, loss 0.0236348, acc 0.992188\n",
      "2017-01-11T03:15:25.005081: step 10226, loss 0.00555956, acc 1\n",
      "2017-01-11T03:15:27.059363: step 10227, loss 0.0381933, acc 0.992188\n",
      "2017-01-11T03:15:29.332312: step 10228, loss 0.00921739, acc 1\n",
      "2017-01-11T03:15:31.473056: step 10229, loss 0.0344308, acc 0.976562\n",
      "2017-01-11T03:15:33.502263: step 10230, loss 0.00946906, acc 0.992188\n",
      "2017-01-11T03:15:35.564433: step 10231, loss 0.0518577, acc 0.976562\n",
      "2017-01-11T03:15:37.618836: step 10232, loss 0.0208274, acc 0.992188\n",
      "2017-01-11T03:15:39.638071: step 10233, loss 0.0839585, acc 0.992188\n",
      "2017-01-11T03:15:41.647424: step 10234, loss 0.00330282, acc 1\n",
      "2017-01-11T03:15:43.655230: step 10235, loss 0.0233967, acc 0.992188\n",
      "2017-01-11T03:15:46.672404: step 10236, loss 0.00296957, acc 1\n",
      "2017-01-11T03:15:48.839576: step 10237, loss 0.010271, acc 0.992188\n",
      "2017-01-11T03:15:50.913877: step 10238, loss 0.0228614, acc 0.992188\n",
      "2017-01-11T03:15:53.072998: step 10239, loss 0.000766756, acc 1\n",
      "2017-01-11T03:15:55.288415: step 10240, loss 0.00312232, acc 1\n",
      "2017-01-11T03:15:57.303050: step 10241, loss 0.0130823, acc 1\n",
      "2017-01-11T03:15:59.349852: step 10242, loss 0.00290462, acc 1\n",
      "2017-01-11T03:16:01.395981: step 10243, loss 0.0134709, acc 0.992188\n",
      "2017-01-11T03:16:03.430456: step 10244, loss 0.0953177, acc 0.976562\n",
      "2017-01-11T03:16:05.507947: step 10245, loss 0.0081278, acc 1\n",
      "2017-01-11T03:16:07.503117: step 10246, loss 0.0128678, acc 0.992188\n",
      "2017-01-11T03:16:09.602314: step 10247, loss 0.0431146, acc 0.992188\n",
      "2017-01-11T03:16:11.658901: step 10248, loss 0.0285635, acc 0.984375\n",
      "2017-01-11T03:16:13.682667: step 10249, loss 0.00639891, acc 1\n",
      "2017-01-11T03:16:15.698825: step 10250, loss 0.0139503, acc 0.992188\n",
      "2017-01-11T03:16:17.773311: step 10251, loss 0.0174569, acc 0.992188\n",
      "2017-01-11T03:16:19.817488: step 10252, loss 0.00357952, acc 1\n",
      "2017-01-11T03:16:21.876893: step 10253, loss 0.0851818, acc 0.96875\n",
      "2017-01-11T03:16:23.950914: step 10254, loss 0.0335121, acc 0.992188\n",
      "2017-01-11T03:16:26.293135: step 10255, loss 0.011012, acc 0.992188\n",
      "2017-01-11T03:16:28.331004: step 10256, loss 0.00121045, acc 1\n",
      "2017-01-11T03:16:30.476275: step 10257, loss 0.00513313, acc 1\n",
      "2017-01-11T03:16:32.592995: step 10258, loss 0.00720001, acc 0.992188\n",
      "2017-01-11T03:16:34.654710: step 10259, loss 0.0662965, acc 0.976562\n",
      "2017-01-11T03:16:36.719676: step 10260, loss 0.0356185, acc 0.992188\n",
      "2017-01-11T03:16:38.730168: step 10261, loss 0.0126879, acc 0.992188\n",
      "2017-01-11T03:16:40.773834: step 10262, loss 0.0344881, acc 0.992188\n",
      "2017-01-11T03:16:42.791572: step 10263, loss 0.00228621, acc 1\n",
      "2017-01-11T03:16:44.840430: step 10264, loss 0.0237856, acc 0.992188\n",
      "2017-01-11T03:16:46.886571: step 10265, loss 0.00884148, acc 1\n",
      "2017-01-11T03:16:48.931843: step 10266, loss 0.0337542, acc 0.984375\n",
      "2017-01-11T03:16:50.997975: step 10267, loss 0.00764403, acc 0.992188\n",
      "2017-01-11T03:16:53.060656: step 10268, loss 0.00777643, acc 1\n",
      "2017-01-11T03:16:55.095236: step 10269, loss 0.244436, acc 0.984375\n",
      "2017-01-11T03:16:57.310625: step 10270, loss 0.000284496, acc 1\n",
      "2017-01-11T03:16:59.432402: step 10271, loss 0.000572513, acc 1\n",
      "2017-01-11T03:17:01.491142: step 10272, loss 0.00954965, acc 0.992188\n",
      "2017-01-11T03:17:03.536579: step 10273, loss 0.00835245, acc 1\n",
      "2017-01-11T03:17:05.626307: step 10274, loss 0.0244213, acc 0.992188\n",
      "2017-01-11T03:17:07.649383: step 10275, loss 0.0071644, acc 0.992188\n",
      "2017-01-11T03:17:09.690641: step 10276, loss 0.00207729, acc 1\n",
      "2017-01-11T03:17:11.701801: step 10277, loss 0.0243852, acc 0.992188\n",
      "2017-01-11T03:17:13.750219: step 10278, loss 0.00900972, acc 1\n",
      "2017-01-11T03:17:15.767204: step 10279, loss 0.000298078, acc 1\n",
      "2017-01-11T03:17:17.806196: step 10280, loss 0.00247299, acc 1\n",
      "2017-01-11T03:17:19.843004: step 10281, loss 0.185266, acc 0.976562\n",
      "2017-01-11T03:17:21.900429: step 10282, loss 0.0447575, acc 0.984375\n",
      "2017-01-11T03:17:23.943236: step 10283, loss 0.0700482, acc 0.96875\n",
      "2017-01-11T03:17:25.996822: step 10284, loss 0.027708, acc 0.992188\n",
      "2017-01-11T03:17:28.052836: step 10285, loss 0.0263544, acc 0.992188\n",
      "2017-01-11T03:17:30.448884: step 10286, loss 0.0237107, acc 0.992188\n",
      "2017-01-11T03:17:32.575721: step 10287, loss 8.81254e-05, acc 1\n",
      "2017-01-11T03:17:34.632883: step 10288, loss 0.000329789, acc 1\n",
      "2017-01-11T03:17:36.674325: step 10289, loss 0.0481565, acc 0.976562\n",
      "2017-01-11T03:17:38.689709: step 10290, loss 0.00825907, acc 0.992188\n",
      "2017-01-11T03:17:40.733615: step 10291, loss 0.0121179, acc 1\n",
      "2017-01-11T03:17:42.772755: step 10292, loss 0.0118424, acc 1\n",
      "2017-01-11T03:17:44.814372: step 10293, loss 0.002392, acc 1\n",
      "2017-01-11T03:17:46.844489: step 10294, loss 0.000180114, acc 1\n",
      "2017-01-11T03:17:48.898301: step 10295, loss 0.024248, acc 0.992188\n",
      "2017-01-11T03:17:50.959609: step 10296, loss 0.000388344, acc 1\n",
      "2017-01-11T03:17:53.038910: step 10297, loss 0.000136513, acc 1\n",
      "2017-01-11T03:17:55.100813: step 10298, loss 0.0141876, acc 0.992188\n",
      "2017-01-11T03:17:57.124500: step 10299, loss 0.0122633, acc 0.992188\n",
      "2017-01-11T03:17:59.159617: step 10300, loss 0.00179416, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:18:41.056368: step 10300, loss 0.0707897, acc 0.98388\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10300\n",
      "\n",
      "2017-01-11T03:18:47.193592: step 10301, loss 0.0191011, acc 0.992188\n",
      "2017-01-11T03:18:49.213090: step 10302, loss 0.0190234, acc 0.992188\n",
      "2017-01-11T03:18:51.345366: step 10303, loss 0.0228197, acc 1\n",
      "2017-01-11T03:18:53.622029: step 10304, loss 0.00439411, acc 1\n",
      "2017-01-11T03:18:55.978031: step 10305, loss 0.00129058, acc 1\n",
      "2017-01-11T03:18:58.341791: step 10306, loss 0.0262462, acc 0.992188\n",
      "2017-01-11T03:19:00.517370: step 10307, loss 0.0113335, acc 0.992188\n",
      "2017-01-11T03:19:02.955714: step 10308, loss 0.0360893, acc 0.992188\n",
      "2017-01-11T03:19:05.059417: step 10309, loss 0.00011717, acc 1\n",
      "2017-01-11T03:19:07.116152: step 10310, loss 0.0536645, acc 0.984375\n",
      "2017-01-11T03:19:09.124444: step 10311, loss 0.0119891, acc 0.992188\n",
      "2017-01-11T03:19:11.519816: step 10312, loss 0.000136786, acc 1\n",
      "2017-01-11T03:19:13.740423: step 10313, loss 0.0325247, acc 0.992188\n",
      "2017-01-11T03:19:15.710341: step 10314, loss 0.000760211, acc 1\n",
      "2017-01-11T03:19:17.740932: step 10315, loss 4.18417e-05, acc 1\n",
      "2017-01-11T03:19:19.798725: step 10316, loss 0.0128906, acc 1\n",
      "2017-01-11T03:19:21.885320: step 10317, loss 0.0510967, acc 0.976562\n",
      "2017-01-11T03:19:23.889920: step 10318, loss 0.00074705, acc 1\n",
      "2017-01-11T03:19:26.411667: step 10319, loss 0.0363601, acc 0.992188\n",
      "2017-01-11T03:19:29.038578: step 10320, loss 0.0221564, acc 0.992188\n",
      "2017-01-11T03:19:31.082748: step 10321, loss 0.00438102, acc 1\n",
      "2017-01-11T03:19:33.223402: step 10322, loss 0.0218027, acc 0.992188\n",
      "2017-01-11T03:19:35.341712: step 10323, loss 0.05103, acc 0.992188\n",
      "2017-01-11T03:19:37.408205: step 10324, loss 1.31783e-05, acc 1\n",
      "2017-01-11T03:19:39.441212: step 10325, loss 0.0157453, acc 0.992188\n",
      "2017-01-11T03:19:41.469525: step 10326, loss 0.0295058, acc 0.992188\n",
      "2017-01-11T03:19:43.602290: step 10327, loss 0.000183402, acc 1\n",
      "2017-01-11T03:19:45.853021: step 10328, loss 0.0044186, acc 1\n",
      "2017-01-11T03:19:47.880165: step 10329, loss 0.0158666, acc 0.992188\n",
      "2017-01-11T03:19:49.953214: step 10330, loss 0.027676, acc 0.992188\n",
      "2017-01-11T03:19:51.976882: step 10331, loss 0.162109, acc 0.984375\n",
      "2017-01-11T03:19:54.026335: step 10332, loss 0.00761821, acc 1\n",
      "2017-01-11T03:19:56.059407: step 10333, loss 0.0223122, acc 0.992188\n",
      "2017-01-11T03:19:58.103019: step 10334, loss 0.00205435, acc 1\n",
      "2017-01-11T03:20:00.153608: step 10335, loss 0.0473631, acc 0.984375\n",
      "2017-01-11T03:20:02.153303: step 10336, loss 2.77443e-05, acc 1\n",
      "2017-01-11T03:20:04.212331: step 10337, loss 0.00901906, acc 1\n",
      "2017-01-11T03:20:06.307394: step 10338, loss 0.0560085, acc 0.984375\n",
      "2017-01-11T03:20:08.360996: step 10339, loss 0.00316272, acc 1\n",
      "2017-01-11T03:20:10.392598: step 10340, loss 0.00772463, acc 1\n",
      "2017-01-11T03:20:12.429852: step 10341, loss 0.017428, acc 0.992188\n",
      "2017-01-11T03:20:14.459508: step 10342, loss 0.029496, acc 0.984375\n",
      "2017-01-11T03:20:16.804031: step 10343, loss 0.00942234, acc 1\n",
      "2017-01-11T03:20:18.831150: step 10344, loss 0.0356819, acc 0.992188\n",
      "2017-01-11T03:20:20.894156: step 10345, loss 0.0662477, acc 0.992188\n",
      "2017-01-11T03:20:22.971916: step 10346, loss 0.00506372, acc 1\n",
      "2017-01-11T03:20:24.993889: step 10347, loss 0.0368213, acc 0.992188\n",
      "2017-01-11T03:20:27.023197: step 10348, loss 0.0247086, acc 0.992188\n",
      "2017-01-11T03:20:29.447235: step 10349, loss 0.0219299, acc 0.984375\n",
      "2017-01-11T03:20:31.686062: step 10350, loss 0.0104148, acc 0.992188\n",
      "2017-01-11T03:20:33.836273: step 10351, loss 0.0281174, acc 1\n",
      "2017-01-11T03:20:35.908815: step 10352, loss 0.00018889, acc 1\n",
      "2017-01-11T03:20:37.927203: step 10353, loss 0.0087278, acc 1\n",
      "2017-01-11T03:20:40.000026: step 10354, loss 0.0554527, acc 0.976562\n",
      "2017-01-11T03:20:42.041186: step 10355, loss 0.0495666, acc 0.984375\n",
      "2017-01-11T03:20:44.068126: step 10356, loss 0.0315945, acc 0.984375\n",
      "2017-01-11T03:20:46.922976: step 10357, loss 0.000453718, acc 1\n",
      "2017-01-11T03:20:49.627630: step 10358, loss 0.000466099, acc 1\n",
      "2017-01-11T03:20:51.738040: step 10359, loss 0.0207493, acc 0.992188\n",
      "2017-01-11T03:20:53.788180: step 10360, loss 0.0303527, acc 0.984375\n",
      "2017-01-11T03:20:55.841821: step 10361, loss 0.0439445, acc 0.984375\n",
      "2017-01-11T03:20:57.880557: step 10362, loss 0.00350319, acc 1\n",
      "2017-01-11T03:20:59.922000: step 10363, loss 0.0132706, acc 1\n",
      "2017-01-11T03:21:02.006366: step 10364, loss 0.0203782, acc 0.992188\n",
      "2017-01-11T03:21:04.062273: step 10365, loss 0.0314013, acc 0.984375\n",
      "2017-01-11T03:21:06.137014: step 10366, loss 0.0578108, acc 0.984375\n",
      "2017-01-11T03:21:08.159573: step 10367, loss 0.0221166, acc 0.992188\n",
      "2017-01-11T03:21:10.219896: step 10368, loss 0.0241093, acc 0.992188\n",
      "2017-01-11T03:21:12.266551: step 10369, loss 0.0597736, acc 0.976562\n",
      "2017-01-11T03:21:14.300353: step 10370, loss 0.0404386, acc 0.992188\n",
      "2017-01-11T03:21:16.354597: step 10371, loss 0.135349, acc 0.984375\n",
      "2017-01-11T03:21:18.413931: step 10372, loss 0.00251618, acc 1\n",
      "2017-01-11T03:21:20.777119: step 10373, loss 0.0048304, acc 1\n",
      "2017-01-11T03:21:22.836992: step 10374, loss 0.000100616, acc 1\n",
      "2017-01-11T03:21:24.850817: step 10375, loss 0.0208928, acc 0.992188\n",
      "2017-01-11T03:21:26.890255: step 10376, loss 0.00170139, acc 1\n",
      "2017-01-11T03:21:28.896868: step 10377, loss 0.0191629, acc 0.992188\n",
      "2017-01-11T03:21:30.919420: step 10378, loss 0.0261605, acc 0.992188\n",
      "2017-01-11T03:21:32.924633: step 10379, loss 0.01626, acc 0.992188\n",
      "2017-01-11T03:21:35.103152: step 10380, loss 0.016456, acc 0.992188\n",
      "2017-01-11T03:21:37.147108: step 10381, loss 0.0277729, acc 0.992188\n",
      "2017-01-11T03:21:39.180693: step 10382, loss 0.0392631, acc 0.992188\n",
      "2017-01-11T03:21:41.218568: step 10383, loss 0.00510771, acc 1\n",
      "2017-01-11T03:21:43.266137: step 10384, loss 0.0726652, acc 0.992188\n",
      "2017-01-11T03:21:45.295640: step 10385, loss 0.0105888, acc 0.992188\n",
      "2017-01-11T03:21:47.354702: step 10386, loss 0.00327908, acc 1\n",
      "2017-01-11T03:21:49.397344: step 10387, loss 0.069581, acc 0.992188\n",
      "2017-01-11T03:21:51.576492: step 10388, loss 0.0328569, acc 0.984375\n",
      "2017-01-11T03:21:53.805139: step 10389, loss 0.000171474, acc 1\n",
      "2017-01-11T03:21:55.840244: step 10390, loss 0.0454215, acc 0.984375\n",
      "2017-01-11T03:21:57.881935: step 10391, loss 0.00169379, acc 1\n",
      "2017-01-11T03:21:59.899341: step 10392, loss 0.000149358, acc 1\n",
      "2017-01-11T03:22:01.938253: step 10393, loss 0.00180654, acc 1\n",
      "2017-01-11T03:22:03.978210: step 10394, loss 0.0135074, acc 1\n",
      "2017-01-11T03:22:06.045586: step 10395, loss 0.00815431, acc 1\n",
      "2017-01-11T03:22:08.098924: step 10396, loss 0.00750688, acc 1\n",
      "2017-01-11T03:22:10.126823: step 10397, loss 0.0272655, acc 0.984375\n",
      "2017-01-11T03:22:12.172576: step 10398, loss 0.0484113, acc 0.984375\n",
      "2017-01-11T03:22:14.196658: step 10399, loss 0.0085416, acc 1\n",
      "2017-01-11T03:22:16.255821: step 10400, loss 0.001469, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:22:51.457647: step 10400, loss 0.0698268, acc 0.98416\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10400\n",
      "\n",
      "2017-01-11T03:22:56.152648: step 10401, loss 0.0170096, acc 0.992188\n",
      "2017-01-11T03:22:58.525872: step 10402, loss 0.0312935, acc 0.992188\n",
      "2017-01-11T03:23:00.652818: step 10403, loss 0.0198894, acc 0.992188\n",
      "2017-01-11T03:23:02.693005: step 10404, loss 0.0191015, acc 0.992188\n",
      "2017-01-11T03:23:04.800607: step 10405, loss 0.065101, acc 0.992188\n",
      "2017-01-11T03:23:06.900592: step 10406, loss 0.0355761, acc 0.992188\n",
      "2017-01-11T03:23:09.205927: step 10407, loss 0.0442329, acc 0.992188\n",
      "2017-01-11T03:23:11.337225: step 10408, loss 0.00329473, acc 1\n",
      "2017-01-11T03:23:13.523423: step 10409, loss 0.0121506, acc 0.992188\n",
      "2017-01-11T03:23:15.553208: step 10410, loss 4.43731e-05, acc 1\n",
      "2017-01-11T03:23:17.605738: step 10411, loss 0.00412116, acc 1\n",
      "2017-01-11T03:23:19.720170: step 10412, loss 0.0290147, acc 0.984375\n",
      "2017-01-11T03:23:21.776267: step 10413, loss 0.00107109, acc 1\n",
      "2017-01-11T03:23:24.122472: step 10414, loss 0.0817401, acc 0.96875\n",
      "2017-01-11T03:23:26.892668: step 10415, loss 0.0395765, acc 0.992188\n",
      "2017-01-11T03:23:28.927608: step 10416, loss 0.000205196, acc 1\n",
      "2017-01-11T03:23:31.291236: step 10417, loss 0.0169566, acc 0.992188\n",
      "2017-01-11T03:23:33.319218: step 10418, loss 0.0333201, acc 0.992188\n",
      "2017-01-11T03:23:35.486920: step 10419, loss 0.0148616, acc 0.992188\n",
      "2017-01-11T03:23:37.487148: step 10420, loss 0.0134953, acc 0.992188\n",
      "2017-01-11T03:23:39.533491: step 10421, loss 0.00629087, acc 1\n",
      "2017-01-11T03:23:41.570143: step 10422, loss 0.0514662, acc 0.984375\n",
      "2017-01-11T03:23:43.630233: step 10423, loss 0.0297002, acc 0.992188\n",
      "2017-01-11T03:23:45.625144: step 10424, loss 0.00972189, acc 1\n",
      "2017-01-11T03:23:47.680522: step 10425, loss 0.0368533, acc 0.992188\n",
      "2017-01-11T03:23:49.754295: step 10426, loss 0.022143, acc 0.984375\n",
      "2017-01-11T03:23:51.785759: step 10427, loss 0.0583612, acc 0.976562\n",
      "2017-01-11T03:23:53.816958: step 10428, loss 0.0161783, acc 0.992188\n",
      "2017-01-11T03:23:55.864384: step 10429, loss 0.152169, acc 0.976562\n",
      "2017-01-11T03:23:58.020177: step 10430, loss 0.0191546, acc 1\n",
      "2017-01-11T03:24:00.197103: step 10431, loss 0.00141201, acc 1\n",
      "2017-01-11T03:24:02.580975: step 10432, loss 0.0324962, acc 0.992188\n",
      "2017-01-11T03:24:04.678109: step 10433, loss 0.000396498, acc 1\n",
      "2017-01-11T03:24:06.737659: step 10434, loss 0.0527725, acc 0.976562\n",
      "2017-01-11T03:24:08.762640: step 10435, loss 0.00306951, acc 1\n",
      "2017-01-11T03:24:10.841216: step 10436, loss 0.0230736, acc 0.992188\n",
      "2017-01-11T03:24:12.898871: step 10437, loss 0.0100864, acc 1\n",
      "2017-01-11T03:24:14.937703: step 10438, loss 0.000970863, acc 1\n",
      "2017-01-11T03:24:17.143045: step 10439, loss 0.00499628, acc 1\n",
      "2017-01-11T03:24:19.165257: step 10440, loss 0.0084612, acc 0.992188\n",
      "2017-01-11T03:24:21.260824: step 10441, loss 0.0294903, acc 0.984375\n",
      "2017-01-11T03:24:23.333189: step 10442, loss 0.00254976, acc 1\n",
      "2017-01-11T03:24:25.310381: step 10443, loss 0.0316357, acc 0.992188\n",
      "2017-01-11T03:24:27.318441: step 10444, loss 0.0112875, acc 0.992188\n",
      "2017-01-11T03:24:29.340408: step 10445, loss 0.00138179, acc 1\n",
      "2017-01-11T03:24:31.466957: step 10446, loss 0.0191225, acc 0.992188\n",
      "2017-01-11T03:24:34.785523: step 10447, loss 0.0122264, acc 1\n",
      "2017-01-11T03:24:37.119865: step 10448, loss 0.00176588, acc 1\n",
      "2017-01-11T03:24:39.153253: step 10449, loss 0.0137102, acc 0.992188\n",
      "2017-01-11T03:24:41.230460: step 10450, loss 0.007765, acc 1\n",
      "2017-01-11T03:24:43.299136: step 10451, loss 0.0167994, acc 0.992188\n",
      "2017-01-11T03:24:45.346582: step 10452, loss 0.0121985, acc 0.992188\n",
      "2017-01-11T03:24:47.461525: step 10453, loss 0.149946, acc 0.992188\n",
      "2017-01-11T03:24:49.517518: step 10454, loss 0.0123993, acc 0.992188\n",
      "2017-01-11T03:24:51.612229: step 10455, loss 0.0215223, acc 0.992188\n",
      "2017-01-11T03:24:53.596827: step 10456, loss 0.0148733, acc 1\n",
      "2017-01-11T03:24:55.663919: step 10457, loss 0.0417213, acc 0.984375\n",
      "2017-01-11T03:24:57.691514: step 10458, loss 0.011199, acc 1\n",
      "2017-01-11T03:24:59.752644: step 10459, loss 3.72584e-05, acc 1\n",
      "2017-01-11T03:25:01.790591: step 10460, loss 0.00438574, acc 1\n",
      "2017-01-11T03:25:03.806459: step 10461, loss 0.0066761, acc 1\n",
      "2017-01-11T03:31:14.617199: step 10608, loss 0.00382872, acc 1\n",
      "2017-01-11T03:31:16.609304: step 10609, loss 0.0259518, acc 1\n",
      "2017-01-11T03:31:18.666996: step 10610, loss 0.0387944, acc 0.984375\n",
      "2017-01-11T03:31:20.753099: step 10611, loss 0.0276037, acc 0.984375\n",
      "2017-01-11T03:31:22.791667: step 10612, loss 2.32722e-06, acc 1\n",
      "2017-01-11T03:31:24.832084: step 10613, loss 0.0352427, acc 0.984375\n",
      "2017-01-11T03:31:26.857787: step 10614, loss 3.58574e-05, acc 1\n",
      "2017-01-11T03:31:28.917953: step 10615, loss 0.0233729, acc 0.992188\n",
      "2017-01-11T03:31:30.957103: step 10616, loss 0.000598075, acc 1\n",
      "2017-01-11T03:31:32.978794: step 10617, loss 0.0278735, acc 0.992188\n",
      "2017-01-11T03:31:35.397251: step 10618, loss 0.0188243, acc 0.992188\n",
      "2017-01-11T03:31:37.392527: step 10619, loss 0.00462648, acc 1\n",
      "2017-01-11T03:31:39.442432: step 10620, loss 1.01604e-06, acc 1\n",
      "2017-01-11T03:31:41.551247: step 10621, loss 0.0195975, acc 0.992188\n",
      "2017-01-11T03:31:43.618741: step 10622, loss 0.0146629, acc 0.992188\n",
      "2017-01-11T03:31:45.654717: step 10623, loss 0.000426895, acc 1\n",
      "2017-01-11T03:31:47.699614: step 10624, loss 0.000861369, acc 1\n",
      "2017-01-11T03:31:49.761658: step 10625, loss 0.000737077, acc 1\n",
      "2017-01-11T03:31:51.830343: step 10626, loss 0.00365146, acc 1\n",
      "2017-01-11T03:31:53.870708: step 10627, loss 0.0684987, acc 0.984375\n",
      "2017-01-11T03:31:55.897162: step 10628, loss 0.0348363, acc 0.992188\n",
      "2017-01-11T03:31:57.923553: step 10629, loss 0.00116273, acc 1\n",
      "2017-01-11T03:31:59.976045: step 10630, loss 0.021701, acc 0.992188\n",
      "2017-01-11T03:32:02.008252: step 10631, loss 0.001397, acc 1\n",
      "2017-01-11T03:32:04.091694: step 10632, loss 0.00880852, acc 1\n",
      "2017-01-11T03:32:06.232647: step 10633, loss 0.00188631, acc 1\n",
      "2017-01-11T03:32:08.492079: step 10634, loss 0.000301303, acc 1\n",
      "2017-01-11T03:32:10.512452: step 10635, loss 0.0104323, acc 0.992188\n",
      "2017-01-11T03:32:12.553845: step 10636, loss 0.000865272, acc 1\n",
      "2017-01-11T03:32:14.574801: step 10637, loss 0.00627547, acc 1\n",
      "2017-01-11T03:32:16.625149: step 10638, loss 0.000994143, acc 1\n",
      "2017-01-11T03:32:18.635278: step 10639, loss 0.0281718, acc 0.992188\n",
      "2017-01-11T03:32:20.715482: step 10640, loss 0.0421264, acc 0.984375\n",
      "2017-01-11T03:32:22.803744: step 10641, loss 0.00890692, acc 1\n",
      "2017-01-11T03:32:24.859207: step 10642, loss 0.0265756, acc 0.992188\n",
      "2017-01-11T03:32:26.898956: step 10643, loss 0.0217504, acc 0.992188\n",
      "2017-01-11T03:32:28.919059: step 10644, loss 2.29077e-06, acc 1\n",
      "2017-01-11T03:32:31.039273: step 10645, loss 0.040028, acc 0.984375\n",
      "2017-01-11T03:32:33.086479: step 10646, loss 0.0028728, acc 1\n",
      "2017-01-11T03:32:35.137426: step 10647, loss 0.0023388, acc 1\n",
      "2017-01-11T03:32:37.156309: step 10648, loss 0.000872811, acc 1\n",
      "2017-01-11T03:32:39.542061: step 10649, loss 0.0067649, acc 0.992188\n",
      "2017-01-11T03:32:41.696869: step 10650, loss 0.0211979, acc 0.992188\n",
      "2017-01-11T03:32:43.741822: step 10651, loss 0.0616174, acc 0.984375\n",
      "2017-01-11T03:32:45.775037: step 10652, loss 0.000221517, acc 1\n",
      "2017-01-11T03:32:47.800782: step 10653, loss 0.0108547, acc 1\n",
      "2017-01-11T03:32:49.883222: step 10654, loss 0.0124168, acc 0.992188\n",
      "2017-01-11T03:32:51.953459: step 10655, loss 0.000605245, acc 1\n",
      "2017-01-11T03:32:53.997580: step 10656, loss 0.000208423, acc 1\n",
      "2017-01-11T03:32:56.049297: step 10657, loss 0.0197426, acc 0.992188\n",
      "2017-01-11T03:32:58.093947: step 10658, loss 0.0265064, acc 0.992188\n",
      "2017-01-11T03:33:00.135805: step 10659, loss 0.0124757, acc 1\n",
      "2017-01-11T03:33:02.194218: step 10660, loss 0.000294463, acc 1\n",
      "2017-01-11T03:33:04.231443: step 10661, loss 0.0138018, acc 0.992188\n",
      "2017-01-11T03:33:06.276909: step 10662, loss 0.00304671, acc 1\n",
      "2017-01-11T03:33:08.299187: step 10663, loss 0.0105978, acc 0.992188\n",
      "2017-01-11T03:33:10.583872: step 10664, loss 0.0684187, acc 0.992188\n",
      "2017-01-11T03:33:12.675648: step 10665, loss 0.0282883, acc 0.992188\n",
      "2017-01-11T03:33:14.686635: step 10666, loss 0.0179082, acc 0.992188\n",
      "2017-01-11T03:33:16.713307: step 10667, loss 0.000209079, acc 1\n",
      "2017-01-11T03:33:18.740821: step 10668, loss 4.72312e-06, acc 1\n",
      "2017-01-11T03:33:20.982022: step 10669, loss 9.63148e-05, acc 1\n",
      "2017-01-11T03:33:23.040557: step 10670, loss 0.0128731, acc 1\n",
      "2017-01-11T03:33:25.079451: step 10671, loss 0.0221479, acc 0.992188\n",
      "2017-01-11T03:33:27.076974: step 10672, loss 0.000334096, acc 1\n",
      "2017-01-11T03:33:29.121133: step 10673, loss 0.00607337, acc 1\n",
      "2017-01-11T03:33:31.154922: step 10674, loss 0.000154022, acc 1\n",
      "2017-01-11T03:33:33.234884: step 10675, loss 0.000591513, acc 1\n",
      "2017-01-11T03:33:35.304686: step 10676, loss 0.00623816, acc 1\n",
      "2017-01-11T03:33:38.324141: step 10677, loss 0.0660672, acc 0.96875\n",
      "2017-01-11T03:33:40.479479: step 10678, loss 0.0292899, acc 0.992188\n",
      "2017-01-11T03:33:42.941855: step 10679, loss 0.0632302, acc 0.984375\n",
      "2017-01-11T03:33:44.945922: step 10680, loss 0.0246718, acc 0.992188\n",
      "2017-01-11T03:33:47.006372: step 10681, loss 1.2808e-05, acc 1\n",
      "2017-01-11T03:33:49.056183: step 10682, loss 0.00599784, acc 1\n",
      "2017-01-11T03:33:51.114605: step 10683, loss 0.00555346, acc 1\n",
      "2017-01-11T03:33:53.171825: step 10684, loss 0.0010128, acc 1\n",
      "2017-01-11T03:33:55.230255: step 10685, loss 0.0401504, acc 0.992188\n",
      "2017-01-11T03:33:57.216424: step 10686, loss 0.0153418, acc 0.992188\n",
      "2017-01-11T03:33:59.251853: step 10687, loss 0.000762678, acc 1\n",
      "2017-01-11T03:34:01.298842: step 10688, loss 0.000273783, acc 1\n",
      "2017-01-11T03:34:03.352349: step 10689, loss 5.17716e-05, acc 1\n",
      "2017-01-11T03:34:05.452963: step 10690, loss 0.0479901, acc 0.984375\n",
      "2017-01-11T03:34:07.487857: step 10691, loss 0.0246524, acc 0.992188\n",
      "2017-01-11T03:34:09.511887: step 10692, loss 0.0314898, acc 0.992188\n",
      "2017-01-11T03:34:11.557167: step 10693, loss 0.0014135, acc 1\n",
      "2017-01-11T03:34:13.608225: step 10694, loss 0.0100179, acc 1\n",
      "2017-01-11T03:34:15.954339: step 10695, loss 0.00509856, acc 1\n",
      "2017-01-11T03:34:18.003115: step 10696, loss 0.0226066, acc 0.984375\n",
      "2017-01-11T03:34:20.090507: step 10697, loss 0.000376758, acc 1\n",
      "2017-01-11T03:34:22.177138: step 10698, loss 5.48407e-05, acc 1\n",
      "2017-01-11T03:34:24.205694: step 10699, loss 0.0224478, acc 0.992188\n",
      "2017-01-11T03:34:26.286874: step 10700, loss 0.0079, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:35:03.316508: step 10700, loss 0.0731253, acc 0.9836\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10700\n",
      "\n",
      "2017-01-11T03:35:08.153766: step 10701, loss 0.0109126, acc 1\n",
      "2017-01-11T03:35:10.178186: step 10702, loss 0.0232809, acc 0.992188\n",
      "2017-01-11T03:35:12.223864: step 10703, loss 0.000163128, acc 1\n",
      "2017-01-11T03:35:14.257608: step 10704, loss 0.00225102, acc 1\n",
      "2017-01-11T03:35:16.345024: step 10705, loss 0.0762252, acc 0.984375\n",
      "2017-01-11T03:35:18.455232: step 10706, loss 0.00739489, acc 0.992188\n",
      "2017-01-11T03:35:20.581555: step 10707, loss 0.00433252, acc 1\n",
      "2017-01-11T03:35:23.016440: step 10708, loss 0.013332, acc 0.992188\n",
      "2017-01-11T03:35:25.230129: step 10709, loss 0.0181402, acc 0.992188\n",
      "2017-01-11T03:35:27.285592: step 10710, loss 0.0082229, acc 1\n",
      "2017-01-11T03:35:29.522700: step 10711, loss 0.0273884, acc 0.992188\n",
      "2017-01-11T03:35:31.564312: step 10712, loss 0.00149566, acc 1\n",
      "2017-01-11T03:35:33.647723: step 10713, loss 0.01343, acc 0.992188\n",
      "2017-01-11T03:35:35.727849: step 10714, loss 0.00872358, acc 1\n",
      "2017-01-11T03:35:37.783837: step 10715, loss 0.000138608, acc 1\n",
      "2017-01-11T03:35:39.840228: step 10716, loss 0.005221, acc 1\n",
      "2017-01-11T03:35:41.884932: step 10717, loss 5.40853e-05, acc 1\n",
      "2017-01-11T03:35:44.383649: step 10718, loss 0.00634911, acc 0.992188\n",
      "2017-01-11T03:35:47.097576: step 10719, loss 0.0038273, acc 1\n",
      "2017-01-11T03:35:49.103254: step 10720, loss 3.95473e-05, acc 1\n",
      "2017-01-11T03:35:51.159402: step 10721, loss 0.008031, acc 1\n",
      "2017-01-11T03:35:53.254119: step 10722, loss 0.00212431, acc 1\n",
      "2017-01-11T03:35:55.590046: step 10723, loss 0.002725, acc 1\n",
      "2017-01-11T03:35:57.619807: step 10724, loss 0.0415188, acc 0.984375\n",
      "2017-01-11T03:35:59.652869: step 10725, loss 0.000518327, acc 1\n",
      "2017-01-11T03:36:01.722878: step 10726, loss 0.0143845, acc 0.992188\n",
      "2017-01-11T03:36:03.760627: step 10727, loss 0.0193531, acc 0.992188\n",
      "2017-01-11T03:36:05.821054: step 10728, loss 0.0226448, acc 0.992188\n",
      "2017-01-11T03:36:07.846849: step 10729, loss 0.0169446, acc 1\n",
      "2017-01-11T03:36:09.905538: step 10730, loss 0.0345857, acc 0.984375\n",
      "2017-01-11T03:36:11.968950: step 10731, loss 0.0238072, acc 0.992188\n",
      "2017-01-11T03:36:13.977288: step 10732, loss 0.00282811, acc 1\n",
      "2017-01-11T03:36:16.021200: step 10733, loss 0.0369866, acc 0.992188\n",
      "2017-01-11T03:36:18.074602: step 10734, loss 3.5343e-05, acc 1\n",
      "2017-01-11T03:36:20.150847: step 10735, loss 8.94792e-05, acc 1\n",
      "2017-01-11T03:36:22.241428: step 10736, loss 0.00398573, acc 1\n",
      "2017-01-11T03:36:24.280599: step 10737, loss 0.0112815, acc 0.992188\n",
      "2017-01-11T03:36:26.328965: step 10738, loss 0.00326924, acc 1\n",
      "2017-01-11T03:36:28.679398: step 10739, loss 0.00571531, acc 1\n",
      "2017-01-11T03:36:30.806565: step 10740, loss 0.0337235, acc 0.992188\n",
      "2017-01-11T03:36:32.877032: step 10741, loss 0.0107567, acc 1\n",
      "2017-01-11T03:36:34.970251: step 10742, loss 0.00619212, acc 1\n",
      "2017-01-11T03:36:36.994943: step 10743, loss 0.000322221, acc 1\n",
      "2017-01-11T03:36:39.020707: step 10744, loss 0.0092492, acc 0.992188\n",
      "2017-01-11T03:36:41.359453: step 10745, loss 0.000939676, acc 1\n",
      "2017-01-11T03:36:43.756805: step 10746, loss 0.00389878, acc 1\n",
      "2017-01-11T03:36:45.913795: step 10747, loss 0.0490776, acc 0.984375\n",
      "2017-01-11T03:36:48.115132: step 10748, loss 0.000757748, acc 1\n",
      "2017-01-11T03:36:50.292888: step 10749, loss 0.0302662, acc 0.992188\n",
      "2017-01-11T03:36:52.362231: step 10750, loss 0.00333979, acc 1\n",
      "2017-01-11T03:36:54.395757: step 10751, loss 0.00614055, acc 1\n",
      "2017-01-11T03:36:56.468144: step 10752, loss 0.0174432, acc 0.992188\n",
      "2017-01-11T03:36:58.541343: step 10753, loss 0.0177436, acc 0.992188\n",
      "2017-01-11T03:37:00.929122: step 10754, loss 0.000424677, acc 1\n",
      "2017-01-11T03:37:02.980083: step 10755, loss 0.00348174, acc 1\n",
      "2017-01-11T03:37:05.085356: step 10756, loss 0.000665114, acc 1\n",
      "2017-01-11T03:37:07.171360: step 10757, loss 0.0251243, acc 0.992188\n",
      "2017-01-11T03:37:09.185168: step 10758, loss 0.00024357, acc 1\n",
      "2017-01-11T03:37:11.205783: step 10759, loss 0.000247788, acc 1\n",
      "2017-01-11T03:37:13.247413: step 10760, loss 0.00058553, acc 1\n",
      "2017-01-11T03:37:15.252630: step 10761, loss 0.00118594, acc 1\n",
      "2017-01-11T03:37:17.300761: step 10762, loss 0.0184372, acc 0.992188\n",
      "2017-01-11T03:37:19.328830: step 10763, loss 0.0120834, acc 0.992188\n",
      "2017-01-11T03:37:21.432891: step 10764, loss 0.0262431, acc 0.984375\n",
      "2017-01-11T03:37:23.468315: step 10765, loss 0.0165277, acc 0.992188\n",
      "2017-01-11T03:37:25.534693: step 10766, loss 0.0396125, acc 0.992188\n",
      "2017-01-11T03:37:27.573839: step 10767, loss 0.102811, acc 0.976562\n",
      "2017-01-11T03:37:29.600310: step 10768, loss 0.0160032, acc 0.992188\n",
      "2017-01-11T03:37:31.953557: step 10769, loss 0.0102983, acc 0.992188\n",
      "2017-01-11T03:37:33.962773: step 10770, loss 0.0122503, acc 1\n",
      "2017-01-11T03:37:36.035768: step 10771, loss 0.00182245, acc 1\n",
      "2017-01-11T03:37:38.060631: step 10772, loss 0.0396693, acc 0.984375\n",
      "2017-01-11T03:37:40.117383: step 10773, loss 0.00236677, acc 1\n",
      "2017-01-11T03:37:42.386700: step 10774, loss 0.00251378, acc 1\n",
      "2017-01-11T03:37:44.553480: step 10775, loss 0.00686727, acc 1\n",
      "2017-01-11T03:37:46.560269: step 10776, loss 0.0753413, acc 0.984375\n",
      "2017-01-11T03:37:48.580417: step 10777, loss 0.0111299, acc 0.992188\n",
      "2017-01-11T03:37:50.652642: step 10778, loss 0.00346439, acc 1\n",
      "2017-01-11T03:37:52.725591: step 10779, loss 0.0100241, acc 1\n",
      "2017-01-11T03:37:54.758526: step 10780, loss 0.00626798, acc 1\n",
      "2017-01-11T03:37:56.816047: step 10781, loss 0.0209221, acc 0.992188\n",
      "2017-01-11T03:37:58.840091: step 10782, loss 0.00474853, acc 1\n",
      "2017-01-11T03:38:00.890369: step 10783, loss 0.0266762, acc 0.984375\n",
      "2017-01-11T03:38:03.197252: step 10784, loss 0.0240126, acc 0.984375\n",
      "2017-01-11T03:38:05.483638: step 10785, loss 0.0017745, acc 1\n",
      "2017-01-11T03:38:07.610865: step 10786, loss 0.000288204, acc 1\n",
      "2017-01-11T03:38:09.695012: step 10787, loss 0.0142458, acc 1\n",
      "2017-01-11T03:38:11.758117: step 10788, loss 3.98835e-05, acc 1\n",
      "2017-01-11T03:38:13.776359: step 10789, loss 0.0318683, acc 0.992188\n",
      "2017-01-11T03:38:15.781547: step 10790, loss 0.00808564, acc 1\n",
      "2017-01-11T03:38:17.826925: step 10791, loss 0.00311771, acc 1\n",
      "2017-01-11T03:38:19.953985: step 10792, loss 0.000314382, acc 1\n",
      "2017-01-11T03:38:22.011770: step 10793, loss 0.000385596, acc 1\n",
      "2017-01-11T03:38:24.046178: step 10794, loss 0.0397628, acc 0.984375\n",
      "2017-01-11T03:38:26.269859: step 10795, loss 0.0390985, acc 0.992188\n",
      "2017-01-11T03:38:28.288885: step 10796, loss 0.00393101, acc 1\n",
      "2017-01-11T03:38:30.388926: step 10797, loss 0.0382957, acc 0.992188\n",
      "2017-01-11T03:38:32.608591: step 10798, loss 0.0376777, acc 0.992188\n",
      "2017-01-11T03:38:34.789323: step 10799, loss 0.00753521, acc 1\n",
      "2017-01-11T03:38:37.073674: step 10800, loss 0.00531695, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-01-11T03:39:03.658074: step 10800, loss 0.0732301, acc 0.98356\n",
      "\n",
      "Saved model checkpoint to /Users/ostegmaier/Documents/personal_code/predict-language/runs/1484108460/checkpoints/model-10800\n",
      "\n",
      "2017-01-11T03:39:08.398324: step 10801, loss 0.0104575, acc 0.992188\n",
      "2017-01-11T03:39:10.415978: step 10802, loss 0.000844927, acc 1\n",
      "2017-01-11T03:39:12.449321: step 10803, loss 0.0106621, acc 1\n",
      "2017-01-11T03:39:14.434637: step 10804, loss 0.00845306, acc 1\n",
      "2017-01-11T03:39:16.471068: step 10805, loss 0.0147791, acc 0.992188\n",
      "2017-01-11T03:39:18.534618: step 10806, loss 0.00221353, acc 1\n",
      "2017-01-11T03:39:20.591152: step 10807, loss 0.010062, acc 1\n",
      "2017-01-11T03:39:22.666319: step 10808, loss 0.00596417, acc 1\n",
      "2017-01-11T03:39:24.708997: step 10809, loss 0.00403133, acc 1\n",
      "2017-01-11T03:39:26.752897: step 10810, loss 0.00183588, acc 1\n",
      "2017-01-11T03:39:28.799523: step 10811, loss 0.0512347, acc 0.984375\n",
      "2017-01-11T03:39:30.840389: step 10812, loss 0.00808511, acc 1\n",
      "2017-01-11T03:39:32.862574: step 10813, loss 0.0388223, acc 0.984375\n",
      "2017-01-11T03:39:35.128998: step 10814, loss 0.0256567, acc 0.992188\n",
      "2017-01-11T03:39:37.175004: step 10815, loss 0.0420089, acc 0.992188\n",
      "2017-01-11T03:39:39.426605: step 10816, loss 0.0134316, acc 0.992188\n",
      "2017-01-11T03:39:41.778754: step 10817, loss 0.00431662, acc 1\n",
      "2017-01-11T03:39:43.823480: step 10818, loss 0.00509318, acc 1\n",
      "2017-01-11T03:39:45.955342: step 10819, loss 0.00135158, acc 1\n",
      "2017-01-11T03:39:48.022871: step 10820, loss 0.00997271, acc 1\n",
      "2017-01-11T03:39:50.892679: step 10821, loss 0.0226332, acc 0.992188\n",
      "2017-01-11T03:39:53.221465: step 10822, loss 0.0201175, acc 0.992188\n",
      "2017-01-11T03:39:55.251670: step 10823, loss 0.0108999, acc 1\n",
      "2017-01-11T03:39:57.247652: step 10824, loss 2.50042e-05, acc 1\n",
      "2017-01-11T03:39:59.298064: step 10825, loss 0.0328919, acc 0.992188\n",
      "2017-01-11T03:40:01.366209: step 10826, loss 3.97738e-06, acc 1\n",
      "2017-01-11T03:40:03.407610: step 10827, loss 0.0338105, acc 0.992188\n",
      "2017-01-11T03:40:05.507847: step 10828, loss 0.00525872, acc 1\n",
      "2017-01-11T03:40:07.577671: step 10829, loss 0.000737026, acc 1\n",
      "2017-01-11T03:40:09.608993: step 10830, loss 0.0035698, acc 1\n",
      "2017-01-11T03:40:11.913670: step 10831, loss 0.00502057, acc 1\n",
      "2017-01-11T03:40:14.009470: step 10832, loss 0.0673334, acc 0.984375\n",
      "2017-01-11T03:40:16.052724: step 10833, loss 0.0381482, acc 0.992188\n",
      "2017-01-11T03:40:18.077558: step 10834, loss 0.001722, acc 1\n",
      "2017-01-11T03:40:20.160482: step 10835, loss 0.000366829, acc 1\n",
      "2017-01-11T03:40:22.219646: step 10836, loss 4.31281e-05, acc 1\n",
      "2017-01-11T03:40:24.269952: step 10837, loss 0.00440051, acc 1\n",
      "2017-01-11T03:40:26.288987: step 10838, loss 0.0319519, acc 0.992188\n",
      "2017-01-11T03:40:28.316168: step 10839, loss 0.0276934, acc 0.992188\n",
      "2017-01-11T03:40:30.397873: step 10840, loss 0.000315368, acc 1\n",
      "2017-01-11T03:40:32.599455: step 10841, loss 0.00104156, acc 1\n",
      "2017-01-11T03:40:35.045099: step 10842, loss 0.0179156, acc 0.992188\n",
      "2017-01-11T03:40:37.084633: step 10843, loss 0.00272502, acc 1\n",
      "2017-01-11T03:40:39.108683: step 10844, loss 0.00627005, acc 1\n",
      "2017-01-11T03:40:41.134481: step 10845, loss 0.0297097, acc 0.992188\n",
      "2017-01-11T03:40:43.362493: step 10846, loss 5.68079e-05, acc 1\n",
      "2017-01-11T03:40:45.629606: step 10847, loss 0.000218171, acc 1\n",
      "2017-01-11T03:40:47.672425: step 10848, loss 1.3062e-05, acc 1\n",
      "2017-01-11T03:40:49.746495: step 10849, loss 0.000724418, acc 1\n",
      "2017-01-11T03:40:52.769947: step 10850, loss 0.0347607, acc 0.984375\n",
      "2017-01-11T03:40:55.009524: step 10851, loss 0.0142026, acc 0.992188\n",
      "2017-01-11T03:40:57.083486: step 10852, loss 0.00249509, acc 1\n",
      "2017-01-11T03:40:59.125637: step 10853, loss 6.38234e-05, acc 1\n",
      "2017-01-11T03:41:01.156656: step 10854, loss 0.00406694, acc 1\n",
      "2017-01-11T03:41:03.172349: step 10855, loss 0.0435815, acc 0.992188\n",
      "2017-01-11T03:41:05.250039: step 10856, loss 0.00181456, acc 1\n",
      "2017-01-11T03:41:07.273784: step 10857, loss 0.0613754, acc 0.984375\n",
      "2017-01-11T03:41:09.218973: step 10858, loss 0.00146709, acc 1\n",
      "2017-01-11T03:41:11.260312: step 10859, loss 0.0213989, acc 0.992188\n",
      "2017-01-11T03:41:13.319470: step 10860, loss 0.036527, acc 0.992188\n",
      "2017-01-11T03:41:15.454998: step 10861, loss 0.0107595, acc 1\n",
      "2017-01-11T03:41:17.667224: step 10862, loss 0.00264501, acc 1\n",
      "2017-01-11T03:41:19.758362: step 10863, loss 0.0422166, acc 0.984375\n",
      "2017-01-11T03:41:21.815294: step 10864, loss 0.0562012, acc 0.984375\n",
      "2017-01-11T03:41:23.845426: step 10865, loss 0.0208421, acc 0.992188\n",
      "2017-01-11T03:41:25.887809: step 10866, loss 0.010686, acc 0.992188\n",
      "2017-01-11T03:41:27.961462: step 10867, loss 0.00243022, acc 1\n",
      "2017-01-11T03:41:29.991820: step 10868, loss 0.000258325, acc 1\n",
      "2017-01-11T03:41:32.044238: step 10869, loss 0.010403, acc 0.992188\n",
      "2017-01-11T03:41:35.259633: step 10870, loss 0.000451814, acc 1\n",
      "2017-01-11T03:41:37.329022: step 10871, loss 0.0124033, acc 0.992188\n",
      "2017-01-11T03:41:39.363060: step 10872, loss 0.0235417, acc 0.992188\n",
      "2017-01-11T03:41:41.387002: step 10873, loss 0.00517879, acc 1\n",
      "2017-01-11T03:41:43.415036: step 10874, loss 0.0129719, acc 1\n",
      "2017-01-11T03:41:45.563253: step 10875, loss 0.00623806, acc 0.992188\n",
      "2017-01-11T03:41:47.738801: step 10876, loss 0.0796715, acc 0.992188\n",
      "2017-01-11T03:41:49.992162: step 10877, loss 0.00270964, acc 1\n",
      "2017-01-11T03:41:52.030220: step 10878, loss 0.005964, acc 1\n",
      "2017-01-11T03:41:54.057145: step 10879, loss 0.00871622, acc 0.992188\n",
      "2017-01-11T03:41:56.112145: step 10880, loss 0.00800739, acc 1\n",
      "2017-01-11T03:41:58.172048: step 10881, loss 0.0050873, acc 1\n",
      "2017-01-11T03:42:00.206906: step 10882, loss 0.000477128, acc 1\n",
      "2017-01-11T03:42:02.212097: step 10883, loss 0.001822, acc 1\n",
      "2017-01-11T03:42:04.286968: step 10884, loss 0.0211066, acc 0.992188\n",
      "2017-01-11T03:42:06.377138: step 10885, loss 0.0413149, acc 0.984375\n",
      "2017-01-11T03:42:08.425082: step 10886, loss 0.0122014, acc 0.992188\n",
      "2017-01-11T03:42:10.467368: step 10887, loss 0.0417933, acc 1\n",
      "2017-01-11T03:42:12.518851: step 10888, loss 0.0158454, acc 0.992188\n",
      "2017-01-11T03:42:14.547212: step 10889, loss 0.0915093, acc 0.976562\n",
      "2017-01-11T03:42:16.595338: step 10890, loss 0.00172985, acc 1\n",
      "2017-01-11T03:42:18.646619: step 10891, loss 0.00996106, acc 1\n",
      "2017-01-11T03:42:21.063122: step 10892, loss 0.011449, acc 0.992188\n",
      "2017-01-11T03:42:23.138990: step 10893, loss 0.0253577, acc 0.992188\n",
      "2017-01-11T03:42:25.144756: step 10894, loss 0.0111829, acc 0.992188\n",
      "2017-01-11T03:42:27.211987: step 10895, loss 0.00225087, acc 1\n",
      "2017-01-11T03:42:29.265315: step 10896, loss 0.0193682, acc 1\n",
      "2017-01-11T03:42:31.424646: step 10897, loss 0.0222556, acc 0.992188\n",
      "2017-01-11T03:42:33.439185: step 10898, loss 5.95754e-06, acc 1\n",
      "2017-01-11T03:42:35.495467: step 10899, loss 0.029169, acc 0.992188\n",
      "2017-01-11T03:42:37.526097: step 10900, loss 0.0193395, acc 0.992188\n",
      "\n",
      "Evaluation:\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
