{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "def stripXML(t):\n",
    "    '''Strips XML and comments from text'''\n",
    "    if len(t) == 0:\n",
    "        return False\n",
    "    elif t[0] in  ['<', '(']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def onlyLetters(t):\n",
    "    '''Keeps only letters in any language'''\n",
    "    try: \n",
    "        words = re.findall(r'[^\\W_0-9]+',t.decode('utf8'),re.U)\n",
    "        return \" \".join([w for w in words])\n",
    "    except: \n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def makeRecord(t, lang, pad=\"<PAD>\", n=70):\n",
    "    '''Takes sentence and adds a comma,\n",
    "    the language and a newline -> CSV record, \n",
    "    pads the sentence with <PAD> word to get to 70 words'''\n",
    "    lang = lang.replace('/', '')\n",
    "    try:\n",
    "        sentence = onlyLetters(t).encode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return \"\"\n",
    "    length = len(sentence.split(' '))\n",
    "    padwords = \" \" + \" \".join([pad for i in range(n - length + 1)])\n",
    "    sentence = sentence + padwords\n",
    "    record = sentence + \", {0}\\n\".format(lang)\n",
    "    return record\n",
    "\n",
    "\n",
    "def splitNWords(t, n=70):\n",
    "    sentence = onlyLetters(t).encode('utf-8')\n",
    "    words = sentence.split(' ')\n",
    "    sentences = [\" \".join(words[i:i+n]) for i in range(0, len(words), n)]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little utility script to merge all the separate files into one big CSV per language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strip out XML tags and non letters\n",
    "- Had to deal with cyrillic alphabet and non-ascii\n",
    "- Split into senteces of 70 words or less and then padded with `<PAD>` for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = './clean_txt_data/'\n",
    "txtdir = './raw_txt/'\n",
    "already_done = [d.split('_')[-1].split('.')[0] for d in os.listdir(datadir) if '.csv' in d]\n",
    "langs = [d for d in os.listdir(txtdir) if os.path.isdir(txtdir + d) and d not in already_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    out_file = open(datadir + '/cleaned_data_{}.csv'.format(lang), 'a')\n",
    "    lang = lang + '/'\n",
    "    txt_files = os.listdir(txtdir + lang)\n",
    "    print \"Starting {}, {} files to read.\".format(lang, len(txt_files))\n",
    "    for txt in txt_files:\n",
    "        #Read original text\n",
    "        with open(txtdir + lang + txt, 'r') as f:\n",
    "            txt = f.read()\n",
    "        #Split into smaller 70 word sentences & flatten it\n",
    "        txt_list = [splitNWords(t) for t in txt.split('\\n') if stripXML(t)]\n",
    "        txt_list = [item for sublist in txt_list for item in sublist]\n",
    "        #Create CSV record: pad to 70 words, add language Split on newline\n",
    "        txt_list = [makeRecord(t, lang) for t in txt_list]\n",
    "        #Write to en file\n",
    "        out_file.writelines(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all languages into one large file\n",
    "- Allows us to create a vocab and then split into smaller stratified sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = [d for d in os.listdir(txtdir) if os.path.isdir(txtdir + d)]\n",
    "colnames = ['txt', 'lang']\n",
    "df = pd.DataFrame({}, columns=colnames)\n",
    "for lang in langs:\n",
    "    lang_df = pd.read_csv('./clean_txt_data/cleaned_data_{}.csv'.format(lang), names=colnames)\n",
    "    df = pd.concat([df, lang_df])\n",
    "\n",
    "    del(lang_df)\n",
    "df.to_csv('./clean_txt_data/cleaned_data_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 13891597\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./clean_txt_data/cleaned_data_all.csv')\n",
    "print \"Total sentences: {}\".format(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9317166</th>\n",
       "      <td>Gerb pirmininke viskas ko norėčiau paprašyti t...</td>\n",
       "      <td>lt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068382</th>\n",
       "      <td>Ψήφισα υπέρ της έκθεσης του κ Falbr διότι εστι...</td>\n",
       "      <td>el</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786811</th>\n",
       "      <td>Angesichts eines Vorschlags der auch die itali...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142989</th>\n",
       "      <td>Ik ben van mening dat het van groot belang is ...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829904</th>\n",
       "      <td>folgenden Unsicherheit für den Sektor &lt;PAD&gt; &lt;P...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        txt lang\n",
       "9317166   Gerb pirmininke viskas ko norėčiau paprašyti t...   lt\n",
       "3068382   Ψήφισα υπέρ της έκθεσης του κ Falbr διότι εστι...   el\n",
       "1786811   Angesichts eines Vorschlags der auch die itali...   de\n",
       "10142989  Ik ben van mening dat het van groot belang is ...   nl\n",
       "1829904   folgenden Unsicherheit für den Sektor <PAD> <P...   de"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab index\n",
    "- Shuffle the data set\n",
    "- split into sets of 400k - 500k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create arrays of X and y, removed df from memory\n",
    "x_text = df.txt.values\n",
    "y = pd.get_dummies(df.lang).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' bg' ' cs' ' da' ' de' ' el' ' en' ' es' ' et' ' fi' ' fr' ' hu' ' it'\n",
      " ' lt' ' lv' ' nl' ' pl' ' pt' ' ro' ' sk' ' sl' ' sv']\n"
     ]
    }
   ],
   "source": [
    "print pd.get_dummies(df.lang).columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished vocab mapping, took: 0:48:23.797404\n"
     ]
    }
   ],
   "source": [
    "#Create tf \"vocab\" and then map each word to the vocab index\n",
    "start = dt.datetime.now()\n",
    "max_document_length = max([len(t.split(\" \")) for t in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Write vocabulary - will need to load it during \n",
    "vocab_path = './misc/20160113.vocab'\n",
    "vocab_processor.save('./misc/20160113.vocab')\n",
    "del x_text #save memory since we now have labels and data as arrays.\n",
    "print \"Finished vocab mapping, took: {}\".format(dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data before saving so that each pickle file has a random subset. \n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subsets of the Full DF\n",
    "- Each is roughly 600000 rows\n",
    "- Save each as its own file with x,y,path to vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_data(filename, x, y, filepath='./mapped_data/', \n",
    "               vocab_path=vocab_path):\n",
    "    #Stores subset of data as a pickle file\n",
    "    dataset = {'x':x, 'y':y, 'vocab_path': vocab_path}\n",
    "    with open(filepath + filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print \"Saved to \" + filepath + filename\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./mapped_data/mapped_data_shuffled_0.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_1.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_2.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_3.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_4.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_5.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_6.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_7.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_8.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_9.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_10.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_11.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_12.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_13.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_14.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_15.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_16.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_17.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_18.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_19.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_20.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_21.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_22.pkl\n"
     ]
    }
   ],
   "source": [
    "filesize = 600000 #rows in each file\n",
    "for i in xrange(23):\n",
    "    if i == 22:\n",
    "        #Open ended on the last chunk\n",
    "        y_sub = y[i*filesize:]\n",
    "        x_sub = x[i*filesize:]\n",
    "    else:\n",
    "        y_sub = y[i*filesize:(i+1)*filesize]\n",
    "        x_sub = x[i*filesize:(i+1)*filesize]\n",
    "    #Save as pickle\n",
    "    fname = 'mapped_data_shuffled_{}.pkl'.format(i)\n",
    "    store_data(fname, x_sub, y_sub)        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
