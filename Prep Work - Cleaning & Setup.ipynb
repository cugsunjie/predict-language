{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "\n",
    "def stripXML(t):\n",
    "    '''Strips XML and comments from text'''\n",
    "    if len(t) == 0:\n",
    "        return False\n",
    "    elif t[0] in  ['<', '(']:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def onlyLetters(t):\n",
    "    '''Keeps only letters in any language'''\n",
    "    try: \n",
    "        words = re.findall(r'[^\\W_0-9]+',t.decode('utf8'),re.U)\n",
    "        return \" \".join([w for w in words])\n",
    "    except: \n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def makeRecord(t, lang, pad=\"<PAD>\", n=70):\n",
    "    '''Takes sentence and adds a comma,\n",
    "    the language and a newline -> CSV record, \n",
    "    pads the sentence with <PAD> word to get to 70 words'''\n",
    "    lang = lang.replace('/', '')\n",
    "    try:\n",
    "        sentence = onlyLetters(t).encode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        return \"\"\n",
    "    length = len(sentence.split(' '))\n",
    "    padwords = \" \" + \" \".join([pad for i in range(n - length + 1)])\n",
    "    sentence = sentence + padwords\n",
    "    record = sentence + \", {0}\\n\".format(lang)\n",
    "    return record\n",
    "\n",
    "\n",
    "def splitNWords(t, n=70):\n",
    "    sentence = onlyLetters(t).encode('utf-8')\n",
    "    words = sentence.split(' ')\n",
    "    sentences = [\" \".join(words[i:i+n]) for i in range(0, len(words), n)]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little utility script to merge all the separate files into one big CSV per language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Strip out XML tags and non letters\n",
    "- Had to deal with cyrillic alphabet and non-ascii\n",
    "- Split into senteces of 70 words or less and then padded with `<PAD>` for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = './clean_txt_data/'\n",
    "txtdir = './raw_txt/'\n",
    "already_done = [d.split('_')[-1].split('.')[0] for d in os.listdir(datadir) if '.csv' in d]\n",
    "langs = [d for d in os.listdir(txtdir) if os.path.isdir(txtdir + d) and d not in already_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for lang in langs:\n",
    "    out_file = open(datadir + '/cleaned_data_{}.csv'.format(lang), 'a')\n",
    "    lang = lang + '/'\n",
    "    txt_files = os.listdir(txtdir + lang)\n",
    "    print \"Starting {}, {} files to read.\".format(lang, len(txt_files))\n",
    "    for txt in txt_files:\n",
    "        #Read original text\n",
    "        with open(txtdir + lang + txt, 'r') as f:\n",
    "            txt = f.read()\n",
    "        #Split into smaller 70 word sentences & flatten it\n",
    "        txt_list = [splitNWords(t) for t in txt.split('\\n') if stripXML(t)]\n",
    "        txt_list = [item for sublist in txt_list for item in sublist]\n",
    "        #Create CSV record: pad to 70 words, add language Split on newline\n",
    "        txt_list = [makeRecord(t, lang) for t in txt_list]\n",
    "        #Write to en file\n",
    "        out_file.writelines(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all languages into one large file\n",
    "- Allows us to create a vocab and then split into smaller stratified sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "langs = [d for d in os.listdir(txtdir) if os.path.isdir(txtdir + d)]\n",
    "colnames = ['txt', 'lang']\n",
    "df = pd.DataFrame({}, columns=colnames)\n",
    "for lang in langs:\n",
    "    lang_df = pd.read_csv('./clean_txt_data/cleaned_data_{}.csv'.format(lang), names=colnames)\n",
    "    df = pd.concat([df, lang_df])\n",
    "\n",
    "    del(lang_df)\n",
    "df.to_csv('./clean_txt_data/cleaned_data_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 13891597\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./clean_txt_data/cleaned_data_all.csv')\n",
    "print \"Total sentences: {}\".format(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4926599</th>\n",
       "      <td>También quiero apoyar la propuesta de enmienda...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858798</th>\n",
       "      <td>L ordre du jour appelle le rapport A de Jean L...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939192</th>\n",
       "      <td>Was das über die Menschenrechte Gehörte angeht...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10956765</th>\n",
       "      <td>środkom rzecz równości płci w przyszłych strat...</td>\n",
       "      <td>pl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13253341</th>\n",
       "      <td>unionen att återta sitt samarbete med Togo som...</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        txt lang\n",
       "4926599   También quiero apoyar la propuesta de enmienda...   es\n",
       "6858798   L ordre du jour appelle le rapport A de Jean L...   fr\n",
       "1939192   Was das über die Menschenrechte Gehörte angeht...   de\n",
       "10956765  środkom rzecz równości płci w przyszłych strat...   pl\n",
       "13253341  unionen att återta sitt samarbete med Togo som...   sv"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocab index\n",
    "- Shuffle the data set\n",
    "- split into sets of 400k - 500k records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create arrays of X and y, removed df from memory\n",
    "x_text = df.txt.values\n",
    "y = pd.get_dummies(df.lang).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished vocab mapping, took: 0:48:23.797404\n"
     ]
    }
   ],
   "source": [
    "#Create tf \"vocab\" and then map each word to the vocab index\n",
    "start = dt.datetime.now()\n",
    "max_document_length = max([len(t.split(\" \")) for t in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Write vocabulary - will need to load it during \n",
    "vocab_path = './misc/20160113.vocab'\n",
    "vocab_processor.save('./misc/20160113.vocab')\n",
    "del x_text #save memory since we now have labels and data as arrays.\n",
    "print \"Finished vocab mapping, took: {}\".format(dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data before saving so that each pickle file has a random subset. \n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x = x[shuffle_indices]\n",
    "y = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subsets of the Full DF\n",
    "- Each is roughly 600000 rows\n",
    "- Save each as its own file with x,y,path to vocab file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_data(filename, x, y, filepath='./mapped_data/', \n",
    "               vocab_path=vocab_path):\n",
    "    #Stores subset of data as a pickle file\n",
    "    dataset = {'x':x, 'y':y, 'vocab_path': vocab_path}\n",
    "    with open(filepath + filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "    print \"Saved to \" + filepath + filename\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./mapped_data/mapped_data_shuffled_0.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_1.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_2.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_3.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_4.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_5.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_6.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_7.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_8.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_9.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_10.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_11.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_12.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_13.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_14.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_15.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_16.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_17.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_18.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_19.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_20.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_21.pkl\n",
      "Saved to ./mapped_data/mapped_data_shuffled_22.pkl\n"
     ]
    }
   ],
   "source": [
    "filesize = 600000 #rows in each file\n",
    "for i in xrange(23):\n",
    "    if i == 22:\n",
    "        #Open ended on the last chunk\n",
    "        y_sub = y[i*filesize:]\n",
    "        x_sub = x[i*filesize:]\n",
    "    else:\n",
    "        y_sub = y[i*filesize:(i+1)*filesize]\n",
    "        x_sub = x[i*filesize:(i+1)*filesize]\n",
    "    #Save as pickle\n",
    "    fname = 'mapped_data_shuffled_{}.pkl'.format(i)\n",
    "    store_data(fname, x_sub, y_sub)        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
